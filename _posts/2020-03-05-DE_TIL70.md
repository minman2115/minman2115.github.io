---
layout: post
title: "AWS to AWS Data migration 일일 테크노트(20200304)"
tags: [Data Engineering]
comments: true
---

.

Data_Engineering_TIL_(20200304)



#### 1. Redshift 및 HBase 데이터 검증방안

1) 데이터 로우 갯수를 카운트해서 확인

2) 일부데이터를 샘플링하여 AS-IS와 TO-BE간 일치하는지 확인

** 참고사항 : HBase는 Mapreduce Rowcount라는 기능을 활용할 수 있음

#### 2. CDH의 HBase클러스터는 그냥 AMI 이미지를 떠서 옮기면 되는거 아니냐?

- 디스크만 옮긴다고해서 해결할 수 있는 문제가 아님


- HDFS라는 시스템을 생각해야 하는데 HDFS의 기본개념은 데이터를 블락단위로 쪼개고 3개이상 복제를 떠서 각 노드에 저장한다는 것인데 그렇게 하려면 네임노드라는 곳에서 각각의 노드정보를 알고 있어야 한다. 그런 정보는 Hadoop이 구동중일때는 항상 메모리에 갖고 있다가. 시스템을 내리게 되면 하드디스크에 저장하게된다.


- 어쨌든 네임노드는 데이터노드들의 정보와 거기에 뭐가 저장되어 있는지 알고 있는 곳인데 단순하게 디스크만 AMI로 떠서 마이그레이션을 한다고해서 버츄얼 머신의 시스템 정보들도 같이 마이그레이션을 하는게 아니기 때문에 AMI로 디스크만 떠서 옮겨봤자 소용이 없다는 말이다.


- 따라서 클라우드애라에서도 권고하는 것은 동일한 설정의 하둡시스템을 마이그레이션 할 플레이스에 먼저 띄우고 distcp 등의 기능을 이용해서 옮기라고 한다.


- 그렇게 되면 데이터만 특정 클러스터에서 다른 클러스터로 보내주게되고 네임노드에서 데이터를 받아서 그 클러스터 정보에 맞게 네임노드가 데이터노드에 데이터를 분산 저장한다는 것이다.


- HBase는 HDFS라는 로우한 시스템 위에서 작동하는 하이레벨 NoSQL 서비스일 뿐 결론은 HDFS라는 것이다.


- 그래서 이번 프로젝트의 핵심은 AS-IS의 네트워크 밴드위스를 얼마만큼 우리가 쓸 수 있냐에 따라 데이터 마이그레이션 속도가 달라진다는 것이다.


#### 3. 엔지니어로서 성장방향

엔지니어는 하나의 솔루션만 몰입해서 공부하면 안된다. 고객이 어떤 비지니스 환경이고 어떤 목적인지 정확하게 캐치해서 거기에 맞는 데이터 처리 툴들을 잘 조합해서 환경을 구성해야한다. 따라서 상황별로 어떤 툴들을 조합해서 데이터 플로우를 구성을 해야하기 때문에 어떤 상황에서는 어떤 툴이나 도구를 쓸 수 있고, 클라우드 밴더별로도 어떤 옵션이 있는지 큰 그림을 봐야한다.


#### 4. S3 Batch Operation 기능테스트

- 5GB 이상의 대용량 객체는 copy를 할 수 없는 것으로 확인


- s3api를 이용하여 5GB 데이터 객체가 있는 데이터를 식별해 해당 버킷은 AWS DataSync 서비스를 이용하는 방안을 고려중


** s3api 관련 URL : https://stackoverflow.com/questions/52232328/s3api-query-by-size-appears-to-be-not-working
