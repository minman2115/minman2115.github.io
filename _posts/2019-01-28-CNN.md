---
layout: post
title: "CNN 기초개념"
tags: [딥러닝]
comments: true
---

.

Deep_Learning_Studynotes_TIL(20190713)

#### study program : https://www.fastcampus.co.kr/data_camp_deeplearning

- 이미지 classification을 한다고 하면 거의 CNN을 무조건 쓴다고 봐야한다. 현재 이미지 classification 분야에서는 CNN 이상 성능이 잘 나오는 모델은 없다고 해도 무방하다.


- 그러면 이미지 classification 하는 함수는 뭐냐하면 input으로 image를 넣어주면, output으로 label을 출력해주는 함수라고 생각할 수 있다. 그러나 파이썬 프로그램으로 이런 기능의 함수를 직접 하드코딩으로 구현한다는 것은 매우매우 어려운일이다. 그래서 이미지를 분류한다고 하면 예를 들어서 cifar10 이미지를 이용해 Data Driven Approach하게 모델링을 수행할 것이다.


- Data Driven Approach란

이미지와 정답을 함수에 넣어주고, 그 함수가 output으로 model을 출력한다. 그러면 테스트 타임에 그 모델과 테스트 이미지를 넣어서 예측을 한다.

![11](https://user-images.githubusercontent.com/41605276/80858196-b5b6ee80-8c92-11ea-8507-1550f1f44ff2.PNG)

- 그래서 Data Driven Approach 한다면 가장 심플한 형태는 Nearest Neighbor를 구하는 모델일 것이다. 이 방법은 트레이닝을 어떻게 하냐면 그냥 일단 데이터를 전부 저장한다. 그리고 테스트할때 테스트 데이터가 들어오면 내 모델에 저장해놓은 모든데이터와 1대1로 다 비교해본다. 그래서 제일 가까운 이미지를 찾아낸다. 그 제일가까운 이미지의 레이블이 테스트 데이터의 정답이라고 판단할 것이다.


- Nearest Neighbor 방법의 단점은 모델링하면 역시 정확도가 떨어지게 되고, '가깝다', '멀다'의 기준도 정해줘야 한다는 것이다. 예를 들어서 아래 그림과 같이 L1 distance는 모든 픽셀들을 빼서 절대값 취해서 다 더해주는 방법인데 이런것들이 있을 것이다. L1 distance에서 값이 작을수록 비슷한 서로 비슷한 이미지로 판단할 것이다.

![2](https://user-images.githubusercontent.com/41605276/75607712-655dcc00-5b3d-11ea-9b09-14ecc02d16c2.png)

- 그럴사한 방법이지만 실제로는 사람의 직관과 다른 결과가 나오는 경우가 많아서 좋은 방법은 아니다.


- 그러면 제일가까운거 하나만 찾으라고 하면 부정확 할 수 있으니, 가장 가까운거 세개를 찾아서 다수결로 하자는 아이디어도 있는데 그게 K Nearest Neighbors 방법이다. 제일 가까운거 K개를 찾아서 voting을 한다. k가 1일때는 아웃라이어의 영향을 많이 받는 편이고 k개를 늘릴 수록 점점 줄어들게 된다. 하얀색부분은 voting을 해도 결론이 안난부분이다. 예를들어서 k개가 모두 다르게 voting 했을때의 경우이다.

![3](https://user-images.githubusercontent.com/41605276/75609097-29c8ff00-5b49-11ea-8c17-ada11d457b1d.png)


- 위와 같이 Nearest Neighbor가 사람의 직관과 달라 비현실적이다라는 대표적인 예시가 아래 그림과 같다. 오리지널 이미지와 나머지의 이미지들의 거리는 같다. 그러나 분명하게 세 이미지는 다르다는 것을 알 수 있다. 이미지는 사람이 보는 dimension보다 차원이 복잡하기 때문에 이런 거리를 측정해서 분류하는 방법에는 효과적이지 않다. 


- 왜냐하면 이미지가 상당히 하이 디멘전이기 때문에 실제 거리와 Nearest Neighbor의 우리가 생각하는 거리와 많이 다르다.


![4](https://user-images.githubusercontent.com/41605276/75609111-49602780-5b49-11ea-829a-b341e3dd7b0f.png)


- K Nearest Neighbors가 무조건 나쁘다는 얘기는 아니고 이 방법을 쓰는 경우도 있다. 예를 들어서 이미지를 학습한 MLP 모델이 있고 추가로 기존에 트레이닝한 데이터들을 저장하는 레이어를 넣어주면 테스트할때 MLP를 거치고 데이터들을 저장한 레이어를 거치면서 성능이 더 잘 나오는 경우도 있다. MLP 부분이 좋은 피쳐를 잘 뽑아주기 때문에 K Nearest Neighbors을 구하기 더 좋은 조건이라는 것이다.

![image](https://user-images.githubusercontent.com/41605276/80858532-b8b2de80-8c94-11ea-8c46-eaaa8ec12377.png)

- CNN은 위와 같이 레이어가 쭉 쌓여있는 형태이고, 대표적으로 컨볼루션 레이어, 풀링레이어, fully connected layer 세종류의 레이어로 구성되어 있다. fully connected layer는 그냥 MLP라고 생각하면 된다. 컨볼루션 레이어와 풀링레이어에서 하는 역할은 이미지를 구별하기 위한 특징을 추출하는 레이어이다. 위의 그림과 같이 배사진이 들어오면 앞쪽에 컨볼루션 레이어에서 예를들어 배는 앞쪽이 뾰족하다. 라는 특징을 찾는다. 풀링레이어는 그런 특징들을 잘 모아준다. 마지막에 fully connected layer에서는 그 모아진 특징들을 가지고 Classification을 실제 수행한다.

![image](https://user-images.githubusercontent.com/41605276/80858637-86ee4780-8c95-11ea-936d-33a4da0b8968.png)

- convolution 연산을 하는 것이 convolution filter이다. 위에서 가장 좌측상단에 있는 이미지가 오리지널 이미지이고, 컨볼루션 연산을 해서 나온결과가 그 외에 나머지 이미지들이다. 필터의 특징에 따라 이미지의 특정부분을 진하게 표시할 수도 있고, edge를 찾을 수도 있다. 또는 이미지를 전부 1로 만드는 블러링도 할 수 있다. 또는 가장 우측 하단에 있는 이미지처럼 올록볼록하게 엠보싱 효과도 넣어줄 수 있다. 

![image](https://user-images.githubusercontent.com/41605276/80858784-90c47a80-8c96-11ea-8550-8a957147b24a.png)

- 예를들어서 그러면 컨볼루션 연산을 어떻게 하느냐. 위의 그림과 같이 3x3 샤픈이라는 컨볼루션 필터가 있는데 대각선 구석코너에는 0으로 되어 있고, 가운데는 5, 나머지는 1로 채워져 있다. 위의 사람이미지와 같이 필터가 특정지점에 가서 사람이미지의 원래 픽셀값들이 있는데 그거와 필터값들과 연산이 이루어지면서 나온 결과값을 가운데에다가 적어준다. 이게 컨볼루션 연산이다. 이게 2차원으로 되어 있는데 실제로는 3차원에서 이루어진다. 앞서 MLP의 연산과 컨볼루션 연산과 사실 다른게 없다. 결국에는 그 레이어에 입력으로 들어오는 값에 가중치를 곱해서 다 더하는 것이 연산자체의 끝이다.

![image](https://user-images.githubusercontent.com/41605276/80858938-04b35280-8c98-11ea-937f-e13357ea3339.png)

- 과거에는 이런 필터들을 사람들이 직접 디자인했다. 필터를 디자인하고, 피쳐를 뽑아서 뽑힌 특징들을 가지고 Clasifier 보통 서포트 백터 머신을 이용해서 분류를 했었는데 딥러닝에서는 데이터를 더 많이 넣고, 레이어도 더 많이 쌓아서 특징을 뽑아내는 작업을 사람이 아니라 모델이 수행하도록 하게 되었다.


- 컨볼루션 필터를 visualization하면 위에 그림과 같이 아래쪽 레이어에서는 직선 또는 약간의 곡선, 색 같은 아주 단순하지만 기본적인 특징들을 뽑고, 하이 레벨로 갈 수록 이런 아래쪽 레이어에서 뽑은 특징들이 조합되서 점점 복잡한 특징을 뽑아내는 식으로 동작을 한다. 


- 그래서 개념적으로만 보면 이미지가 있으면 이미지를 조각을 내서(실제로는 이 조각들이 약간 겹치게해서 조각을 낸다) 컨볼루션 필터를 여러개를 쓸거고 필터마다 어떤 찾고자 하는 특징이 있다. 예를 들어서 강아지 이미지를 분류하는 모델이라고 하면 강아지를 찾기 위해서 어떤 필터는 예를 들어서 코를 찾아내는 필터가 있고, 그 코를 찾아내는 필터는 조각낸 타일들을 훑어 보면서 강아지 코에 대한 점수를 매긴다. 그래서 강아지 코와 가장 유사한 조각에 대해 높은 점수를 매길것이다. 그래서 그 타일마다 점수가 매겨져 있을 것이고, 그 점수를 다 모으면 점수판이 하나 생길것이다. 그것을 feature map이라고 한다. 그러면 강아지 코를 찾는 필터가 쭉 이미지를 돌고 나면 코에 대한 점수판 즉, 코에 대한 feature map이 만들어진다. 그 다음에 강아지 귀를 찾는 필터가 또 이미지를 쭉 훑어서 강아지 귀에 대한 점수판 즉, 귀에 대한 feature map이 만들어질 것이다. 실제로는 사람이 이해할 수 없는 feature map들이 만들어지는데 사람기준에서 설명을 하면 그렇다는 것이다. 어쩠든 이런 feature map들이 만들어지면 채널별로 합친다. 처음에는 입력이 RGB 세개였는데 필터의 갯수를 많이 쓸수록 채널이 많아지게 된다. 어쨌든 이렇게 채널별로 합친다음에 다음 레이어로 보낸다. 그러면 다음레이어에서 그럴 받아서 그걸 좀더 크게 본다. 예를 들어서 앞에서는 타일을 하나씩 봤는데 이제는 4개씩 보게 된다. 그래서 이미지의 특정부분을 보니까 강아지 코에 대한 점수와 귀에 대한 점수가 높다. 그러면 이부분을 보고 강아지의 얼굴부분이구나 라는 특징이라는 점수를 높게 주게 된다. 얘는 이미지를 직접보는 것이 아니라 이전 레이어의 점수표를 보고 "아! 이자리에 코가 있고, 귀가 있고, 입이 있으니까 여기가 얼굴이 맞네"라는 판단을 하게 된다. 그렇게 점점 레이어가 올라가면 올라갈 수록 처음에는 상당히 단순한 피쳐를 뽑는 것에서 점점 크게보면서 더 하이레벨의 피쳐를 뽑게 된다. 그래서 최종적으로 여기에 얼굴있고, 몸통이 있고, 꼬리있고 그러니까 얘는 강아지구나 라고 판단하는 것이 CNN의 동작원리이다.


- CNN에서 중요한게 로컬하게 특징을 뽑아낸다는 것이다. 전체이미지를 처음부터 다 보고 하는 것이 아니라 딱 일부분만 보고 여기에 그 필터가 찾고자 하는 특징이 있는지 없는지만 본다.(반면에 fully connected layer의 개념은 784개를 처음부터 전체를 본다.) 그래서 이렇게 뽑아낸 특징들을 계층적으로 쌓아가면서 (예를 들어서 첫레이어는 타일 하나씩 보고, 그 다음 레이어는 4개씩 보고, 그 다음 레이어에서는 8개씩 보고 이런식으로 점점 늘려가면서) 내가 이미지를 가까이서 보다가 점점 거시적으로 보는 것이라고 생각하면 된다. 다시말해서 로컬하게 특징들을 뽑아내서 그것들을 조합해서 뭔가 만들어내는 것이다.


- 2D Convolution Layer

cifar 10 이미지와 같이 아래 그림을 보면 가로세로 32 채널이 3(RGB 세개) 가 있고, 우리가 Convolution filter를 얘기할때 5x5 필터입니다 라고 얘기한다. 통상적으로 컨볼루션 필터는 홀수를 쓰는 편인데 반드시 그렇게 해야한다는 것은 아니다. 실제 컨볼루션 필터에 있는 숫자는 25개가 아니라 75개이다 왜냐하면 뒤에 3이 있기 때문이다. 3이라는 숫자는 어디에서 왔냐면 input 채널이 3이기 때문이다. 

여기서 또 컨볼루션의 중요한 특징이 등장하는데 '컨볼루션 필터 1개에 채널은 입력으로 들어오는 input feature map(= input activation map = activation)인데 (어떤 레이어를 딱 짤랐을때 거기에 입력으로 들어오는 것을 input feature map이라고 부른다) 그 input feature map의 채널 수가 그 레이어의 컨볼루션 필터 1개의 채널수를 결정한다. 그래서 내가 컨볼루션 필터를 이번 레이어에서는 5x5짜리로 쓸래라고 해도 5x5필터라고 하지만 실제 거기에 숫자가 몇개 있는지는 그 input 채널을 봐야 알수 있다. 

참고로 output feature map의 채널은 컨볼루션 필터의 갯수가 결정하게 된다. 필터를 몇개쓰냐에 따라 점수판을 몇개 만드냐를 결정하기 때문이다. 그래서 내가 첫번째 입력으로 들어오는 얘는 그냥 이미지이기 때문에 흑백채널이면 채널이 1일 것이고, 컬러이미지면 채널이 3일 것이기 때문에 이거는 사용자가 컨트롤 할 수 없는 부분이지만 그 다음부터는 필터의 갯수를 사용자가 조절할 수 있기 때문에 최초 input 이후로는 사용자가 다음레이어의 채널(=필터개수)을 조절할 수 있다.

그래서 이 5x5 필터가 강아지 코에 대한 점수를 쫙 매기면 점수표가 하나 생기는데 그게 feature map 또는 activation map이라고 하는 것이다. 그 다음에 필터를 하나 더 쓰면 이번에는 강아지 눈에 대한 점수를 쫙 매기면 강아지 눈에 대한 feature map이 또 생기는 것이다. (아래 그림의 두번째 그림과 같이.)

이런 필터를 6개를 쓰면 아래 그림과 같이 6개의 feature map이 생길 것이다. 이것들을 다음 레이어로 보낸다. 들어올때는 채널이 3으로 들어왔지만 아웃풋은 6의 채널로 나오게 된다. 가로세로가 28로 줄어들었는데 만약에 3x3 필터를 썼다면 가로세로가 2가 줄어들고, 5x5 필터를 쓸때는 가로세로 4가 줄어들 것이다.

그래서 이 feafure map들을 다음 레이어로 보내면 일반적으로는  렐루를 적용해서 여기 있는 얘들을 음수를 0으로 바꿔서 다음 레이어로 보내주게 된다.

![image](https://user-images.githubusercontent.com/41605276/80859369-4c87a900-8c9b-11ea-88ad-a3899b455e37.png)


- Dense Layer와 1-D 컨볼루션 레이어의 차이

Dense Layer는 말그대로 fully 연결되어 있는 레이어이다. MLP 같은 경우는 아래의 두번째 그림과 같이 여러개가 연결될 수 있다. 아래 그림에서 색이 다른 이유는 색마다 가중치가 전부 다르다는 의미다. 반면에 컨볼루션 레이어는 아래 그림과 같이 x0,1,2,3 전부 다 보지않고, 예를 들어서 3x3은 어떤 점을 중심으로 왼쪽 오른쪽 하나 대각선방향 하나씩 보면 되는 것처럼 사용자가 지정한 필터크기에 따라 일부만 보게 된다. (5x5는 두개식 보는 필터일 것이다.) 즉, 전체를 다 안보고 로컬하게 본다는 것이다. 그리고 나서 아래 그림의 가장 아래에 있는 부분을 보면 또 다른 점을 계산할때는 여기도 마찬가지로 로컬하게 보는데 빨간색 파란색 초록색 웨이트가 이전 점에서 계산할때 웨이트와 같다는 것이다. 컨볼루션 레이어는 그래서 웨이트를 쉐어하고 로컬하게 커넥트 되어 있다는 것이 fully connected layer와 다른 특징이다. 

![image](https://user-images.githubusercontent.com/41605276/80859861-cec59c80-8c9e-11ea-826d-a2d0c43dd87f.png)


- 그래서 CNN에서 하고자 하는게 뭔데?

아래 예시와 같이 필터를 통해 스캔된 숫자를 찾는 것이다. 컨볼루션 레이어에서 학습을 한다는게 어떤 것이냐면 input으로 들어오는 이미지는 정해져 있고, 필터의 가중치를 찾는 것이다. 집이미지가 들어오면 집이라는 특징이 있으면 그 특징을 뽑아낼 수 있는 그런 필터를 알아서 학습하라는 것이다. 그래서 그 필터를 통해 특징을 뽑아내면 그 모델의 출력이 되는 것이다.

![image](https://user-images.githubusercontent.com/41605276/80860374-37624880-8ca2-11ea-8775-045ee5622b91.png)


- 컨볼루션 연산이 무엇을 하는지

아래 그림에서 필터는 7x7이다. 나머지는 0으로 채워져 있고 일부 30으로 채워진 것을 볼 수 있는데 이를 그림으로 표현하면 쥐의 엉덩이부분이라고 할 수 있다. 쥐 그림이 있는데 해당 엉덩이 부분의 특징을 뽑아내는 컨볼루션 필터로 스캔했을때 연산과정이 아래의 그림과 같다. 그래서 이걸 보고 알 수 있는 것은 결국에 이 필터로 쥐의 엉덩이부분과 똑같은 모양을 찾겠다는 것이다. 엉덩이와 비슷한 모양이 있으면 그 부분은 높은 숫자가 나올것이다 아래와 같이. 반면에 전혀 상관없는 모양이라면 낮은 점수가 나올 것이다. 필터가 찾고자 하는 피쳐에 잘 부합하는 것일 수록 더 큰 숫자를 출력할 것이다. 

![image](https://user-images.githubusercontent.com/41605276/80860523-02a2c100-8ca3-11ea-925c-d58a6ad8efb5.png)

실제로는 필터값이 음수도 있고 훨씬 복잡하지만 개념적으로 이해해보자는 것이다. 만약에 채널이 여러개가 되면 실제로는 아래 그림과 같다. 인풋채널이 3이고, 컨볼루션 필터가 2개인 경우이다. 그러면 출력으로 나가는 것의 채널이 2가 된다. 이 2가 필터의 갯수와 같다.

![image](https://user-images.githubusercontent.com/41605276/80860671-2286b480-8ca4-11ea-9b32-2b3240133153.png)

실제 컨볼루션 연산은 결국에는 3D로 이루어지는 연산이다. 그런데 왜 2D 컨볼루션이라고 했냐면 아래 그림과 같이 필터가 움직이는 방향이 2D로만 움직인다는 것이다. 컨볼루션 연산의 특징은 가로세로 연산은 전체 이미지의 로컬한 일부만 하는데 채널은 항상 전체를 다 한다는 것이다. 그래서 내가 점수표를 전부 뽑았을때 점수표 전체를 다 보고 연산을 해야한다. 강아지의 코,눈,귀를 종합적으로 보고 판단을 하는 것이다.

아래 그림에서는 1번부터 M번까지의 컨볼루션 필터를 쓰는 형태다. input feature는 채널이 C이고, 이는 컨볼루션 필터하나에 채널이 C가 되는 것이다. 그리고 컨볼루션 필터의 갯수가 M개니까 output feature의 채널도 M개가 되는 것이다. 

![image](https://user-images.githubusercontent.com/41605276/80866494-f716c080-8cc9-11ea-9f0d-1883283d3af5.png)

그러나 실제 우리가 텐서플로우나 캐라스로 코딩할때는 위에 그림과 같이 4차원 형태가 된다. 왜냐하면 배치때문에 그렇다. 이미지를 한장만 넣는것이 아니라 한꺼번에 N을 넣을 것이기 때문에 N개가 인풋으로 들어가고, 출력도 N개가 나올것이다. 따라서 4차원 형태가 되는 것이다. 따라서 텐서플로우 캐라스 상관없이 CNN을 구현한다면 입력으로 4차원 텐서가 들어가고, 출력으로 4차원 텐서가 나올 것이다.

- 컨볼루션 연산의 두가지 옵션

option 1) stride

컨볼루션 필터가 한번 컨볼루션을 수행하고 옆으로(또는 아래로) 얼마나 이동할 것인가

![image](https://user-images.githubusercontent.com/41605276/80866678-faf71280-8cca-11ea-907b-5e13278a63d2.png)

option 2) zero padding

주변에 값을 채우는데 0으로 채우는 것을 말한다. 0은 색깔에서 검정색을 얘기하는 것이고 결국에는 검정으로 채우는 것이다. 그러면 0을 왜넣냐 주변에 있는것을 그냥 복사해서 넣을 수도 있는거고 평균을 채워넣을수도 있는건데. 그 이유는 0을 넣어야 연산에 영향을 안주기 때문이다. 또한 점점 이미지가 작아지는 것을 막고 싶어서 하는 것이다. 물론 polling해서 줄여지기는 하는데 이거를 사용자가 컨트롤 하고 싶은 것이다. 왜냐하면 32x32인데 5x5필터쓰면 4씩 줄어든다. 나는 레이어를 100개 쌓고 싶은데 그러면 5x5필터를 쓰다보면 절대 100개를 쌓지 못할것이다. 한 레이어씩 지나갈때마다 4씩 줄어들기 때문이다. 따라서 100개 이상 레이어를 쌓고 싶다면 이렇게 레이어를 지나갈때마다 크기가 작아지는 것을 통제해야한다. 

0을 채워넣는 또 하나의 이유는 구석에 있는 단순하게 생각했을때 연산이 적게 된다. 패딩을 하지 않으면. 필터의 가운데에 있는 얘는 여러번 연산이 되는데 구석에 있는 값들은 한번밖에 연산이 안될것이다. 다시말해서 어떤 필터가 있는데 얘는 오른쪽 아래에 강아지 귀가 있는 것을 찾는 필터라고 하면 그 부분에 패딩이 되어 있지 않으면 찾기가 힘들것이다. 결론적으로 구석에 있는 것도 피쳐를 잘 뽑아 내기 위해서는 패딩을 해줘야 한다는 것이다. 

그래서 컨볼루션 필터를 3x3를 쓴다면 패딩을 하나하면 스트라이드 1기준으로 사이즈가 유지된다. 5x5는 패딩을 두줄하면 사이즈가 유지되고, 7x7은 세줄하면 사이즈가 유지된다.

- 렐루

![image](https://user-images.githubusercontent.com/41605276/80867015-d7cd6280-8ccc-11ea-9527-877d7504ffaa.png)

- 컨볼루션 레이어를 구현할 수 있는 텐서플로우 API

아래 파라미터 값에서 filters는 필터의 갯수를 말하는 것이다. 이게 output 채널을 결정하는 것이다. 커널 사이즈가 3x3인지 5x5인지 이런거를 정해주는 것을 말하고, 스트라이드는 말그대로 스트라이드를 얼마나 줄건지를 집어넣으면 된다. 패딩은 valid와 same 둘중에 하나를 쓰면 된다. valid는 패딩을 안하는 것이고, same은 스트라이드가 1일때를 기준으로 가로세로 사이즈가 변하지 않게 해주는 패딩이다. 3x3필터를 스트라이드 1로 했을때 태딩을 한다면 그냥 가로세로 한줄로 하면 되는데 스트라이드가 2로 하는경우 1로 하는 것과 다르게 딱 안떨어지는 경우가 있다. 그럴겨우 결론적으로 오른쪽과 아래에 한줄을 더 추가해준다. 아래 그림에서 보면 Pend가 오른쪽과 아래쪽을 얘기하는 것이고, Pstart가 왼쪽과 윗쪽을 말하는 것이다. 아래 공식에 대입하면 패딩을 몇줄해야하는지 나온다. 


아래 data_format에서 channels_last는 4차원 텐서를 구성하는 순서가 있는데 맨 앞은 무조건 배치고 그 다음은 세로(height) 그 다음은 가로(width), 그 다음이 채널이다. 이게 채널 라스트이고 채널퍼스트로 바꿀수도 있다.(아래 그림의 설명 참고) 


그리고 커널이라는 것이 컨볼루션 필터를 얘기하는 것이다. 컨볼루션 필터를 커널이라고도 하고 필터라고 하기도 한다. 그런데 커널은 이 필터를 알아서 생성을 한것인데 사용자가 굳이 직접 만들어서 넣어주고 싶으면 tf.variable을 선언해서 4차원으로 만들어서 넣어주면 된다. 그 4차원의 순서가 어떻게 되는지 보면 필터가 height, width, 인풋채널, 아웃풋 채널 이 순서로 되어있다. 필터와 피쳐맵과 4차원을 구성하는 용도가 다르다. 

![image](https://user-images.githubusercontent.com/41605276/80867113-55916e00-8ccd-11ea-8000-655b44f425bf.png)

- 풀링레이어

풀링은 맥스풀링과 에버리지 풀링을 일반적으로 쓰는 편이고, 다른 풀링 방법들도 있지만 이것들은 거의 안쓴다고 보면 된다. 컨볼루션 레이어 중간에 풀링레이어를 끼어넣을거면 맥스풀링을 더 많이 쓴다. 그러면 맥스풀링은 뭐냐 아래 그림과 같이 2x2 필터라고 표현은 하지만 실제로 무언가 연산을 하는게 아니라 제일 큰값을 뽑는 것이다. 그리고 스트라이드를 2로 해놓는다. 보통 풀링은 사이즈를 죽일때 많이 쓰기 때문에 스트라이드를 1로 잘 주지 않는 편이다. 2이상으로 준다. 

또한 풀링은 특징이 채널별로 따로따로 한다. 따라서 풀링을 하고 나면 채널수가 변하지 않는다. 가로세로 싸이즈만 바뀌게 된다. 또한 풀링은 가중치가 없다. 이것은 학습하는 과정이 아니라 어느정도 결과가 정해져 있는 방법이다. 입력이 들어오면 출력이 바로 정해지는 것이지 학습을 해서 학습결과에 따라 출력이 달라지는 것이 아니다.

그래서 CNN이 레이어 몇개짜리 CNN이다라는 것을 자주 따지게 될 것인데 이때 풀링레이어는 레이어로 치지 않는 편이다. 

반대로 에버리지 풀링은 맥스가 아니라 에버리지가 될 것이다. 그 차이만 있는 것이다.

풀링을 하는 이유는 풀링을 안하면 연산양이 너무 많아지기 때문에 적당히 사이즈를 줄여서 모아주는 역할이다. 이때 맥스 풀링을 더 많이 쓰는 이유는 컨볼루션 필터가 찾는 특징에 부합하는 얘가 숫자가 크게 나올 것인데 연산양을 줄이기 위해 사이즈를 줄인다면 어떤거는 버리고 어떤거를 취해야 하냐면 제일 큰값을 취해야 한다. 특징을 잡는데 용이하기 때문이다.

![image](https://user-images.githubusercontent.com/41605276/80867580-d5b8d300-8ccf-11ea-8e7d-0495c3d2ce05.png)

- fully connected layer

28x28의 이미지를 flatten 하는 것과 같이 똑같이 flatten 된 것이 아래 그림과 같이 벡터의 형태가 되는 것이다. 그리고 그 뒤에 멀티레이어 퍼셉트론을 여러개 두면 되는 것이다. 아래 그림은 간단한 예시여서 하나만 있는것이다. 그 멀티레이어 퍼셉트론 뒤에 고양이냐 개냐 구분하는 소프트맥스를 넣으면 분류를 할 수 있을 것이다. 

![image](https://user-images.githubusercontent.com/41605276/80868399-3b5b8e00-8cd5-11ea-8d21-edffb72306f8.png)

- 고전적인 CNN의 특징

아래 그림은 가중치 파라미터수에 대해서 컨볼루션 레이어와 fully connected layer가 차지하는 비율에 대한 것인데 전체 가중치 파라미터 수는 fully connected layer가 거의 대부분 가져간다. 반대로 연산량은 컨볼루션 레이어가 거의 대부분을 차지한다. 왜 이런 현상이 나타나냐 컨볼루션 레이어는 작은 수의 파라미터로 연산을 많이 하는 대신에 파라미터 수는 적다. 반면에 fully connected layer는 가중치 하나가 연산 한번에만 사용된다.

![image](https://user-images.githubusercontent.com/41605276/80868505-0bf95100-8cd6-11ea-8e66-cee169e128a6.png)

하지만 구글넷 같은 모델이후로는 파라미터수도 비슷하게 맞춰진다. 요즘 나오는 CNN 모델의 추세는 구글넷과 비슷한 양상이다.


#### [CNN 기초개념 요약]

#### 그림, 실습코드 등 학습자료 출처 : https://datascienceschool.net

### # CNN : Convolutional Neural Network

#### [등장배경]

- 'fully connected network'로 이미지 classification을 시도하였을때 수렴이 잘 안되는 현상을 식별


- 이미지 용량이 커질수록 이러한 문제는 더 커지게됨 


- 'fully connected network'는 쓸때없이 파라미터가 너무 많다. 이 파라미터를 줄이는 방법을 강구해보자


- 그래서 적은 파라미터를 가지고도 효과적으로 기저함수를 뽑아낼 수 있는 방법을 찾아보자


- 현실적으로 어떤 이미지가 '고양이'다 '강아지'다 라고 구분할 수 있는 과정에서 우리가 feature를 찾을때 구석 1번부터 마지막 픽샐까지 전부다 확인하고 찾지는 않는다.


- 이미지의 일부분만 보고도 고양이와 강아지를 충분히 구분할 수 있다.

#### # CNN의 출발 : 일부 픽셀만 사용해서 classification을 할 수 있는 feature를 뽑아보자

- 하지만 단순히 이미지 픽셀을 백터로 쭉 펼쳐놓고 일부만 고를 수는 없는 노릇이다.

- 그래서 2차원 이미지를 1차원으로 펼치지 않은 상태에서 특정한 패턴을 추출해보자 해서 나온게 convolution이다.

#### # Convolution (Image Filtering)

- 이미지를 특정한 패턴, filter image(kernel)로 내적하며 스캐닝하여 새로운 이미지 생성

#### # feature map

- 어떤 이미지에서 특정 이미지 패턴에 대해 가중치를 a = 1인 출력을 내도록 트레이닝 되었다면 hidden layer는 feature가 존재하는 위치를 표시하게 된다.(feature map)


- 여기에서 feature는 input data를 의미하는 것이 아니라 이미지 분류에 사용되는 input data의 특정한 패턴을 말한다.

#### # Max Pooling

- 중복되어 있는 픽셀을 feature로 다쓸필요 없다


- 일부 이미지를 생략하여 이미지 사이즈를 축소하는 것


- 특정영역을 지정하여 영역내에서 가장 최대값을 출력한다.


- 영역내에 이미지 패턴이 존재하는지의 여부의 정보만 남기는 개념이다.

#### # Multiple Feature Maps

- 하나의 공통 weight set은 한 종류의 image feature만 발견 가능한데


- 예를 들어 개와 고양이를 구분하는데 이 feature가 몇개가 필요한지는 모른다.


- 따라서 복수의 feature map (weight set) 필요

#### weight값을 사용자가 처음에 파라미터로 입력하면 신경망은 점점 classification이 잘되는 방향으로 weight값을 수렴시킨다.

그래서 최종적으로 몇개의 image feature map이 나오게 되고 이 몇개의 image feature map으로 이미지를 구별하게 된다.

#### [28 x 28 컬러 이미지를 CNN을 통해 10개의 클래스로 분류하는 과정 필기]

![1](https://user-images.githubusercontent.com/41605276/75607186-a7840f00-5b37-11ea-8628-a27f51253bec.png)

#### [이론관련 연습문제]

- 한층의 Convolutional layer + max pooling을 통과 후 Fully Connected Network로 분류하는 모델을 만들었을때 다음 문제를 푸시오.


1) 64 x 64 gray scale image에 5 x 5 convolution을 적용하였을때, feature map의 크기는?

#### 1) 정답 : 60 x 60


2) 그리고 이 convolution layer의 feature map의 개수가 16개일때, 파라미터의 개수는?

#### 2) 정답 : 16 x 25 = 400개


3) 이 Feature map을 2 x 2 maxpooling에 통과 시킨 후 feature map의 크기는 ?

#### 3) 정답 : 30 x 30


4) 이후 바로, fully connceted layer에 넣는다면, fully connceted layer의 input dimension은? (모든 경우에 padding은 적용되지 않았고, stride는 1일때)

#### 4) 정답 : 30 x 30 x 16
