---
layout: post
title: "Spark RDD Operation 기본개념 요약"
tags: [Data Engineering]
comments: true
---

.

Data_Engineering_TIL(20210827)

[학습시 참고자료]

"[Spark] Collect 와 Count 의 차이" 블로그글

https://wooono.tistory.com/338?category=914839

[학습내용]

- Spark에서 RDD 는 2가지 Operation(Transformation, Action)을 사용해 조작이 가능함

1) Transformation

기존의 RDD 를 변경하여 새로운 RDD 를 생성하는 것으로 return 값이 RDD임

예시) map, filter 등

2) Action

RDD 값을 기반으로 무엇인가를 계산해서, 결과를 생성하는 연산으로 return 값이 데이터 또는 실행 결과임

예시) collect, count 등

- 위와 같은 RDD 동작 원리의 핵심은 Lazy Evaluation 임

RDD 는 Action 연산을 하기 전까지는, Transformation 연산자가 있으면 그거를 처리하지 않고 쌓아둠

따라서, Executor 에 할당된 RDD 의 내용을 확인하기 위해서는, Action 연산(collect, count 등등)이 필요함

- DAG(Directed Acyclic Graph)

RDD의 연산들은 쌓여서 히스토리를 갖게 되는데 위와 같은 연산들로 RDD를 변경했을때 순서를 리니지라고 함

리니지는 DAG(Directed Acyclic Graph) 형태가 됨

따라서 연산의 순서가 순환되는게 없고, 일정한 방향을 가지기 때문에 연산의 순서가 중요한 형태이다.

그래서 RDD 연산 과정에서 특정 RDD 관련 정보가 메모리에서 유실될 경우 DAG 그래프를 복귀하여 다시 계산하고, 복구할 수 있다.

Spark는 이런 특징 때문에 Fault-tolerant를 보장한다.
