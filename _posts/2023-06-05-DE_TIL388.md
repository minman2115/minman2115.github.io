---
layout: post
title: "카카오게임즈 데이터 파이프라인 운영사례"
tags: [Data Engineering]
comments: true
---

.

Data_Engineering_TIL(20230605)

### Youtube "카카오게임즈 - 대용량 데이터레이크 마이그레이션 사례 공유" 자료를 공부하고 정리한 내용입니다.

** URL : https://youtu.be/HSgy2vvmA-Q

## [공부한 내용]

원래는 온프라미스에서 데이터 파이프라인을 운영하고 있었는데 오딘이라는 게임을 출시하게 되면서 기존의 데이터 파이프라인에서는 못버틸거라고 판단했다. 그래서 오딘 출시전에 새로운 데이터 파이프라인의 빠른 구축이 필요했다.

새로운 데이터 파이프라인의 요건은 아래와 같다.

요건 1. 어떠한 데이터 유형도 빠르게 분석이 가능해야함

요건 2. 신규 게임출시나 신규 콘텐츠 오픈때 폭발적인 트래픽 규모에 따라서 탄력적인 리소스 조정이 가능해야함

요건 3. 데이텉 손실이나 장애발생을 최소화 하도록 가용성과 내구성을 보장해야함

요건 4. 인프라 관리와 운영할때 엔지니어들의 부담을 최소화하여 데이터에만 집중하도록 할것

#### 기존에 온프라미스 데이터 파이프라인 --> 신규 데이터 파이프라인 마이그레이션 일정표

8주 구축 + 3주 오픈준비 = 12주간 진행

1주차 : 프로젝트 체계 수립

2주차 : 아키텍처 구성 계획, s3 버킷 설계

3주차 ; 실시간 데이터 파이프라인 테스트

4주차 : DW 벤치마크

5주차 : 준실시간 데이터 시각화 대시보드 테스트

6주차 : 실시간 데이터 파이프라인 구축, Airflow 구축

7주차 : 준실시간 데이터 시각화 대시보드 구현, Redshift 구축

8주차 : 데이터 마이그레이션, DW 개발

9 ~ 11주차 : Open checklist 점검 및 보완

12주차 : 프로젝트 오픈

#### 데이터 파이프라인을 구성하는 서비스

<img width="968" alt="1" src="https://github.com/minman2115/Data_engineering_studynotes_2023/assets/41605276/5f8c8a1d-8cf2-4dec-bbfb-0c3b2f0a4b28">

데이터 소스는 게임 클라이언트와 플랫폼에 남겨지는 로그 데이터임

이 로그 데이터를 키네니스로 수집한 다음에 s3에 저장함

#### TO-BE 데이터 파이프라인 아키텍처

<img width="879" alt="2" src="https://github.com/minman2115/Data_engineering_studynotes_2023/assets/41605276/e60ee0b3-724a-4311-a89e-461cac9397c7">

#### 데이터 레이크 (S3) 내 구조

아래와 같이 티어를 구분하여 데이터를 저장하는 구조임

<img width="895" alt="3" src="https://github.com/minman2115/Data_engineering_studynotes_2023/assets/41605276/28b4cfaf-727d-43b3-978d-f9ab1b53d2a7">

#### 수집하는 로그데이터 샘플

<img width="939" alt="4" src="https://github.com/minman2115/Data_engineering_studynotes_2023/assets/41605276/611c7c34-acaf-4a3c-896a-30e50bbd6e75">

code가 item인 것으로 보아 아이템 데이터임. json과 유사한 형태지만 json 데이터는 아님.

#### 로그데이터를 키네시스 데이터 스트림즈로 받아서 이를 전처리하기 위해 고려했던 4가지 서비스

<img width="565" alt="5" src="https://github.com/minman2115/Data_engineering_studynotes_2023/assets/41605276/d3501784-acf7-43a8-9ee7-715f7e783f69">

카카오 게임즈에서는 이 네가지 서비스를 모두 구축해서 테스트를 해본결과 성능, 비용, 효율성 등을 종합적으로 고려했을때 람다를 이용해서 스트리밍 데이터를 처리하는 것이 적합하다고 판단함.

람다로 선택한 이유는 서버리스 서비스로 탄력적인 자원운영을 쉽게 할 수 있고, 파이썬으로 쉽게 코드를 짜서 바로 적용이 가능하다는 장점도 있었다. 

<img width="816" alt="6" src="https://github.com/minman2115/Data_engineering_studynotes_2023/assets/41605276/7205a511-0b5a-45dc-a556-e0d386a54f27">

하지만 이 람다가 오토스케일링이 잘 되어도 어느정도 버틸 수 있는지 확인이 필요해서 부하테스트를 아래와 같이 진행했다.

평소 로그의 4 ~ 5배 정도인 초당 20만 ~ 30만 건의 데이터를 버틸 수 있는지 테스트를 진행하였다.

<img width="958" alt="7" src="https://github.com/minman2115/Data_engineering_studynotes_2023/assets/41605276/956a0c65-bbd3-49e8-acfc-27149a54b0e9">

30만건으로 테스트시 : 키네시스 테스트 제네레이터 환경에서는 로그가 발생하지 않았음

28만건으로 테스트시 : read는 가능했지만 write가 26만건으로 밀리는 현상이 발생함

부하발생 6시간 경과후 람다 동시실행 제한은 2000개, 메모리는 256mb 설정으로 초당 26만건까지는 안정적으로 처리할 수 있다는 결론을 확인함.

#### 배치처리 파이프라인의 핵심

EMR 클러스터 생성시 메타스토어를 글루로 지정하게 되면 글루기반의 테이블을 소스로 활용할 수 있음.

<img width="887" alt="8" src="https://github.com/minman2115/Data_engineering_studynotes_2023/assets/41605276/6dc9e154-8a15-474b-9ef5-73efe53bc147">

위와 같이 처리한 데이터는 위에 동그라미 아이콘에 문구들과 같이 4가지 장점이 있었다.

#### DW에 데이터 적재를 하기 위한 배치 파이프라인 아키텍처임

<img width="1009" alt="9" src="https://github.com/minman2115/Data_engineering_studynotes_2023/assets/41605276/902f4290-e4b1-43d1-8873-dea47c77d67d">

에어플로우는 오픈소스를 커스텀하게 설치한 에어플로우 서버를 구축해서 사용하였고 워크플로우 흐름은 위와 같다.

에어플로우 오퍼레이터를 이용해서 S3 config bucket에 있는 EMR 환경구성 file을 읽어와서 EMR 클러스터를 생성했다.

그리고 EMR은 아래와 같이 성능테스트를 진행하여 결론을 도출했다.

<img width="963" alt="10" src="https://github.com/minman2115/Data_engineering_studynotes_2023/assets/41605276/b23f00e4-2a23-4773-b68a-9d5db49d661d">

#### DW 솔루션은 redshift를 채택했다.

<img width="984" alt="11" src="https://github.com/minman2115/Data_engineering_studynotes_2023/assets/41605276/975bb81d-06b5-460f-a08c-eedf38b87bf7">

#### 데이터 파이프라인 마이그레이션 결과

<img width="748" alt="12" src="https://github.com/minman2115/Data_engineering_studynotes_2023/assets/41605276/9ee0d2bd-edf5-4ff3-8dfd-fadefd69647b">