---
layout: post
title: "Data Engineering TIL (20190312	)"
tags: [Data Engineering]
comments: true
---

Apache Spark basic 1


#### [학습목표]


- Apache Spark 개요 및 기본원리 이해


#### [학습기록]


- 스파크의 핵심개념은 RDD이다. RDD는 Resilient Distributed Dataset으로 탄력적이면서 분산된 데이터셋이라는 의미이다. 분산처리(병렬처리)가 되는 가상의 데이터셋(리스트(array))인데 오류 자동복구가 가능하다는 것이다. 다양한 계산수행이 가능하고 메모리를 활용하여 높은 성능을 보여주는 것이 특징이다.


- RDD는 결국에 리스트인데 가상에 리스트라고 생각하면 된다. map, reduce, count, filter, join 등 다양한 작업 가능하고, 작업을 병렬적으로 처리할수 있으며, 여러 작업을 설정해두고 결과를 얻을 때 lazy하게 계산이 가능하다. lazy하게 계산이 가능하다는 것은 RDD에 연산되는 모든스텝의 연산과정을 메모리에 담고 있고, 한단계 더 나아가서 빅데이터 스케일에서 장점을 보인다. 예를들어서 3단계의 맵리듀스 연산 단계가 있다면 각 단계별로 결과를 확인하면서 한스텝 한스텝 나아가는데 스파크에서는 메모리에 각 연산과정을 저장해두고 실행을 했을때 메모리에 저장되었던 연산과정이 한번에 일어난다.


- 하둡의 맵리듀스를 어떻게하면 좀 더 편하게 써볼까하는 아이디어에서 나온게 RDD이다. 이후에 사람들이 빅데이터를 접하다보니까 비정형데이터 보다는 그래도 정형으로 된 데이블데이터를 많이 실제로는 활용하게 되어서 테이블데이터를 조금 더 편하게 다룰수 있는 도구가 있어야 겠다는 필요성을 느꼈고 또한 머신러닝 기술이 발전하면서 머신러닝 알고리즘에 들어가는 데이터도 많은경우에 테이블형태의 데이터이기 때문에 이런 요구사항이 커져서 RDD형태보다는 데이터프레임 형태로 된게 여러모로 유용하다고 판단되어 스파크 데이터프레임이 만들어지게 되었다.


- 스파크가 큰 인기를 끈 또다른 이유는 스칼라 언어로 된 인터페이스가 굉장히 잘 되어 있기 때문이다. 프로그램을 사용하기 매우 편하다는 의미이다. 예를들어서 맵리듀스 코드를 구현하려면 매우 어렵고 많은 양의 코드타이핑이 필요하지만 스파크에서는 단 몇줄이면 전부 구현 가능하다.


- 스파크의 또다른 장점은 스파크 엔진을 기반으로 다양한 확장 프로젝트를 제공한다는 것이다. 스파크 하나만 잘 배워도 아래의 확장 프로젝트도 쉽게 접근할 수 있다.

1) Spark SQL: Hive와 비슷하게 SQL로 데이터 분석

2) Spark Streaming: 실시간 분석

3) MLlib: 머신러닝 라이브러리

4) GraphX: 페이지랭크같은 그래프 연산(분석)


- 스파크 데이터프레임은 판다스 데이터프레임과 매우 유사한데 차이점은 판다스 데이터프레임은 컴퓨터 한대가 커버할 수 있는 범위만 허용되지만 스파크 데이터 프레임은 빅데이터 스킬도 커버가 가능하다.


- 스파크에서 RDD 동작은 action(표현된 데이터를 가져옴)과 transformation(데이터를 어떻게 구해낼지를 표현)로 이루어져 있다. Map, Filter 등이 transformation에 해당되고 count, collect, take, data load 등이 action에 해당된다.


- Lineage와 Lazy Execution도 잘 알고 있어야 한다. Lineage는 특정데이터가 어떤 연산을 거쳐 어떻게 바뀌어야 하는지에 대한 수행단계 내용을 말한다. Lineage는 잘 갖고 있어야 하는데 그 이유는 클러스터 중 일부의 고장 등으로 작업이 중간에 실패하더라도 Lineage를 통해 데이터를 복구가 가능하기 때문이다. Lazy Execution은 Transformation시에는 계산을 수행하지 않고 Action이 수행되는 시점부터 데이터를 읽어들여서 계산을 시작하는 것을 말하다. Lazy Execution의 장점은 맵리듀스에 비해서 중간단계의 연산결과를 디스크에 저장하지 않기 때문에 성능면에서 효율적이다.


- 예를 들어서 데이터양이 많아서 한 스텝의 연산별로 2시간씩 걸린다고 치면 과거 하둡 맵리듀스 시절에는 2시간 기다렸다 연산결과 확인하고, 또 2시간 기다렸다 연산결과 확인하고, 또 2시간 기다렸다 연산결과 확인하는 방식이라면 스파크는 실행을 누르면 그냥 6시간을 기다리면 되지만 실제로는 6시간도 아니고 1~2시간이면 끝난다. 굉장히 일이 빨리 수행되는 그런 장점이 있다.


- take는 collect와 비슷한데 collect는 RDD에 있는 데이터를 전부 꺼내는데, take는 RDD에 있는 데이터 일부만 꺼낸다. 빅데이터 스케일에서는 collect를 쓸 수 없고, take를 써야한다.


- 예를 들어서 1000억개의 데이터를 로드를 한다면 먼저 로드를 하고 RDD를 만드는데 실제로 로드하지는 않고 로드 하는척만하고 로드한 정보만 갖고 있다. 그러다가 count같은 액션이 일어나면 그때 데이터를 로드를 하기 시작한다. 이때 클러스터에서 응답이 가능한 메모리 용량만큼 읽어서 RDD에서 count한 count 끝나면 읽어온 데이터 버리고, 그다음에 또 응답가능한 메모리만큼 읽어서 RDD에서 count한 다음에 count 끝나면 읽어온 데이터 버리고 이런 방식으로 연산하여 최종결과를 얻어오는 방식이다.


- 사실 RDD에서 count action을 할때는 데이터가 메모리에 있을 필요가 없이 필요한 만큼 하나씩 읽어서 count하면 되는건데 join이나 group을 해야할때는 메모리가 많이 필요하다. 메모리가 부족한 경우도 있는데 이 경우에는 부득이하게 디스크에 저장하게된다.


- RDD Transformations은 RDD의 데이터를 다른 형태로 변환하는것으로 실제로 데이터가 변환되는 것이 아니라, 데이터를 읽어들여서 어떻게 바꾸는지 방식만을 기록하는 것이다. 실제 변환은 Action이 수행되는 시점에서 이루어진다. map, filter, flatMap, mapPartitions, sample, union, intersection, distinct, groupByKey, reduceByKey, join, repartition 등 다양한 연산을 할 수 있다.


- RDD Actions은 여러가지 변환 (Transformation)이 담긴 RDD의 정보를 통한 계산을 수행하는 것이다. reduce, collect, count, first, take, saveAsTextFile, countByKey, foreach 등이 있다.


- RDD Caching는 반복 계산에서의 성능 향상을 위해 RDD의 내용을 메모리에 캐싱이 가능하다는 의미이다. 다르게 얘기하면 반복계산이 아닌경우 캐싱을 할필요가 없다는 말이다. rdd.persist() 또는 rdd.cache() 메서드를 통해 구현할 수 있다. rdd.unpersist()는 캐싱을 해제하는 것을 말한다.


- 스파크에서 캐싱을 하려면 예를들어서 스파크 클러스터가 있다고 치자 클러스터를 구성하는 컴퓨터는 10대가 있다고 치자. 각각의 클러스터는 32기가의 메모리를 갖고 있다고 가정하자. 그러면 총 320기가의 메모리의 클러스터가 된다. 이때 연산하고자 하는 데이터가 100기가라고 치자. 이 클러스터에서 캐싱을 위해 활용하는 메모리가 전체 메모리의 60%라고 쳐도 100기가의 데이터는 충분히 수용할 수 있을것이다. 만약에 이 데이터가 100기가가 아니라 400기가면 메모리에 넣어도 넘쳐 흐르기 때문에 넘치는 부분을 디스크로 내려버린다. 그러면 또 느려지게 된다. 그래서 전부 다시 읽는것보다 느릴 수도 있다. 이럴 경우 캐싱을 안하는게 오히려 이득일 수도 있다. 따라서 캐싱을 한다고해서 무조건 빠른것은 아니다.


- 참고로 오픈소스 프로젝트는 공식문서를 참고하는게 가장 정확하고 가장 좋다고 할 수 있다.

- RDD 내부는 4가지 파트로 구성된다.

1) Partition : 데이터를 나누는 단위

2) Dependency : RDD의 파티션이 어디에서 파생되었는지를 기술하는 모델, 리니지의 한 단계라고 할 수 있다. 성능에 가장 impact가 있는 부분이다.

3) Function : 부모 RDD에서 어떻게 파생되었는지를 계산하는 함수

4) Metadata : 데이터 위치와 파티션 정보를 가지고 있음

![1](https://user-images.githubusercontent.com/41605276/54258642-9c427700-45a6-11e9-9d1e-eb3cedba7442.png)

- 디펜던시는 두가지 타입이 있다. wide 디펜던시는 성능의 문제가 걸려있다. 왜냐하면 데이터를 다른 컴퓨터에 네트워크로 전송을 해야하기 때문에 네트워크 속도 문제도 있고, 많은 데이터를 엮어서 다른 데이터를 만드는 것이기 때문에 메모리 문제도 생긴다. 빅데이터 문제에서는 가능하면 wide 디펜던시를 피하는게 좋다.

![2](https://user-images.githubusercontent.com/41605276/54258651-a3698500-45a6-11e9-999a-2ebd28b0ad79.png)

- 스파크 RDD에서 join 시 키를 그때그때 정해줘서 쿼리를 날리는 것이 아니라 데이터 구조를 아예 키벨류 구조로 키를 정해놓고 join하면 이미 정해진 키끼리 조인이 되는 것이다. outerjoin은 키가 매치가 안되어도 빈칸으로 가져오라는 것이다.


- 반면에 스파크 데이터프레임에서는 sql처럼 그때그때 무슨키로 조인할때 지정해주는식으로 sql처럼 join을 해주면 된다.

![3](https://user-images.githubusercontent.com/41605276/54258661-aa909300-45a6-11e9-98d9-b3adf047bd33.png)

- 클러스터 매니저 : 마스터, 컴퓨터 한대


- 워커 : 슬레이브로 실제 일을하는 요소다. 워커들은 각각 컴퓨터 한대


- 드라이브 프로그램 : pyspark를 실행하면 spark context (sc)라는 객체가 있는데 컨텍스트를 통해서 마스터와 연락하거나 워커로 부터 데이터를 받으면서 일을 수행하는 것을 말한다. 


- 테스크 : 스파크에서는 테스크 단위로 수행한다. 데이터가 너무 크면 쪼개져서 하나의 파티션이라는 단위로 나뉘어져서 RDD에서 처리하게 된다. 파티션 하나를 맵하면 파티션 하나가 다른 파티션으로 변환될텐데 그 하나가 테스크 하나가 된다. 워커에는 executer라는게 있고 거기에서 테스크들을 여러개를 수행할 수 있는 방식으로 되어있다. 워커에 여러개의 executer를 띄울 수 있다. executer마다 캐시를 가지고 있어서 데이터를 캐싱하기도 한다.

- 스파크 간단한 실습 예시 : word count

spark.md이라는 텍스트 파일의 word count를 해보자

step1) spark shell 구동

./bin/pyspark

step2) 텍스트 로드

text = sc.textFile("README.md")

step3) split

스페이스로 스플릿한다. flatmap은 string 하나를 split했더니 array로 출력되어 이중array가 된 데이터를 하나의 어레이 안에 정리해주는 함수를 말한다. 

words = text.flatMap(lambda s: s.split(" "))

step4) word count

words.map(lambda w: (w, 1))는 데이터를 키벨류로 정리해주는 것이고

reduceByKey(lambda a,b: a+b)는 정리한 키를 모아서 데이터가 몇개가 나왔는지 카운트 해준다.

counts = words.map(lambda w: (w, 1)).reduceByKey(lambda a,b: a+b)

step5) print

result = counts.sortBy(lambda x: x[1], False).take(20)

for x in result:

print(x)
