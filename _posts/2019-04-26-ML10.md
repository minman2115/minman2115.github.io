---
layout: post
title: "분류모델 성능평가 기초개념"
tags: [Classification]
comments: true
---

.

#### # 학습 시 참고 URL : https://datascienceschool.net

#### 1. 분류모델 성능평가 개요

우리가 회귀분석을 할때는 성능평가 기준이 비교적 간단했다. 항상 그런건 아니지만 대부분의 경우에는 잔차제곱합 RSS를 줄이면 우리가 원하는 좋은 모델이 되는 경우가 많았다.

그러나 분류문제에 있어서는 성능을 평가하는 것이 회귀분석과 비교했을때 복잡하다.

Scikit-Learn 에서 지원하는 분류 성능평가 sklearn.metrics 서브 패키지의 메서드만 해도 아래와 같이 7개가 있다.

1) confusion_matrix()

2) classfication_report()

3) accuracy_score(y_true, y_pred)

4) precision_score(y_true, y_pred)

5) recall_score(y_true, y_pred)

6) fbeta_score(y_true, y_pred, beta)

7) f1_score(y_true, y_pred)

- 분류문제에서 벌점

회귀분석에서는 오차가 크면 그 큰 숫자가 그대로 잔차제곱합에 반영이 되어 그것을 줄여줘야 했는데, 분류문제에서 오차함수 즉 벌점은 아주 미세하게 틀렸어도 벌점 1점이고, 아주 큰차이로 틀렸던 똑같은 벌점 1점으로 같다.

#### 2. confusion matrix (분류결과표)

먼저 confusion matrix에 대해 알아보자. 어떤 classification을 했을때 가장 기본적으로 결과를 나타내는 리포트가 Confusion Matrix 또는 Confusion Table 이라는게 있다.

Confusion Matrix는 타겟의 원래 클래스와 모형이 예측한 클래스가 일치하는지는 갯수로 센 결과이다. 원래 클래스는 행으로 예측한 클래스는 열로 나타낸다.

![1](https://user-images.githubusercontent.com/41605276/56806591-4f211880-6867-11e9-9620-dadbcc41f578.jpg)

분류모형에 대한 성능이 위와같이 정량화되어 테이블 형태로 나오게 된다.

좋은모델은 정답을 맞추는 대각선 부분의 숫자가 많고, 비대각선 부분이 적어야 한다.

#### 3. Binary Confusion Matrix (이진분류 결과표)

- 통상적으로 양성반응은 Positive(ex) 사기거래,환자 등), 음성은 Negative(ex) 정상거래,정상인 등)


- 결과표는 prediction한 결과로 작성하게 된다.

![2](https://user-images.githubusercontent.com/41605276/56806602-57795380-6867-11e9-85f5-f29bf6a4d954.jpg)

#### 4. 평가점수

위와 같이 3x3가 되었든 2x2가 되었든 confusion matrix로 결과를 보게되면 정량적으로 모델의 성능을 확인할 수 있다. 이것만으로 어떤 스코어라고 말할 수 없다. 숫자가 하나가 아니라 9개나 4개이기 때문이다. 그래서 우리가 무언가 평가를 하려면 하나의 숫자 스칼라값으로 압축을 시켜야한다. 예를들어서 회귀분석할때는 RSS로 압축하듯이 그렇게 해줘야 한다. 분류문제도 마찬가지로 1개의 숫자로 나타낼 수 있어야 한다.

그런 평가방법들이 아래의 그림과 같다.

![3](https://user-images.githubusercontent.com/41605276/56806612-5ea06180-6867-11e9-85b8-2269a20e4e71.jpg)

- accuracy는 전체문제 중에 모델이 맞춘것만 따진다는 것이다. 가장 상식적인 방법이다. 내가 푼 모든문제 다시말해 2x2 confusion matrix라면 4개의 cell의 모든 값들 중 TP + TN 의 비율을 말한다. 현실적으로 우리가 만드는 모델들은 전부 이거를 기준으로 최적모델을 찾는다.


- 그러면 accuracy만 쓰면 되지 다른 지표들은 왜있는거냐. 분류문제에서는 특히나 이진분류표에서 각각의 셀은 이런것들이 현실적으로는 모두가 똑같은 중요도를 갖고 있지 않다. 푸는문제에 따라 각각의 셀의 중요도가 달라지게 된다. 가장 대표적인 예가 진단과 QA문제이다. 실제로 정상인인데 암환자라고 오진하는 것도 문제지만 실제 환자인데 정상인이라고 판별하는 것은 그 사람에게는 정말 치명적인 문제이기 때문이다. QA문제도 마찬가지로 이 타이어가 실제로는 문제가 있는데 정상제품이라고 판별을 하는 순간 이 제품을 쓰는 소비자의 안전에 치명타가 될 수 있는 문제이다. 이런문제는 recall이 무조건 100프로를 지향해야한다.


- 그래서 단순히 accuracy만 볼 문제가 아니기 때문에 여러가지면에서 스코어를 측정할 수 있어야 한다는 아이디어에서 위와 같이 각각의 케이스에 따른 평가방법이 있는 것이다.


- 반대로 fallout은 억울한 케이스의 비율이기 때문에 recall과 대조적으로 낮은 수치를 지향해야한다. 정상인 얘들중에 False Positive인 얘의 비율.


- 일반적으로 precision을 높이면 recall이 떨어지는 경향이 있고, recall을 높이면 precision이 떨어지는 경향이 있다. 의사가 질병을 진단하는 분류 문제라고 치면 precision을 높이는 방법은 그 질병으로 판단하게 되는 threshold를 높여주면 된다. 거꾸로 recall을 높이고 싶으면 thereshold를 낮추면 되는데 그러면 precision을 떨어질 수 밖에 없다.


- 위에서 설명한 각종 평가 점수들은 서로 밀접한 관계를 맺고 있다. 예를 들어 recall과fall-out은 양의 상관관계가 있다. 그리고 precision와 recall은 대략적으로 음의 상관 관계가 있다.

recall을 높이기 위해서는 양성으로 판단하는 기준(threshold)을 낮추어 약간의 증거만 있어도 양성으로 판단하도록 하면 된다. 그러나 이렇게 되면 음성임에도 양성으로 판단되는 표본 데이터가 같이 증가하게 되어 fall-out이 동시에 증가한다. 

반대로 fall-out을 낮추기 위해 양성을 판단하는 기준을 엄격하게 두게 되면 증거부족으로 음 판단을 받는 표본 데이터의 수가 같이 증가하므로 recall이 떨어진다.

정밀도의 경우에는 recall과 fall-out처럼 정확한 상관관계는 아니지만 대략적으로 음의 상관 관계를 가진다. 즉 정밀도를 높이기 위해 판단 기준을 엄격하게 할수록 재현율이나 위양성율이 감소하는 경향을 띤다.


- F score에서 어느한쪽에 가중치를 더 두고 싶다하면 배타를 조정하면 된다. precision에 더 가중치를 두고 싶다고하면 배타를 1보다 크게해주면 된다. 그러면 precision에 조금 더 가중치를 주게 된다. 통상적으로 배타를 얼마를 두냐라는 기준이 없어서 1로 두는경우가 많다. 배타를 1로 두는 경우를 F1 score라고 한다.


- 우리가 분류모델을 만들고 classification report를 출력하게 되면 아래와 같은 표가 나오게 된다.(classification report 예시) 

여기서 참고로 support는 실제로 정답이 class 0인 데이터의 갯수 5개, 실제로 정답이 class 1인 데이터의 갯수 2개를 말한다.

우리가 이 report를 보고 보통은 밑에 avg 부분을 보는경우가 많다. 그 중에서도 f1-score의 avg를 기본적인 퍼포먼스로 보는 경우가 많다. 그러나 이것만 딱 보면은 안된다. 위에도 언급했듯이 문제가 어떤 문제냐에 따라 recall rate가 중요한 경우가 있기 때문이다.

가끔은 이런경우가 나오기도 한다. 전체 평균은 60 ~ 70인데 리콜벨류만 확떨어지는 경우가 있다. 어떤 특정한 클래스 얘들을 전혀 못잡는 현상인 것이다. 그렇게되면 문제가 뭐냐에 따라 달라지기는 하지만 그 문제가 리콜이 중요한 문제라고 하면 그러면 이 모델이 무언가 잘못된 것이라고 판단하면 된다.

![4](https://user-images.githubusercontent.com/41605276/56806623-695af680-6867-11e9-8d04-37cd292b84f5.jpg)

#### 5. ROC 커브

위와 같이 이 모델이 무언가 잘못되었다고 판단하고 변형시킬 수 있는 방법 중에 하나가 ROC 커브이다.

문제에 대해서 fall-out하고 recall의 변화를 2차원상의 스케터플롯으로 그려준 것이다. 아래의 그림이 ROC 커브의 예시인데 가로축이 fallout이고 세로축이 recall이다.

하나하나 점을 이어준 선이다.

![5](https://user-images.githubusercontent.com/41605276/56806630-6fe96e00-6867-11e9-80d8-e7bc7332fdd4.png)

이진분류모형은 판별함수값이 음수이면 0인 클래스, 양수이면 1인 클래스에 해당한다고 판별한다. 즉 0 이 클래스 판별 기준값이 된다.

ROC 커브는 이 클래스 판별 기준값이 달라진다면 판별 결과가 어떻게 달라지는지는 표현한 것이다.

Scikit-Learn의 Classification 클래스는 다음처럼 판별 함수 값을 계산하는 decision_function 메서드를 제공한다. 아래 그림의 표는 분류문제를 풀고 decision_function 메서드를 이용하여 모든 표본 데이터에 대해 판별 함수 값을 계산한 다음 계산된 판별 함수 값이 가장 큰 데이터부터 가장 작은 데이터 순서로 정렬한 예시이다. 

아래 표는 로지스틱 리그레션으로 출력한 결과인데 그러면 원래 predict_probability 메서드만 갖고 있어야 하는거 아니냐 라고 할 수 있는데 사이킷런에 있는 대부분의 얘들은 동시에 decision function도 갖고 있다. 그러면 확률만 계산했는데 어떻게 decision function이 나오냐라고 따질 수도 있는데 사이킷런 내부적으로 여러가지 방법이 있는데 가장 간단한 방법은 우리가 구한 확률에서 0.5를 빼주면 되는 방식이 있다. 그러면 확률이 50%인 순간 0이 되는 것이고, 확률이 50%보다 커지는 순간 1이 되는 방식이다. 그러면 decision function의 역할을 할 수 있는 것이다. 실제로는 이것보다는 복잡한 방식을 쓴다.

어쨌든 이 probability를 알면 decision function을 계산할 수 있고 이 decision function을 알면 일종에 sudo probability를 계산할 수 있다. 그래서 현실적으로 우리가 싸이킷런에 있는 모델을 보면 얘가 확률적인 모형인데도 불구하고 decision function이 있다. 또는 얘가 판별함수임에도 불구하고 probability라고 하는 메서드가 존재하는 경우가 있다.

decision function은 '얘가 양수다'라고 판단하는 증거의 크기라고 할 수 있다. 아래 표의 예시는 decision function이 0보다 크면 얘를 1이라고 모형이 판단하였고, 음수인얘는 다 0이라고 판단을 했다. 아래표에서 정답을 달아놓은 것이 'y' 컬럼이다.

아래표에서 상단에 있는 얘들은 decision function이 강한편이다. 다시말해 증거가 쌔다는 것이다. 이렇게 증거가 쌘 얘들은 실제로도 정답이 진짜로 1일 가능성이 높을 것이다.

그러나 6번 데이터처럼 decision function이 양수여서 1로 예측했는데 실제로는 정답이 0인 경우도 있다.

![6](https://user-images.githubusercontent.com/41605276/56806636-7677e580-6867-11e9-8e95-348ac6d2e361.png)

현재는 0을 threshold으로 클래스를 구분하여 판별함수값이 0보다 크면 Positive, 작으면 negative가 된다.데이터 분류가 다르게 되도록 threshold을 증가 혹은 감소시킨다. 

위의 표에서는 threshold을 0.244729보다 크도록 올리면 6번 데이터는 positive가 아니다. threshold을 여러가지 방법으로 증가 혹은 감소시키면서 이를 반복하면 여러가지 다른 기준값에 대해 분류 결과가 달라지고 recall, fallout 등의 성능평가 점수도 달라진다.


위에서는 실제로 positive인 얘들이 8개가 있는데 이중에 위에 표에서는 7개를 잡아냈다. 87.5%의 recall을 나타낸 것이고, fallout은 8개의 정상적인 얘들이 있었는데 그중애 positive라고 판별한 억울한 얘들이 2개가 있다. 그러면 25%가 나올 것이다.

이렇게 두개를 구하면 ROC 커브상에 좌표를 표시할 수 있다. 만약에 현재 recall이 마음에 안든다 recall을 90% 이상은 하고 싶다 그러면 threshold을 더 낮추면 된다. 이제는 조금이라도 의심이 가는 얘를 positive라고 판별하면 recall이 올라갈 것이다. 그러면 recall을 올림으로써 억울하게 판별되는 케이스인 fallout도 같이 올라갈 것이다.


Scikit-Learn에서 만들어진 모형은 기본적으로 accuracy를 최대화하는 모형이다. 하지만 accuracy, recall, precision 등의 성능이 동일한 모형도 ROC 커브에서 살펴보면 성능이 달라지는 것을 볼 수 있다.

다시말해 confusion matrix, classification 리포트의 내용이 거의 비슷해도 ROC 커브를 그려보면 성능이 다르다는 말이다. 쓰레시홀드를 변경했을때 어느지점에서는 두 모델이 다르게 나올 수도 있다는 말이다. 쓰레시홀드의 특정지점에서는 두 모델이 성능이 완전히 같을 수도 있지만 또 다른 특정지점에서는 성능이 달라질 수 있다는 말이다.

ROC 커브에서 예를 들어서 두개의 모델의 곡선이 있고 이 둘이 어떤게 좋을지 비교한다고하면 fallout이 같다면 당연히 recall이 높은게 좋다. 만약에 리콜이 같다고 하면 fallout이 낮은게 좋다. 정리하면 커브를 그렸을때 커브가 좌측상단의 구석에 더 가까운 커브인 모델이 좋다.


threshold를 바꾼다는 것은 동전의 앞면 또는 뒷면이 나오는 확률을 조작해주는 것과도 같다. 


ROC 커브에서 양끝단은 random guess라고한다.


정상적인 모형이라면 ROC 양끝단을 이어주는 직선 아래로 좌표가 찍히지 않을것이다. 그렇게 되었다면 모델이 무언가 이상한것이다. 쉽게말해 어떤 형사가 범인이라고 잡은 사람은 모두 정상인이었고, 정상인이라고 모두 놓아준 사람들이 모두 범인인 이런 케이스 일때 그런 좌표가 찍힐것이다.


#### 5. AUC 

AUC는 Area Under the Curve를 말하는 것으로 아까 ROC 커브를 그렸을때 좌측 상단의 구석으로 가까운 커브가 좋은 모델이라고 했는데 그것을 수치로 표현한 것을 말한다.

AUC는 ROC curve의 면적을 뜻한다. Fallout 대비 Recall 값이 클 수록 AUC가 1에 가까운 값이며 좋은 모형이다. 가장 좋은 모델은 1이고, 가장 최악의 모델은 0.5이다.
