---
layout: post
title: "Randomly Wired Neural Networks 기초개념"
tags: [딥러닝]
comments: true
---

.

Deep_Learning_TIL(20210228)

study program : https://www.fastcampus.co.kr/data_camp_deeplearning

패스트캠퍼스 "TensorFlow로 시작하는 딥러닝 입문 CAMP"를 공부하고 정리한 내용입니다.

** 참고자료 : https://github.com/jwlee-ml/TensorFlow_Training_13th

[학습내용]

앞서 NASNet에서 봤듯이 search space를 줄이는 것에 대한 반대생각을 표현한 논문이다.

2020년 나온 논문으로 알려진 네트워크이다. 입력이 들어가서 컨볼루젼을 하고, 동그라미 마다 어떤 연산을 하게 된다. 노란색 동그라미를 기준으로 큰 덩어리들이 랜덤하게 만들어진 것이다. 그냥 그래프 이론에 random graph generation 하는 방법이 몇가지가 있는데 그 중에 몇가지 방법을 써서 이 구조를 랜덤그래프로 만든다. 다시말해서 네트워크 구조를 그냥 랜덤하게 만든것이다. 이렇게해서 학습을 시켰더니 사람보다 더 잘한다는 충격적인 결론이 있었다. 그러면 지금까지 우리는 뭘한거냐 라고 의문을 던진다. 

![11](https://user-images.githubusercontent.com/41605276/109441030-78201f00-7a77-11eb-865b-84003f05efa4.PNG)

이 논문에서 궁극적으로 말하고 싶은 것은 사람들이 지금까지 쌓아두었던 knowledge에 제약을 두고 search space를 찾는데 그러지말고 조금 더 풀어서 해야한다. 딥러닝이 나오기전에 머신러닝을 할때 사람들이 피쳐를 손으로 뽑아서 피쳐들을 머신러닝 알고리즘을 통해서 classification을 했다. 딥러닝이 나오면서 이 피쳐를 자동을 뽑게 되었다. 그리고 auto ML이 나오면서 얼마전까지만해서 네트워크를 사람의 손에서 설계가 되었는데 이를 자동화하게 되었다. 그러면 네트워크를 generate하는 것을 어떻게 하면 잘 할건지를 생각해야 한다. 네트워크를 하나하나 사람이 디자인 하던거를 자동화를 했으니까 이를 어떻게 잘할건지를 생각을 해야된다는 것이다. 그래서 다양한 방법의 generator를 만들어서 랜덤하게 해봤더니 이렇게 해도 잘 되니까 우리가 좀더 생각의 폭을 넓여야 하지 않겠느냐 라는 것이다.