---
layout: post
title: "머신러닝 이해를 위한 엔트로피 기초개념"
tags: [머신러닝]
comments: true
---

.


#### # '패스트캠퍼스'에서 공부한 내용을 정리한 것으로 일부 주관적이며 오류가 있을 수 있습니다.

- 패스트캠퍼스 : https://www.fastcampus.co.kr

- 참고자료 출처 : https://datascienceschool.net

### # 머신러닝 이해를 위한 엔트로피 기초개념

- Y=0  또는  Y=1인 두 가지 값을 가지는 확률 분포가 다음과 같이 세 종류가 있다고 하자.
- 확률 분포  Y1 :  P(Y=0)=0.5 ,  P(Y=1)=0.5 
- 확률 분포  Y2 :  P(Y=0)=0.8 ,  P(Y=1)=0.2 
- 확률 분포  Y3 :  P(Y=0)=1.0 ,  P(Y=1)=0.0


- 베이즈 관점에서 위의 확률분포를 보게 되면 0이냐 1이냐에 대해서 우리가 알고 있는 지식의 정도를 나타내는 정보다라고 할 수 있다.


- 확률분포 Y1은 얘가 y가 0인지 1인지 모르겠다는 의미고,Y2은 얘가 y가 0이라는 확신의 정도가 80%라는거고, Y2은 얘가 y가 0이라는것이 확실하다는 의미이다. Y2같은 경우는 사실 확률적인 것이 아니게 된다. 결정론적인것이다.


- 우리가 y에 대한 확률분포를 안다는 것은 y에 대한 현실세계의 지식을 우리가 알고 있다는 것이다. 우리가 데이터분석을 해서 알아내고자 하는 최종목적은 사실 y가 어떤 값이 잘 나오는지 분포를 아는 것이다. 혹은 알아낼 수가 없다는 것을 알아내고자 하는 것이다.


- 위와 같은 과정이 prediction이다. 그래서 우리가 알아낸 값들이 영양가가 있는건가 없는건가 예를들어 Y1과 같이 실제 주사위를 던지기 전까지는 알수 없는것들은 영양가가 없는 것이다.


- 위와 같은 여러가지 케이스가 있을것이다. 그러면 우리가 알아낸 값들이 영양가가 있는지 없는지 측정하는 지표가 있다. 이 지표를 엔트로피라고 한다. Y1처럼 이 확률분포만 알아서는 영양가가 없는 케이스가 엔트로피가 높은 것이다. 엔트로피가 낮다는 것은 반대로 결정론적인 것을 말한다.


- 엔트로피라는 것은 우리가 확률분포를 봤을때 그 확률분포가 얼마나 결정론적인 것에 가까운가 결정론적인 것에서 멀리 떨어져 있는가를 말한다. 


- 수학적으로는 다음과 같이 수식을 표시할 수 있다. 


확률변수 Y가 이산확률변수라면


$$\ H[Y] = -\sum_{k=1}^K P(y_k) \log_2 P(y_k) $$


확률변수가 Y가 연속 확률변수라면


$$\ H[Y] = -\int_{-\infty}^{\infty} p(y) \log_2 p(y) \; dy $$


- 위의 수식들은 어떤 함수들이 있으면 이 함수들을 스칼라값으로 바꿔주는 연산이다. 이런함수를 범함수라고 한다. (입력값 : 함수 -> 출력값 : 스칼라)


- 좀더 정확하게는 위의 수식에는 확률분포 함수들이 들어간다. 근데 출력값으로 튀어나오는건 숫자가 나온다. 확률값에 로그취한것에 기대값이라고 할 수 있다.


- 참고로 확률값에 로그 취한것을 '정보량'이라고도 부른다. 즉 위의 식은 정보량의 기댓값이라고 할 수 있다.


- 확률변수가 결정론적이다 라는 것은 앤트로피가 0이 된다. 그리고 엔트로피가 갖을 수 있는 가장 작은값이다. 엔트로피가 최대라는 것은 결정론적인 것에서 가장 반대쪽 극단에 있는 경우이다. 다시말해 전혀 예측할 수 없을때를 말한다. 예를들어 맨위에 '확률분포  Y1' 케이스를 말한다.


- 클래스가 2의 k승개라고 하면 이산확률변수가 가질 수 있는 엔트로피의 최대값은 각 클래스가 모두 같은 확률을 가질때이다. 이때 엔트로피의 값은 $$\ H = -\frac{2^K}{2^K}\log_2\dfrac{1}{2^K} = K $$ 이다.


- 그러면 베르누이 확률변수에서 엔트로피가 가장 높을때는 1이다. 나올 수 있는 사면체 주사위에서는 그러면 엔트로피가 가장 높을때는 2이다. 만약에 면에 8개인 주사위일 경우는 엔트로피가 가장 높을때가 3이다.


- 엔트로피가 0이면 확률변수는 결정론적이므로 확률 변수의 표본값은 항상 같고, 따라서 확률 변수의 표본값을 관측한다고 해도 우리가 얻을 수 있는데이터의 정보량은 없다고 할 수 있다.


- 확률변수가 모든 정보를 이미 내포하고 있고, 그래서 굳이 데이터를 얻을 필요가 없다. 


- 엔트로피는 사실 통신분야에서 나온 개념인데 예를 들어서 어떤 언어는 A,B,C,D 이렇게 4개의 문자로만 표현할 수 있는 언어라고 치자. 문자를 전송하기 위해 A = "00", B = "01", C = "10", D = "11"로 인코딩을 하게 되는데 좀 더 좋은 인코딩 방식이 없나 해서 고안된 개념이다. 


- 위와 같은 방법으로 인코딩을 하게되면 한글자가 두글자로 인코딩이 되게 된다. 이게 비효율적이어서 수학자들이 이런방식을 생각해냈다. 실제로 이 A,B,C,D를 쓰는데 있어서 문자를 사용하는 비중이 같지 않다. 


- 예를 들어서 A,B,C,D 각 알파벳이 나올 확률이 동일하지 않고 다음과 같다고 가정하자. A가 나올 확률 : 1/2, B가 나올 확률 : 1/4, C가 나올 확률 : 1/8, D가 나올 확률 : 1/8


- 이럴때는 다음과 같이 인코딩을 하면 위의 인코딩 방식보다 훨씬 인코딩 숫자를 효율적으로 줄일 수 있다. A = "0", B = "10", C = "110", D = "111" 요런 방식을 variable length encoding이라고 한다. 


- 엔트로피라는 것은 이런식으로 가변길이 무손실 인코딩을 할때 나오는 평균길이를 말한다


- 실제 데이터가 주어진 경우에는 확률질량함수를 추정하여 엔트로피를 계산할 수 있다.


- 엔트로피는 로그계산이 들어가 있으므로 컴퓨터가 연산하는 양이 적지 않다. 따라서 엔트로피와 유사한 개념인 지니불순도라는 것을 많이 쓴다.

$$\ G[Y] = \sum_{k=1}^K P(y_k) (1 - P(y_k)) $$

로그계산이 없기 때문에 계산양이 적어서 많이 쓰이는 개념이다.


- 두개의 이산확률변수 X,Y에 대해 결합엔트로피를 구할 수도 있다.

엔트로피와 완전히 똑같은 개념이라고 생각하면되고 다만 x,y가 스칼라가 아닌 2차원 벡터로 바뀐것 뿐이다.

이산확률변수일때

$$\ H[X, Y] = - \sum_{i=1}^{K_x} \sum_{j=1}^{K_y} \,P(x_i, y_j) \log_2 P(x_i, y_j) $$

연속확률변수일때

$$\ H[X, Y] = - \int_{x} \int_{y} \,p(x, y) \log_2 p(x, y)  \; dxdy $$


- 조건부 엔트로피라는 개념도 있다. 상관관계가 있는 두 확률변수  X ,  Y 가 있고  X 의 값을 안다면  Y 의 확률변수가 가질 수 있는 정보의 양을 말한다. 그래서 X에 대한 Y의 조건부 확률을 X,Y 전체로 가중평균을 한 것이다.


- X가 xi는 특정한 값으로 정해졌을때의 Y값의 엔트로피값 고거의 가중평균이다.


- 크로스 엔트로피라는 개념도 있다. 같은 확률변수에 대한 두 개의 추정 확률분포를 비교하는데 주로 쓰이기 때문에 결합 엔트로피처럼 확률변수를 인수로 사용하지 않고 확률분포를 인수로 사용한다.


- 이 크로스 엔트로피를 어디에다 쓸 수 있냐. 우리가 분류를해서 그때 성능을 측정할때 쓸 수 있다.


- 쿨백-라이블러 발산이라는 개념도 있다. 두 확률분포  p(y),  q(y) 의 차이를 정량화하는 방법중 하나이다.

### # 연습 문제

A, B, C, D, E, F, G, H의 8글자로 이루어진 문서가 있고 각각의 글자가 나올 확률이 다음과 같다고 가정하자.

$\ \Big\{ \dfrac{1}{2}, \dfrac{1}{4}, \dfrac{1}{8}, \dfrac{1}{16}, \dfrac{1}{64}, \dfrac{1}{64}, \dfrac{1}{64}, \dfrac{1}{64} \Big\} $

이 문서를 위한 가변 길이 인코딩 방식을 서술하고 한 글자를 인코딩하는데 필요한 평균 비트수를 계산하라.


```python
encoding = {"A" : 0,
            "B" : 10,
            "C" : 110,
            "D" : 1110,
            "E" : 111100,
            "F" : 111101,
            "G" : 111110,
            "H" : 111111}

entropy = -1/2*np.log2(1/2) -1/4*np.log2(1/4) -1/8*np.log2(1/8) -1/16*np.log2(1/16) -4/64*np.log2(1/64)
entropy
```




    2.0
