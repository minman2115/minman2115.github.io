---
layout: post
title: "TF v2를 이용한 회귀모델 구현실습"
tags: [딥러닝]
comments: true
---

.

Deep_Learning_Studynotes_(20190713)

study program : https://www.fastcampus.co.kr/data_camp_deeplearning

실습환경 : Google colab gpu 엔진

[목차]

심플한 형태의 회귀모델을 아래와 같은 유형들에 따라 각각 구현해본다.

- 유형 1. 캐라스를 이용한 선형회귀모델

- 유형 2. TF v2 를 이용한 선형회귀모델

- 유형 3. 로컬에서 텍스트 파일을 로드하여 구현한 선형회귀모델

- 유형 4. 유형 3에 추가적으로 미니배치를 적용한 모델 1

- 유형 5. 유형 3에 추가적으로 미니배치를 적용한 모델 2

- 유형 6. Pima Indians Diabetes Dataset을 이용한 Logistic Regression model 구현

- 유형 7. mnist digit image dataset을 이용한 Logistic Regression model 구현

#### 유형 1. 캐라스를 이용한 선형회귀모델


```python
!pip install tensorflow-gpu==2.0.0
```

    Requirement already satisfied: tensorflow-gpu==2.0.0 in /usr/local/lib/python3.6/dist-packages (2.0.0)
    Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.1.8)
    Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.34.2)
    Requirement already satisfied: tensorflow-estimator<2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (2.0.1)
    Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.27.1)
    Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.12.0)
    Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (3.1.0)
    Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (3.10.0)
    Requirement already satisfied: tensorboard<2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (2.0.2)
    Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.11.2)
    Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.1.0)
    Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.0.8)
    Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.9.0)
    Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.1.0)
    Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.8.1)
    Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.17.5)
    Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.2.2)
    Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==2.0.0) (45.1.0)
    Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (1.0.0)
    Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (2.21.0)
    Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (1.7.2)
    Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (3.2.1)
    Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (0.4.1)
    Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==2.0.0) (2.8.0)
    Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (2.8)
    Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (3.0.4)
    Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (2019.11.28)
    Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (1.24.3)
    Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (0.2.8)
    Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (3.1.1)
    Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (4.0)
    Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (1.3.0)
    Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (0.4.8)
    Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (3.1.0)
    


```python
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

print(tf.__version__)
```

    2.0.0
    


```python
#tf.enable_eager_execution()

# For reproducibility
np.random.seed(777)
```


```python
# inputs
x = np.linspace(0, 1, 100, dtype=np.float32)

# ground truth
#slopes = np.random.normal(1, 0.5, 100).astype(np.float32)
#intercept = 2.

slopes = 1
intercept = np.random.normal(2, 0.2, 100).astype(np.float32)

# outputs
y = x * slopes + intercept
```


```python
slopes
```




    1




```python
intercept
```




    array([1.9063582, 1.835435 , 1.9869239, 1.8573276, 2.1812701, 2.1532474,
           2.1652107, 1.7352635, 1.6495111, 2.2004898, 2.1089618, 2.3790321,
           1.8461285, 1.7193809, 1.8735065, 1.8882253, 1.7533537, 1.9120992,
           2.1829574, 2.053008 , 1.723326 , 2.1371024, 2.0912182, 1.9077251,
           2.01894  , 1.6914377, 2.495874 , 2.0913734, 1.9372255, 2.0042074,
           2.1921587, 2.0116966, 1.9107935, 2.0638394, 2.1682336, 1.6934476,
           1.9436831, 2.3488905, 1.8651522, 2.1176803, 2.3608727, 2.41125  ,
           2.2909164, 1.9723177, 2.0685744, 1.8544763, 1.7192107, 1.7518778,
           1.9113035, 1.990535 , 2.151537 , 1.9695828, 1.9457442, 1.8800321,
           1.5946192, 2.0660684, 1.9338338, 1.9930116, 2.0579495, 1.878746 ,
           1.9463191, 2.2382956, 2.0315228, 2.2349648, 2.264182 , 1.8303926,
           2.1491416, 1.9376757, 1.7897869, 1.7847006, 2.090122 , 2.0817482,
           1.7140917, 2.2034855, 1.9836458, 1.9232163, 1.9523549, 2.0017576,
           2.1040938, 2.080741 , 1.9196435, 1.860551 , 2.1291966, 1.9448934,
           1.924158 , 2.393607 , 2.0404994, 2.0745394, 2.1962693, 2.1472924,
           2.2822602, 1.9746964, 2.1106315, 1.8368928, 2.1077404, 1.556441 ,
           1.8187416, 1.7075751, 1.864487 , 2.3045607], dtype=float32)




```python
plt.scatter(x, y)
plt.plot(x, x * 1 + 2., label="ground truth", c="r")
plt.legend()
plt.show()
```


![aa_7_0](https://user-images.githubusercontent.com/41605276/75512619-660b3b00-5a35-11ea-8a3e-b9d876ac48be.png)



```python
x.dtype
```




    dtype('float32')




```python
y.dtype
```




    dtype('float32')




```python
x.shape
```




    (100,)




```python
y.shape
```




    (100,)




```python
l0 = tf.keras.layers.Dense(units=1, input_shape=[1])
# dense layer라고해서 fully connected layer이다.
# 하나의 입력이 있고 하나의 units(출력의 갯수)가 있는 것이다.
# input shape = 1로 되어 있는데 데이터 100개가 있고 이게 다 들어갈거지만 
# 데이터가 하나씩 스칼라로 들어가기 때문에 1이다.
```


```python
model = tf.keras.models.Sequential([l0])
# 모델.시퀀스에 레이어를 넣어주면 된다.
```


```python
model.summary()
# 레이어가 하나이고 입력으로 들어갈 것은 
# (None, 1)은 None은 항상 배치사이즈이다. 
# 데이터는 배치사이즈 만큼 들어갈거고 우리는 100개 다 집어넣을 것이다.
# 그리고 아웃풋은 항상 1로 나오는 걸로 할 것이다.
# 파라미터는 w랑 b 두개이다.
```

    Model: "sequential"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    dense (Dense)                (None, 1)                 2         
    =================================================================
    Total params: 2
    Trainable params: 2
    Non-trainable params: 0
    _________________________________________________________________
    


```python
model.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.SGD(0.1))

history = model.fit(x, y, epochs=20)
```

    Train on 100 samples
    Epoch 1/20
    100/100 [==============================] - 2s 17ms/sample - loss: 6.6415
    Epoch 2/20
    100/100 [==============================] - 0s 132us/sample - loss: 0.6983
    Epoch 3/20
    100/100 [==============================] - 0s 133us/sample - loss: 0.1895
    Epoch 4/20
    100/100 [==============================] - 0s 127us/sample - loss: 0.1129
    Epoch 5/20
    100/100 [==============================] - 0s 152us/sample - loss: 0.1068
    Epoch 6/20
    100/100 [==============================] - 0s 124us/sample - loss: 0.0977
    Epoch 7/20
    100/100 [==============================] - 0s 115us/sample - loss: 0.0912
    Epoch 8/20
    100/100 [==============================] - 0s 118us/sample - loss: 0.0855
    Epoch 9/20
    100/100 [==============================] - 0s 126us/sample - loss: 0.0824
    Epoch 10/20
    100/100 [==============================] - 0s 101us/sample - loss: 0.0764
    Epoch 11/20
    100/100 [==============================] - 0s 126us/sample - loss: 0.0718
    Epoch 12/20
    100/100 [==============================] - 0s 142us/sample - loss: 0.0702
    Epoch 13/20
    100/100 [==============================] - 0s 168us/sample - loss: 0.0671
    Epoch 14/20
    100/100 [==============================] - 0s 111us/sample - loss: 0.0614
    Epoch 15/20
    100/100 [==============================] - 0s 111us/sample - loss: 0.0588
    Epoch 16/20
    100/100 [==============================] - 0s 122us/sample - loss: 0.0566
    Epoch 17/20
    100/100 [==============================] - 0s 122us/sample - loss: 0.0546
    Epoch 18/20
    100/100 [==============================] - 0s 112us/sample - loss: 0.0535
    Epoch 19/20
    100/100 [==============================] - 0s 169us/sample - loss: 0.0520
    Epoch 20/20
    100/100 [==============================] - 0s 135us/sample - loss: 0.0499
    


```python
plt.xlabel('Epoch Number')
plt.ylabel("Loss Magnitude")
plt.plot(history.history['loss'])
```




    [<matplotlib.lines.Line2D at 0x7feeb6343470>]




![aa_16_1](https://user-images.githubusercontent.com/41605276/75512465-e9785c80-5a34-11ea-85e2-ae4cd763cbbe.png)



```python
model.variables[0].numpy(), model.variables[1].numpy()
# 0번은 w, 1번은 b를 말하는 것이다.
```




    (array([[0.65409386]], dtype=float32), array([2.1847663], dtype=float32))




```python
plt.scatter(x, y)
plt.plot(x, x * model.variables[0][0].numpy() + model.variables[1].numpy(), label="model", c="r")
plt.plot(x, x * 1 + 2., label="ground truth", c="g")
plt.legend()
plt.show()
```


![aa_18_0](https://user-images.githubusercontent.com/41605276/75512475-f006d400-5a34-11ea-8cb2-071a601abc6f.png)


#### 유형 2. TF v2 를 이용한 선형회귀모델


```python
# For reproducibility
np.random.seed(777)
```


```python
# inputs
x = np.linspace(0, 1, 100, dtype=np.float32)

# ground truth
#slopes = np.random.normal(1, 0.5, 100).astype(np.float32)
#intercept = 2.

slopes = 1
intercept = np.random.normal(2, 0.2, 100).astype(np.float32) 
# intercept는 y절편을 말하는건데 가우시안노이즈를 추가했다.
# 평균을 2로하고 표준편차를 0.2로해서 숫자를 100개를 뽑은 것이다.

# outputs
y = x * slopes + intercept

print(slopes,'\n')
print(intercept,'\n')
```

    1 
    
    [1.9063582 1.835435  1.9869239 1.8573276 2.1812701 2.1532474 2.1652107
     1.7352635 1.6495111 2.2004898 2.1089618 2.3790321 1.8461285 1.7193809
     1.8735065 1.8882253 1.7533537 1.9120992 2.1829574 2.053008  1.723326
     2.1371024 2.0912182 1.9077251 2.01894   1.6914377 2.495874  2.0913734
     1.9372255 2.0042074 2.1921587 2.0116966 1.9107935 2.0638394 2.1682336
     1.6934476 1.9436831 2.3488905 1.8651522 2.1176803 2.3608727 2.41125
     2.2909164 1.9723177 2.0685744 1.8544763 1.7192107 1.7518778 1.9113035
     1.990535  2.151537  1.9695828 1.9457442 1.8800321 1.5946192 2.0660684
     1.9338338 1.9930116 2.0579495 1.878746  1.9463191 2.2382956 2.0315228
     2.2349648 2.264182  1.8303926 2.1491416 1.9376757 1.7897869 1.7847006
     2.090122  2.0817482 1.7140917 2.2034855 1.9836458 1.9232163 1.9523549
     2.0017576 2.1040938 2.080741  1.9196435 1.860551  2.1291966 1.9448934
     1.924158  2.393607  2.0404994 2.0745394 2.1962693 2.1472924 2.2822602
     1.9746964 2.1106315 1.8368928 2.1077404 1.556441  1.8187416 1.7075751
     1.864487  2.3045607] 
    
    


```python
plt.scatter(x, y)
plt.plot(x, x * 1 + 2., label="ground truth", c="r")
plt.legend()
plt.show()
```


![aa_22_0](https://user-images.githubusercontent.com/41605276/75512640-715e6680-5a35-11ea-894e-ba1a4920e56b.png)



```python
x.dtype
```




    dtype('float32')




```python
y.dtype
```




    dtype('float32')




```python
x.shape
```




    (100,)




```python
y.shape
```




    (100,)




```python
# Computation
## Variables = Parameters = Weights
w = tf.Variable(.1, tf.float32)
b = tf.Variable(0., tf.float32)

# y = 0.1x + 0 부터 시작하겠다는 의미이다.
```


```python
learning_rate = 0.1

loss_list, w_list, b_list = [], [], []

for epoch in range(20): 
# 데이터가 100가 있는데 미니배치를 하지는 않을 것이고 통째로 20 에포크를 돈다는 것이다.
    with tf.GradientTape() as tape: 
        ## prediction = y_hat = hypothesis
        preds = x * w + b # (100,)
        loss = tf.reduce_mean(tf.square(preds - y))
        
    w_grad, b_grad = tape.gradient(loss, [w, b])
    # 그레디언트로 w와 b로 미분해라
    w.assign_sub(learning_rate * w_grad)
    # 이전의 w값에다가 러닝레이트 곱하기 w그레디언트 계산한 것을 할당해라
    b.assign_sub(learning_rate * b_grad)
    print(epoch+1, "\t", loss.numpy(), "\t", w.numpy(), "\t", b.numpy())
    loss_list.append(loss.numpy())
    w_list.append(w.numpy())
    b_list.append(b.numpy())
```

    1 	 6.1178756 	 0.36076 	 0.49028212
    2 	 3.4251547 	 0.55502 	 0.85643184
    3 	 1.9249654 	 0.6996489 	 1.1299256
    4 	 1.0891632 	 0.80723786 	 1.3342577
    5 	 0.6235067 	 0.88718474 	 1.4869645
    6 	 0.3640671 	 0.94650424 	 1.6011353
    7 	 0.21951582 	 0.9904321 	 1.6865399
    8 	 0.13897178 	 1.0228761 	 1.7504709
    9 	 0.09408791 	 1.0467533 	 1.7983712
    10 	 0.0690715 	 1.0642405 	 1.8343037
    11 	 0.05512394 	 1.0769627 	 1.8613011
    12 	 0.047343373 	 1.0861328 	 1.8816267
    13 	 0.042998843 	 1.0926559 	 1.8969703
    14 	 0.04056885 	 1.0972075 	 1.9085928
    15 	 0.039205775 	 1.100292 	 1.9174356
    16 	 0.03843734 	 1.1022855 	 1.9242015
    17 	 0.038000435 	 1.1034689 	 1.9294147
    18 	 0.03774846 	 1.1040516 	 1.933467
    19 	 0.037599746 	 1.10419 	 1.9366506
    20 	 0.03750879 	 1.1040008 	 1.9391836
    


```python
plt.plot(loss_list, label="loss")
plt.plot(w_list, label="w")
plt.plot(b_list, label="b")
plt.legend()
plt.show()
```


![aa_29_0](https://user-images.githubusercontent.com/41605276/75512488-f85f0f00-5a34-11ea-94a7-1213dad58220.png)



```python
# 빨간색 선이 모델에서 예측한 선, 초록색이 정답
plt.scatter(x, y)
plt.plot(x, x * w_list[-1] + b_list[-1], label="model", c="r")
plt.plot(x, x * 1 + 2., label="ground truth", c="g")
plt.legend()
plt.show()
```


![aa_30_0](https://user-images.githubusercontent.com/41605276/75512498-06ad2b00-5a35-11ea-971e-6f1da9aa16a7.png)


#### 유형 3. 로컬에서 텍스트 파일을 로드하여 구현한 선형회귀모델


```python
from google.colab import drive
drive.mount('/content/gdrive')

#DATA_FILE = './data/birth_life_2010.txt'
DATA_FILE = '/content/gdrive/My Drive/TensorFlow_Training_13th/data/birth_life_2010.txt'
```

    Mounted at /content/gdrive
    


```python
def read_birth_life_data(filename):
    """
    Read in birth_life_2010.txt and return:
    data in the form of NumPy array
    n_samples: number of samples
    """
    text = open(filename, 'r').readlines()[1:]
    data = [line[:-1].split('\t') for line in text]
    births = [float(line[1]) for line in data]
    lifes = [float(line[2]) for line in data]
    data = list(zip(births, lifes))
    n_samples = len(data)
    data = np.asarray(data, dtype=np.float32)
    return data, n_samples
```


```python
data, n_samples = read_birth_life_data(DATA_FILE)
data[:3]
```




    array([[ 1.822  , 74.82825],
           [ 3.869  , 70.81949],
           [ 3.911  , 72.15066]], dtype=float32)




```python
w = tf.Variable(0.1, tf.float32)
b = tf.Variable(0., tf.float32)
```


```python
learning_rate = 0.001
optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)
```


```python
for epoch in range(100):
    total_loss = 0.
    for x, y in data:
        with tf.GradientTape() as tape:
            hypothesis = w * x + b
            loss = tf.reduce_mean(tf.square(hypothesis - y))
        grads = tape.gradient(loss, [w, b])
        optimizer.apply_gradients(grads_and_vars=zip(grads, [w, b]))
        # sgd라는 옵티마이저를 이용해서 그레디언트를 업데이트 해달라는 의미이고
        # grads and vars라는 곳에는 그레디언트랑 weight 바이어스를 zip으로 묶어서 넣어주면 된다.
        total_loss += loss
    print('Epoch {0}: {1}'.format(epoch+1, total_loss/n_samples))
```

    Epoch 1: 1656.550048828125
    Epoch 2: 957.05224609375
    Epoch 3: 845.3574829101562
    Epoch 4: 751.33642578125
    Epoch 5: 668.1957397460938
    Epoch 6: 594.6158447265625
    Epoch 7: 529.498779296875
    Epoch 8: 471.8717956542969
    Epoch 9: 420.87432861328125
    Epoch 10: 375.7457275390625
    Epoch 11: 335.8111877441406
    Epoch 12: 300.47357177734375
    Epoch 13: 269.2049865722656
    Epoch 14: 241.5377655029297
    Epoch 15: 217.05789184570312
    Epoch 16: 195.3992156982422
    Epoch 17: 176.23704528808594
    Epoch 18: 159.28453063964844
    Epoch 19: 144.2869415283203
    Epoch 20: 131.0201873779297
    Epoch 21: 119.28470611572266
    Epoch 22: 108.90458679199219
    Epoch 23: 99.72322845458984
    Epoch 24: 91.60327911376953
    Epoch 25: 84.4225845336914
    Epoch 26: 78.07245635986328
    Epoch 27: 72.45730590820312
    Epoch 28: 67.49275970458984
    Epoch 29: 63.103614807128906
    Epoch 30: 59.22303771972656
    Epoch 31: 55.793113708496094
    Epoch 32: 52.76158905029297
    Epoch 33: 50.08254623413086
    Epoch 34: 47.715213775634766
    Epoch 35: 45.623287200927734
    Epoch 36: 43.77528762817383
    Epoch 37: 42.142906188964844
    Epoch 38: 40.701255798339844
    Epoch 39: 39.42827224731445
    Epoch 40: 38.304073333740234
    Epoch 41: 37.31173324584961
    Epoch 42: 36.435970306396484
    Epoch 43: 35.66320037841797
    Epoch 44: 34.98141860961914
    Epoch 45: 34.38019561767578
    Epoch 46: 33.8498649597168
    Epoch 47: 33.38246536254883
    Epoch 48: 32.97057342529297
    Epoch 49: 32.60768508911133
    Epoch 50: 32.288116455078125
    Epoch 51: 32.00660705566406
    Epoch 52: 31.758989334106445
    Epoch 53: 31.54110336303711
    Epoch 54: 31.34950065612793
    Epoch 55: 31.181076049804688
    Epoch 56: 31.033130645751953
    Epoch 57: 30.903223037719727
    Epoch 58: 30.789226531982422
    Epoch 59: 30.689340591430664
    Epoch 60: 30.601768493652344
    Epoch 61: 30.525074005126953
    Epoch 62: 30.457931518554688
    Epoch 63: 30.399337768554688
    Epoch 64: 30.348064422607422
    Epoch 65: 30.30340003967285
    Epoch 66: 30.264480590820312
    Epoch 67: 30.23057746887207
    Epoch 68: 30.20111083984375
    Epoch 69: 30.175609588623047
    Epoch 70: 30.153446197509766
    Epoch 71: 30.134305953979492
    Epoch 72: 30.11781883239746
    Epoch 73: 30.10361671447754
    Epoch 74: 30.09145164489746
    Epoch 75: 30.081008911132812
    Epoch 76: 30.072124481201172
    Epoch 77: 30.06454086303711
    Epoch 78: 30.05818748474121
    Epoch 79: 30.05280303955078
    Epoch 80: 30.048303604125977
    Epoch 81: 30.044593811035156
    Epoch 82: 30.041576385498047
    Epoch 83: 30.0390682220459
    Epoch 84: 30.037050247192383
    Epoch 85: 30.035449981689453
    Epoch 86: 30.034282684326172
    Epoch 87: 30.033401489257812
    Epoch 88: 30.032764434814453
    Epoch 89: 30.03238868713379
    Epoch 90: 30.03215980529785
    Epoch 91: 30.03208351135254
    Epoch 92: 30.032203674316406
    Epoch 93: 30.03240394592285
    Epoch 94: 30.03264045715332
    Epoch 95: 30.03304100036621
    Epoch 96: 30.03343391418457
    Epoch 97: 30.033906936645508
    Epoch 98: 30.034433364868164
    Epoch 99: 30.034929275512695
    Epoch 100: 30.0355281829834
    


```python
# plot the results
plt.plot(data[:,0], data[:,1], 'bo', label='Real data')
plt.plot(data[:,0], data[:,0] * w.numpy() + b.numpy(), 'r', label='Predicted data')
plt.legend()
plt.show()
```


![aa_38_0](https://user-images.githubusercontent.com/41605276/75512518-13ca1a00-5a35-11ea-95f8-4ec17ed95d50.png)


#### 유형 4. 유형 3에 추가적으로 미니배치를 적용한 모델 1


```python
data, n_samples = read_birth_life_data(DATA_FILE)
```


```python
w = tf.Variable(0.1, tf.float32)
b = tf.Variable(0., tf.float32)
```


```python
learning_rate = 0.01
optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)
```


```python
batch_size = 10
n_epoch = 100

total_steps = int(n_samples/batch_size)
for epoch in range(n_epoch):
    total_loss = 0.
    for i in range(total_steps):
        x = data[i*batch_size:(i+1)*batch_size, 0]
        y = data[i*batch_size:(i+1)*batch_size, 1]
        with tf.GradientTape() as tape:
            hypothesis = w * x + b
            loss = tf.reduce_mean(tf.square(hypothesis - y))
        grads = tape.gradient(loss, [w, b])
        optimizer.apply_gradients(grads_and_vars=zip(grads, [w, b]))
        total_loss += loss / total_steps
    print('Epoch {0}: {1}'.format(epoch+1, total_loss))
```

    Epoch 1: 1770.02099609375
    Epoch 2: 1039.58984375
    Epoch 3: 919.225830078125
    Epoch 4: 815.1405029296875
    Epoch 5: 723.2711181640625
    Epoch 6: 642.1721801757812
    Epoch 7: 570.5804443359375
    Epoch 8: 507.3812255859375
    Epoch 9: 451.5906677246094
    Epoch 10: 402.33984375
    Epoch 11: 358.8621520996094
    Epoch 12: 320.48052978515625
    Epoch 13: 286.5976867675781
    Epoch 14: 256.68603515625
    Epoch 15: 230.28004455566406
    Epoch 16: 206.96868896484375
    Epoch 17: 186.38916015625
    Epoch 18: 168.22128295898438
    Epoch 19: 152.18214416503906
    Epoch 20: 138.02249145507812
    Epoch 21: 125.52175903320312
    Epoch 22: 114.48564910888672
    Epoch 23: 104.74223327636719
    Epoch 24: 96.1402359008789
    Epoch 25: 88.54573822021484
    Epoch 26: 81.84073638916016
    Epoch 27: 75.92105102539062
    Epoch 28: 70.69452667236328
    Epoch 29: 66.07992553710938
    Epoch 30: 62.005653381347656
    Epoch 31: 58.40829849243164
    Epoch 32: 55.23198318481445
    Epoch 33: 52.427513122558594
    Epoch 34: 49.95112991333008
    Epoch 35: 47.76454162597656
    Epoch 36: 45.83372116088867
    Epoch 37: 44.12874221801758
    Epoch 38: 42.62318420410156
    Epoch 39: 41.29365539550781
    Epoch 40: 40.11951446533203
    Epoch 41: 39.082672119140625
    Epoch 42: 38.16698455810547
    Epoch 43: 37.35821533203125
    Epoch 44: 36.643943786621094
    Epoch 45: 36.0130729675293
    Epoch 46: 35.455810546875
    Epoch 47: 34.9636116027832
    Epoch 48: 34.52882385253906
    Epoch 49: 34.14472579956055
    Epoch 50: 33.80543899536133
    Epoch 51: 33.50569534301758
    Epoch 52: 33.2408447265625
    Epoch 53: 33.00681686401367
    Epoch 54: 32.80006790161133
    Epoch 55: 32.61733627319336
    Epoch 56: 32.455875396728516
    Epoch 57: 32.313167572021484
    Epoch 58: 32.18703079223633
    Epoch 59: 32.07549285888672
    Epoch 60: 31.976947784423828
    Epoch 61: 31.889814376831055
    Epoch 62: 31.812776565551758
    Epoch 63: 31.744647979736328
    Epoch 64: 31.684396743774414
    Epoch 65: 31.631132125854492
    Epoch 66: 31.583980560302734
    Epoch 67: 31.542285919189453
    Epoch 68: 31.505399703979492
    Epoch 69: 31.472734451293945
    Epoch 70: 31.443845748901367
    Epoch 71: 31.418251037597656
    Epoch 72: 31.395597457885742
    Epoch 73: 31.375545501708984
    Epoch 74: 31.357770919799805
    Epoch 75: 31.342031478881836
    Epoch 76: 31.32808494567871
    Epoch 77: 31.315719604492188
    Epoch 78: 31.30475616455078
    Epoch 79: 31.295061111450195
    Epoch 80: 31.28642463684082
    Epoch 81: 31.278789520263672
    Epoch 82: 31.271989822387695
    Epoch 83: 31.265966415405273
    Epoch 84: 31.260616302490234
    Epoch 85: 31.255868911743164
    Epoch 86: 31.251630783081055
    Epoch 87: 31.247886657714844
    Epoch 88: 31.244539260864258
    Epoch 89: 31.2415771484375
    Epoch 90: 31.23893165588379
    Epoch 91: 31.236572265625
    Epoch 92: 31.234472274780273
    Epoch 93: 31.232601165771484
    Epoch 94: 31.230918884277344
    Epoch 95: 31.22943687438965
    Epoch 96: 31.22810173034668
    Epoch 97: 31.2269229888916
    Epoch 98: 31.22586441040039
    Epoch 99: 31.224899291992188
    Epoch 100: 31.224042892456055
    


```python
# plot the results
plt.plot(data[:,0], data[:,1], 'bo', label='Real data')
plt.plot(data[:,0], data[:,0] * w.numpy() + b.numpy(), 'r', label='Predicted data')
plt.legend()
plt.show()
```


![aa_44_0](https://user-images.githubusercontent.com/41605276/75512534-1c225500-5a35-11ea-9f20-05b876dd75c3.png)


#### 유형 5. 유형 3에 추가적으로 미니배치를 적용한 모델 2

** 미니배치를 dataset에 적용한 경우


```python
data, n_samples = read_birth_life_data(DATA_FILE)
```


```python
dataset = tf.data.Dataset.from_tensor_slices((data[:,0], data[:,1]))
dataset = dataset.shuffle(1000).batch(10)
```


```python
w = tf.Variable(0.1, tf.float32)
b = tf.Variable(0., tf.float32)
```


```python
learning_rate = 0.01
optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)
```


```python
total_steps = int(n_samples/batch_size)
for epoch in range(n_epoch):
    total_loss = 0.
    for x, y in dataset:        
        with tf.GradientTape() as tape:
            hypothesis = w * x + b
            loss = tf.reduce_mean(tf.square(hypothesis - y))
        grads = tape.gradient(loss, [w, b])
        optimizer.apply_gradients(grads_and_vars=zip(grads, [w, b]))
        total_loss += loss / total_steps
    print('Epoch {0}: {1}'.format(epoch+1, total_loss))
```

    Epoch 1: 1687.0469970703125
    Epoch 2: 1128.005615234375
    Epoch 3: 986.34619140625
    Epoch 4: 856.6626586914062
    Epoch 5: 750.8270874023438
    Epoch 6: 646.872802734375
    Epoch 7: 570.501953125
    Epoch 8: 496.1466979980469
    Epoch 9: 429.7283935546875
    Epoch 10: 370.2839050292969
    Epoch 11: 333.27154541015625
    Epoch 12: 291.93206787109375
    Epoch 13: 259.6501770019531
    Epoch 14: 228.00595092773438
    Epoch 15: 204.34674072265625
    Epoch 16: 179.04010009765625
    Epoch 17: 160.6471405029297
    Epoch 18: 143.68077087402344
    Epoch 19: 127.80083465576172
    Epoch 20: 114.70220184326172
    Epoch 21: 104.09292602539062
    Epoch 22: 93.3449935913086
    Epoch 23: 86.05628967285156
    Epoch 24: 79.77397918701172
    Epoch 25: 72.7884750366211
    Epoch 26: 66.23856353759766
    Epoch 27: 61.97942352294922
    Epoch 28: 57.610504150390625
    Epoch 29: 53.88559341430664
    Epoch 30: 51.31080627441406
    Epoch 31: 48.9115104675293
    Epoch 32: 46.83890914916992
    Epoch 33: 44.05195999145508
    Epoch 34: 42.711788177490234
    Epoch 35: 40.67366027832031
    Epoch 36: 39.505313873291016
    Epoch 37: 38.51683044433594
    Epoch 38: 37.426124572753906
    Epoch 39: 36.65864562988281
    Epoch 40: 35.190853118896484
    Epoch 41: 35.46449661254883
    Epoch 42: 34.323795318603516
    Epoch 43: 34.299720764160156
    Epoch 44: 33.40859603881836
    Epoch 45: 33.41132354736328
    Epoch 46: 33.05862045288086
    Epoch 47: 32.75035858154297
    Epoch 48: 32.46639633178711
    Epoch 49: 31.823938369750977
    Epoch 50: 31.63040542602539
    Epoch 51: 31.80669403076172
    Epoch 52: 31.49301528930664
    Epoch 53: 31.98589324951172
    Epoch 54: 31.545286178588867
    Epoch 55: 31.433839797973633
    Epoch 56: 31.795259475708008
    Epoch 57: 31.318918228149414
    Epoch 58: 30.481916427612305
    Epoch 59: 31.48868179321289
    Epoch 60: 31.44417953491211
    Epoch 61: 31.102758407592773
    Epoch 62: 31.187904357910156
    Epoch 63: 30.946733474731445
    Epoch 64: 30.729522705078125
    Epoch 65: 30.987564086914062
    Epoch 66: 31.358787536621094
    Epoch 67: 30.56254005432129
    Epoch 68: 30.804153442382812
    Epoch 69: 31.33156967163086
    Epoch 70: 30.798065185546875
    Epoch 71: 31.445398330688477
    Epoch 72: 30.706581115722656
    Epoch 73: 30.681434631347656
    Epoch 74: 30.70384979248047
    Epoch 75: 30.711322784423828
    Epoch 76: 30.92323875427246
    Epoch 77: 30.48532485961914
    Epoch 78: 30.515974044799805
    Epoch 79: 30.906641006469727
    Epoch 80: 30.630022048950195
    Epoch 81: 30.377601623535156
    Epoch 82: 30.863910675048828
    Epoch 83: 30.90697479248047
    Epoch 84: 30.843477249145508
    Epoch 85: 30.81010627746582
    Epoch 86: 30.876224517822266
    Epoch 87: 31.176042556762695
    Epoch 88: 30.675006866455078
    Epoch 89: 30.230438232421875
    Epoch 90: 30.862018585205078
    Epoch 91: 30.630569458007812
    Epoch 92: 30.89525604248047
    Epoch 93: 30.611726760864258
    Epoch 94: 30.686668395996094
    Epoch 95: 30.65570068359375
    Epoch 96: 31.035356521606445
    Epoch 97: 30.71820068359375
    Epoch 98: 31.294849395751953
    Epoch 99: 29.821693420410156
    Epoch 100: 31.17538070678711
    


```python
# plot the results
plt.plot(data[:,0], data[:,1], 'bo', label='Real data')
plt.plot(data[:,0], data[:,0] * w.numpy() + b.numpy(), 'r', label='Predicted data')
plt.legend()
plt.show()
```


![aa_51_0](https://user-images.githubusercontent.com/41605276/75512545-26dcea00-5a35-11ea-8424-bf12da69358c.png)


#### 유형 6. Pima Indians Diabetes Dataset을 이용한 Logistic Regression model 구현

#### Pima Indians Diabetes Dataset for Binary Classification

This dataset describes the medical records for Pima Indians and whether or not each patient will have an onset of diabetes within five years.

This dataset can be downloaded from https://www.kaggle.com/kumargh/pimaindiansdiabetescsv

이 dataset의 몇가지 주요 항목을 살펴보면 다음과 같습니다

- 인스턴스 수 : 768개
- 속성 수 : 8가지
- 클래스 수 : 2가지

8가지 속성(1번~8번)과 결과(9번)의 상세 내용은 다음과 같습니다.

1. 임신 횟수
2. 경구 포도당 내성 검사에서 2시간 동안의 혈장 포도당 농도
3. 이완기 혈압 (mm Hg)
4. 삼두근 피부 두겹 두께 (mm)
5. 2 시간 혈청 인슐린 (mu U/ml)
6. 체질량 지수
7. 당뇨 직계 가족력
8. 나이 (세)
9. 5년 이내 당뇨병이 발병 여부


```python
#tf.enable_eager_execution()

#DATA_FILE = './data/pima-indians-diabetes.csv'
DATA_FILE = '/content/gdrive/My Drive/TensorFlow_Training_13th/data/pima-indians-diabetes.csv'
```


```python
xy = np.loadtxt(DATA_FILE, delimiter=',', dtype=np.float32)
x_train = xy[:, 0:-1]
y_train = xy[:, [-1]]
# 여기서 -1이라는 것은 맨 마지막 부터인것은 동일하나 대괄호가 들어가면 디멘전이 달라지게 된다.

print(x_train.shape, y_train.shape)
print(xy)
```

    (768, 8) (768, 1)
    [[  6.    148.     72.    ...   0.627  50.      1.   ]
     [  1.     85.     66.    ...   0.351  31.      0.   ]
     [  8.    183.     64.    ...   0.672  32.      1.   ]
     ...
     [  5.    121.     72.    ...   0.245  30.      0.   ]
     [  1.    126.     60.    ...   0.349  47.      1.   ]
     [  1.     93.     70.    ...   0.315  23.      0.   ]]
    


```python
# 피쳐들의 스케일이 다르면 가중치를 곱해져서 더할건데 이때 스케일이 큰숫자가 가장 영향력이 커지기 때문에
# 모든데이터에서 이 데이터들 중에 미니멈값을 뺀 숫자를 맥시멈에서 미니멈을 뺀 것을 나눠주게 된다.
# 통상 피쳐들을 0 ~ 1 사이의 값으로 노멀라이징하는 전처리를 해주게 된다.

# 참고로 다음과 같은 패키지를 임포트하여도 됨
# from sklearn.preprocessing import MinMaxScaler

def MinMaxScaler(data):
    ''' Min Max Normalization
    Parameters
    ----------
    data : numpy.ndarray
        input data to be normalized
        shape: [Batch size, dimension]
    Returns
    ----------
    data : numpy.ndarry
        normalized data
        shape: [Batch size, dimension]
    References
    ----------
    .. [1] http://sebastianraschka.com/Articles/2014_about_feature_scaling.html
    '''
    numerator = data - np.min(data, 0)
    denominator = np.max(data, 0) - np.min(data, 0)
    # noise term prevents the zero division
    return numerator / (denominator + 1e-7)
  
  # 여기서 10의 -7승이 왜 들어갔냐면 모든숫자가 다 같으면 맥시멈 - 미니멈이 0이 되기 때문에 그것을 방지하기 위함이다.
```


```python
x_train = MinMaxScaler(x_train)
x_train
```




    array([[0.3529412 , 0.74371856, 0.59016395, ..., 0.5007451 , 0.23441501,
            0.48333332],
           [0.05882353, 0.42713568, 0.5409836 , ..., 0.39642325, 0.11656704,
            0.16666667],
           [0.47058824, 0.919598  , 0.52459013, ..., 0.34724292, 0.25362936,
            0.18333334],
           ...,
           [0.29411766, 0.6080402 , 0.59016395, ..., 0.390462  , 0.07130657,
            0.15      ],
           [0.05882353, 0.63316584, 0.4918033 , ..., 0.44858423, 0.11571307,
            0.43333334],
           [0.05882353, 0.46733668, 0.57377046, ..., 0.45305514, 0.10119555,
            0.03333334]], dtype=float32)




```python
batch_size = x_train.shape[0]
n_epoch = 1000
learning_rate = 0.1
```


```python
dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(1000).batch(batch_size)
```


```python
w = tf.Variable(tf.random_normal_initializer()([8, 1]))
# 8행 1열 매트릭스가 되야지 길이가 8짜리 x가 들어오면 곱할 수 있기 때문이다.
# 그래서 8값에 입력의 길이를 보통 넣어주게 된다.

b = tf.Variable(tf.random_normal_initializer()([1]))
```


```python
def logistic_regression(inputs):
    hypothesis = tf.keras.activations.sigmoid(tf.matmul(inputs, w) + b)
    return hypothesis
```


```python
def loss_fn(inputs, labels):
    hypothesis = logistic_regression(inputs)
    loss = tf.reduce_mean(tf.keras.losses.binary_crossentropy(labels, hypothesis))
    return loss
```


```python
def grad(inputs, labels):
    hypothesis = logistic_regression(inputs)
    with tf.GradientTape() as tape:
        loss = loss_fn(inputs, labels)
    grads = tape.gradient(loss, [w, b])
    return grads
```


```python
def accuracy_fn(inputs, labels):
    hypothesis = logistic_regression(inputs)
    prediction = tf.cast(hypothesis > 0.5, dtype=tf.float32)
    # true는 1.0, false는 0.0을 리턴해줄 것이다.
    
    accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, labels), dtype=tf.float32))
    return accuracy
```


```python
optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)
```


```python
total_steps = int(x_train.shape[0]/batch_size)
for epoch in range(n_epoch):
    total_loss = 0.
    for x, y in dataset: 
        grads = grad(x, y)        
        optimizer.apply_gradients(grads_and_vars=zip(grads, [w, b]))
        loss = loss_fn(x, y)
        total_loss += loss / total_steps
    if (epoch+1) % 10 == 0:
        print('Epoch {0}: {1:.8f}'.format(epoch+1, total_loss))
```

    Epoch 10: 0.67337918
    Epoch 20: 0.66236740
    Epoch 30: 0.65734679
    Epoch 40: 0.65440542
    Epoch 50: 0.65221131
    Epoch 60: 0.65030080
    Epoch 70: 0.64850968
    Epoch 80: 0.64677870
    Epoch 90: 0.64508635
    Epoch 100: 0.64342451
    Epoch 110: 0.64178997
    Epoch 120: 0.64018112
    Epoch 130: 0.63859731
    Epoch 140: 0.63703781
    Epoch 150: 0.63550216
    Epoch 160: 0.63398975
    Epoch 170: 0.63250029
    Epoch 180: 0.63103324
    Epoch 190: 0.62958819
    Epoch 200: 0.62816471
    Epoch 210: 0.62676245
    Epoch 220: 0.62538087
    Epoch 230: 0.62401962
    Epoch 240: 0.62267846
    Epoch 250: 0.62135679
    Epoch 260: 0.62005430
    Epoch 270: 0.61877066
    Epoch 280: 0.61750549
    Epoch 290: 0.61625844
    Epoch 300: 0.61502916
    Epoch 310: 0.61381733
    Epoch 320: 0.61262256
    Epoch 330: 0.61144465
    Epoch 340: 0.61028308
    Epoch 350: 0.60913771
    Epoch 360: 0.60800821
    Epoch 370: 0.60689431
    Epoch 380: 0.60579556
    Epoch 390: 0.60471171
    Epoch 400: 0.60364264
    Epoch 410: 0.60258800
    Epoch 420: 0.60154754
    Epoch 430: 0.60052085
    Epoch 440: 0.59950781
    Epoch 450: 0.59850812
    Epoch 460: 0.59752166
    Epoch 470: 0.59654802
    Epoch 480: 0.59558707
    Epoch 490: 0.59463841
    Epoch 500: 0.59370208
    Epoch 510: 0.59277773
    Epoch 520: 0.59186500
    Epoch 530: 0.59096402
    Epoch 540: 0.59007430
    Epoch 550: 0.58919567
    Epoch 560: 0.58832806
    Epoch 570: 0.58747119
    Epoch 580: 0.58662480
    Epoch 590: 0.58578879
    Epoch 600: 0.58496302
    Epoch 610: 0.58414721
    Epoch 620: 0.58334130
    Epoch 630: 0.58254510
    Epoch 640: 0.58175832
    Epoch 650: 0.58098084
    Epoch 660: 0.58021265
    Epoch 670: 0.57945353
    Epoch 680: 0.57870311
    Epoch 690: 0.57796150
    Epoch 700: 0.57722843
    Epoch 710: 0.57650387
    Epoch 720: 0.57578748
    Epoch 730: 0.57507938
    Epoch 740: 0.57437915
    Epoch 750: 0.57368702
    Epoch 760: 0.57300246
    Epoch 770: 0.57232559
    Epoch 780: 0.57165617
    Epoch 790: 0.57099420
    Epoch 800: 0.57033950
    Epoch 810: 0.56969196
    Epoch 820: 0.56905138
    Epoch 830: 0.56841779
    Epoch 840: 0.56779093
    Epoch 850: 0.56717086
    Epoch 860: 0.56655741
    Epoch 870: 0.56595045
    Epoch 880: 0.56534976
    Epoch 890: 0.56475550
    Epoch 900: 0.56416738
    Epoch 910: 0.56358540
    Epoch 920: 0.56300944
    Epoch 930: 0.56243938
    Epoch 940: 0.56187516
    Epoch 950: 0.56131661
    Epoch 960: 0.56076378
    Epoch 970: 0.56021655
    Epoch 980: 0.55967480
    Epoch 990: 0.55913836
    Epoch 1000: 0.55860740
    


```python
print('Accuracy: {}'.format(accuracy_fn(x_train, y_train)))
```

    Accuracy: 0.72265625
    

#### 유형 7. mnist digit image dataset을 이용한 Logistic Regression model 구현

## Importing Libraries


```python
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.utils import to_categorical
import numpy as np
import matplotlib.pyplot as plt
import os

print(tf.__version__)
print(keras.__version__)
```

    2.0.0
    2.2.4-tf
    

## Enable Eager Mode


```python
if tf.__version__ < '2.0.0':
    tf.enable_eager_execution()
```

## Hyper Parameters


```python
learning_rate = 0.001
training_epochs = 20
batch_size = 100
n_class = 10
```

## MNIST/Fashion MNIST Data


```python
## MNIST Dataset #########################################################
mnist = keras.datasets.mnist
class_names = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']
##########################################################################

## Fashion MNIST Dataset #################################################
# mnist = keras.datasets.fashion_mnist
# class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']
##########################################################################
```

## Datasets


```python
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()  
```


```python
type(train_images), type(train_labels)
```




    (numpy.ndarray, numpy.ndarray)




```python
train_images.shape, train_labels.shape
## 트레이닝 이미지는 6만개, 클래스는 10개
```




    ((60000, 28, 28), (60000,))




```python
test_images.shape, test_labels.shape
```




    ((10000, 28, 28), (10000,))




```python
n_train = train_images.shape[0]
print(n_train)
n_test = test_images.shape[0]
print(n_test)
```

    60000
    10000
    


```python
plt.figure()
plt.imshow(train_images[0], cmap=plt.cm.binary)
plt.colorbar()
```




    <matplotlib.colorbar.Colorbar at 0x7fee975bd1d0>




![aa_83_1](https://user-images.githubusercontent.com/41605276/75512586-483dd600-5a35-11ea-8674-885af594a570.png)



```python
plt.figure(figsize=(15,15))
for i in range(25):
    plt.subplot(5,5,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(train_images[i], cmap=plt.cm.binary)
    plt.xlabel(class_names[train_labels[i]])
```


![aa_84_0](https://user-images.githubusercontent.com/41605276/75512594-4ffd7a80-5a35-11ea-83bb-b2555c83769b.png)



```python
train_images = train_images.astype(np.float32) / 255.
test_images = test_images.astype(np.float32) / 255.
    
train_labels = to_categorical(train_labels, n_class)
test_labels = to_categorical(test_labels, n_class)    
    
train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(buffer_size=100000).batch(batch_size)
test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(batch_size)
```

## Model Function

MLP로 Classification 하는 모델구현


```python
def create_model():
    model = keras.Sequential()
    model.add(keras.layers.Flatten(input_shape=(28,28)))
    model.add(keras.layers.Dense(10, kernel_initializer=tf.keras.initializers.RandomNormal(),activation='softmax'))    
    return model
```


```python
model = create_model()
model.summary()
```

    Model: "sequential_2"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    flatten_1 (Flatten)          (None, 784)               0         
    _________________________________________________________________
    dense_2 (Dense)              (None, 10)                7850      
    =================================================================
    Total params: 7,850
    Trainable params: 7,850
    Non-trainable params: 0
    _________________________________________________________________
    

## Loss Function




```python
def loss_fn(model, images, labels):
    predictions = model(images, training=True)
    loss = tf.reduce_mean(keras.losses.categorical_crossentropy(labels, predictions))   
    return loss
```

## Calculating Gradient & Updating Weights


```python
def train(model, images, labels):
    with tf.GradientTape() as tape:
        loss = loss_fn(model, images, labels)
    grads = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(grads, model.trainable_variables))
```

## Caculating Model's Accuracy


```python
def evaluate(model, images, labels):
    predictions = model(images, training=False)
    correct_prediction = tf.equal(tf.argmax(predictions, 1), tf.argmax(labels, 1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))    
    return accuracy
```

## Optimizer


```python
optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
```

## Training


```python
# train my model
print('Learning started. It takes sometime.')
for epoch in range(training_epochs):
    avg_loss = 0.
    avg_train_acc = 0.
    avg_test_acc = 0.
    train_step = 0
    test_step = 0
    
    for images, labels in train_dataset:
        train(model,images, labels)
        loss = loss_fn(model, images, labels)
        acc = evaluate(model, images, labels)        
        avg_loss = avg_loss + loss
        avg_train_acc = avg_train_acc + acc
        train_step += 1
    avg_loss = avg_loss / train_step
    avg_train_acc = avg_train_acc / train_step
    
    for images, labels in test_dataset:        
        acc = evaluate(model, images, labels)        
        avg_test_acc = avg_test_acc + acc
        test_step += 1    
    avg_test_acc = avg_test_acc / test_step    

    print('Epoch:', '{}'.format(epoch + 1), 'loss =', '{:.8f}'.format(avg_loss), 
          'train accuracy = ', '{:.4f}'.format(avg_train_acc), 
          'test accuracy = ', '{:.4f}'.format(avg_test_acc))


print('Learning Finished!')
```

    Learning started. It takes sometime.
    Epoch: 1 loss = 0.62760425 train accuracy =  0.8413 test accuracy =  0.9035
    Epoch: 2 loss = 0.34553069 train accuracy =  0.9061 test accuracy =  0.9153
    Epoch: 3 loss = 0.30822840 train accuracy =  0.9151 test accuracy =  0.9204
    Epoch: 4 loss = 0.29064599 train accuracy =  0.9193 test accuracy =  0.9223
    Epoch: 5 loss = 0.28040126 train accuracy =  0.9218 test accuracy =  0.9223
    Epoch: 6 loss = 0.27306738 train accuracy =  0.9245 test accuracy =  0.9234
    Epoch: 7 loss = 0.26793337 train accuracy =  0.9253 test accuracy =  0.9251
    Epoch: 8 loss = 0.26366964 train accuracy =  0.9268 test accuracy =  0.9269
    Epoch: 9 loss = 0.26021346 train accuracy =  0.9280 test accuracy =  0.9265
    Epoch: 10 loss = 0.25724363 train accuracy =  0.9289 test accuracy =  0.9275
    Epoch: 11 loss = 0.25470653 train accuracy =  0.9294 test accuracy =  0.9273
    Epoch: 12 loss = 0.25237504 train accuracy =  0.9304 test accuracy =  0.9266
    Epoch: 13 loss = 0.25095078 train accuracy =  0.9305 test accuracy =  0.9278
    Epoch: 14 loss = 0.24927010 train accuracy =  0.9318 test accuracy =  0.9272
    Epoch: 15 loss = 0.24801041 train accuracy =  0.9311 test accuracy =  0.9288
    Epoch: 16 loss = 0.24623889 train accuracy =  0.9323 test accuracy =  0.9269
    Epoch: 17 loss = 0.24511783 train accuracy =  0.9326 test accuracy =  0.9277
    Epoch: 18 loss = 0.24425504 train accuracy =  0.9330 test accuracy =  0.9255
    Epoch: 19 loss = 0.24327569 train accuracy =  0.9326 test accuracy =  0.9270
    Epoch: 20 loss = 0.24202774 train accuracy =  0.9330 test accuracy =  0.9275
    Learning Finished!
    


```python
def plot_image(i, predictions_array, true_label, img):
    predictions_array, true_label, img = predictions_array[i], true_label[i], img[i]
    plt.grid(False)
    plt.xticks([])
    plt.yticks([])

    plt.imshow(img,cmap=plt.cm.binary)

    predicted_label = np.argmax(predictions_array)
    if predicted_label == true_label:
        color = 'blue'
    else:
        color = 'red'

    plt.xlabel("{} {:2.0f}% ({})".format(class_names[predicted_label],
                                100*np.max(predictions_array),
                                class_names[true_label]),
                                color=color)

def plot_value_array(i, predictions_array, true_label):
    predictions_array, true_label = predictions_array[i], true_label[i]
    plt.grid(False)
    #plt.xticks([])
    plt.xticks(range(n_class), class_names, rotation=90)
    plt.yticks([])
    thisplot = plt.bar(range(n_class), predictions_array, color="#777777")
    plt.ylim([0, 1]) 
    predicted_label = np.argmax(predictions_array)
 
    thisplot[predicted_label].set_color('red')
    thisplot[true_label].set_color('blue')
```


```python
rnd_idx = np.random.randint(1, n_test//batch_size)
img_cnt = 0
for images, labels in test_dataset:
    img_cnt += 1
    if img_cnt != rnd_idx:
        continue
    predictions = model(images, training=False)
    num_rows = 5
    num_cols = 3
    num_images = num_rows*num_cols
    labels = tf.argmax(labels, axis=-1)
    plt.figure(figsize=(3*2*num_cols, 4*num_rows))
    plt.subplots_adjust(hspace=1.0)
    for i in range(num_images):
        plt.subplot(num_rows, 2*num_cols, 2*i+1)
        plot_image(i, predictions.numpy(), labels.numpy(), images.numpy())
        plt.subplot(num_rows, 2*num_cols, 2*i+2)
        plot_value_array(i, predictions.numpy(), labels.numpy())        
    break
```


![aa_101_0](https://user-images.githubusercontent.com/41605276/75512607-5ab80f80-5a35-11ea-877d-1a4f19b8d982.png)

