---
layout: post
title: "확률론적 선형회귀모형 기초이론 요약"
tags: []
comments: true
---

.

#### # '패스트캠퍼스'에서 공부한 내용을 정리한 것으로 일부 오류가 있을 수 있습니다.

- 패스트캠퍼스 : https://www.fastcampus.co.kr

- 참고자료 출처 : https://datascienceschool.net

### # 확률론적 선형회귀모형

- 결정론적 선형회귀분석은 OLS라는 방법을 사용한다. OLS는 x라던가 y라던가 둘다 확률변수를 가정하지 않았다. 그냥 숫자일 뿐이다.x와 y가 어떻게 나온 숫자이며 뭐가 잘 나오고 뭐가 잘 나올 수 없다라는 사전지식이 하나도 없이 그냥 숫자를 받으면 최적의 y헷을 계산하는 것이다.


- 그런데 이렇게 했을때 가장 큰 문제는 우리가 이렇게 구한 w는 반드시 오차가 있을것이다. 우리가 현실데이터에서 선형회귀분석을 했을때 나온 w값도 조금 오차가 있을것이다. 예를들어 만약에 미세한 오차때문에 w가 음수에서 양수로 바뀐다면 결론자체가 바뀔 수 있는 큰 문제이기 때문에 중요한 포인트라고 할 수 있다. 따라서 w가 얼마인지 아는것도 중요하지만 오차가 어느정도 인지 아는것도 그만큼 상당히 중요한 문제다.


- 그러면 이 오차를 어떻게 알아내야하는가. 수학자들이 부트스트래핑이라는 것을 알아냈다. 오차라는 것은 데이터를 집어넣을때 내가 집어넣는 데이터가 달라지게 되면 w가 달라지게 될 것이다. 


- 예를들어서 서울지역의 아파트 집값 하나하나에 대해서 회귀분석을 한다고 가정하자. 서울에 있는 모든 아파트를 한채한채 다 조사해서 구하게 되면 그게 가장 좋은 모델이겠지만 현실적으로는 쉽지 않은 부분이다. 그래서 대표로 표본을 뽑아서 조사를 하는 것이 통상적이다. 이때 샘플을 뽑을때 내가 어떤 샘플을 뽑느냐에 따라 w값은 분명히 달라지게 될 것이다.


- 그래서 우리가 가지고 있는 정답과 달라지는 부분이 결국에는 데이터가 달라졌을때 얼마나 달라지는지 그것을 보게 되면은 이 데이터가 선형회귀분석에 끼치는 영향력의 크기가 대략 어느정도인지 감이 올 것이다.


- 그런데 이 샘플링을 한다는 것은 어떤 의미에서 표본을 다시 뽑는다는 의미인데 보통은 그렇게 하기가 힘들다. 주어진 데이터가 전부인 경우가 대부분일 것이다. 그러면 편법으로 할 수 있는게 뭐냐면 원래 처음에 내가 받은 데이터가 있다면 이 데이터를 전부 쓰지 않는다. 여기서 일부만 써서 w를 계산하고 다른 일부를 써서 또 w를 계산하고 이런방식으로 일부만 계속뽑아서 w를 뽑아내본다. 그러면 w값도 계속 왔다 갔다할 것인데 이 값들이 전혀 엉뚱하게 다른값이 나오지는 않을 것이다. 어떤 값 근처에 몰려서 나올 것이다. 그러면 내가 얼마정도 데이터를 다르게 주면 w가 얼마정도 왔다 갔다 할건지 대략 감을 잡을 수 있다.


- 그래서 원래 데이터에서 일부만 뽑아내는 작업을 부트스트래핑이라고 한다. 일부만 뽑아내서 그걸로 계수를 찾는 과정을 말한다. 부트스트래핑을 해보면 계수가 대략적으로 어느정도 오차를 갖는지 감을 잡을 수 있다. 


- 하지만 부트스트래핑을 하다보면 선형회귀분석을 반복적으로 여러번 많이 해야하는 문제가 발생한다. 


- 그런데 우리가 statesmodels를 사용해서 선형회귀분석을 하다보면 summary에 std err를 보면 부트스트래핑 했을때 생기는 오차의 표준편차와 유사한 수치를 확인할 수 있다. 오차의 표준편차, 95%확률로 왔다갔다하면 이 정도 왔다갔다 한다고 확인할 수 있다. 근데 이 std err은 부트스트래핑을 해서 구한 수치가 아니다.


- 이 std err은 추가적인 우리의 사전지식이 들어간 수치이다. x와 y 데이터의 관계에 대해서 확률분포를 가정을 했다. 확률분포를 가정한다는 것은 우리가 알고 있는 확률적인 도메인 지식을 추가한다는 것이다. 이게 아무렇게나 나올 수 있는게 아니라 이 숫자가 나올 수 있는 어떤 데이터셋이 어떤것은 잘나오고 어떤것은 잘 안나오는 분포의 지식이 있다는 것이다.

- 그 분포의 지식이 확률론적 선형회귀모형이다. 어떤 데이터가 잘 나오고 어떤 데이터가 잘 안나올지 가정을 했다. 이 가정이 맞다 틀리다라는 것은 분석하는 사람이 판단하는 것이다. 절대적으로 이 가정이 어떻다라고 말 할 수는 없다. 


- 확률론적 선형회귀모형은 다음과 같은 사전지식을 가정을 했다. 


1) 원래 x와 y는 기본적으로 선형관계이다. 


독립변수 x와 가중치 w의 조합으로 y가 나온다. 그런데 이 조합이 정확하지는 않는데 그 원인중에 하나가 정규분포만큼 위아래로 왔다갔다 할 수 있다. 그 정규분포의 크기는 시그마 제곱이다.


종속변수  y가 독립 변수  x의 선형 조합으로 결정되는 기댓값과 고정된 분산  시그마제곱 을 가지는 가우시안 정규 분포라는 것이다. x와 y데이터가 아무렇게나 나오는게 아니다. x데이터가 있으면 x데이터에 뭔가 비례하는 비례상수가 있어서 그것의 조합인 y값이 특정하게 분포가 되는데 문제는 이게 정확하게 안나오고 위아래로 잡음이 가우시안 노멀분포의 형태로 끼게된다.


여기서 주의해야할 점은 y가 x에 대해 조건부로 정규분포를 이룬다는 것이다. y자체가 무조건부로 정규분포는 아니다. x가 정해졌을때 disturbance가 정규분포를 이룬다는 것이다. 더군다나 x가 어떻게 나와야 하는지는 어떠한 가정도 두지 않았다.


이렇게 쓰는 방식도 있고 또 한가지 방식은 잡읍(noise) epslion을 정의를 하는 방법이 있다. 수식으로는 e = y - wTx와 같이 표현할 수 있고 이는 일종에 평균을 제거하는 것이다. 이렇게되면 disturbance 즉 잡음의 분포는 기댓값이 0인 0을 중심으로 왔다갔다하는 정규분포가 된다. 요런식으로 epslion으로 정의하는 경우도 있다.


2) 외생성 가정이라는게 있다.


잡음의 기댓값이 x와 상관없이 0이라는 것이다.x를 조건부로 했을때 x가 정해졌을때 잡음의 기댓값은 0이라는 것이다. 이 외생성 가정으로부터 잡음 엡실론의 무조건부 기댓값이 0임을 또 알 수 있다. 전체적으로 봤을때 잡음의 기댓값이 0이라는 것이다. 또한 잡음과 독립변수 x와 상관관계가 없다.


3) 조건부 독립 가정


첫번째 샘플 얻을때와 두번째 샘플 얻을때 또는 n번째 샘플을 얻을때 생기는 disturbance 잡음이 서로 아무런 상관관계가 없다는 뜻이다.


- 위와 같이 하면 x와 y간에 확률분포가 정해졌기 때문에 Maximum Likelihood Estimation을 통해서 w를 계산할 수 있다. 이제는 OLS로 W를 계산하는 것이 아니다. OLS로 구하던 Maximum Likelihood Estimation로 W를 구하던 값은 결국은 똑같이 나온다. 단 확률적인 모형은 추가로 노멀분포를 따르는 disturbance를 추가로 쓸 수 있다. 잔차는 정규분포를 따른다. 이는 qq플롯으로 확인할 수 있다. 잔차 정규성을 통과했을때 이 모델은 우리가 가정한 확률론적 선형회귀모형이 맞구나 라고 판단 할 수 있다.


- 잔차의 정규성 검정은 옵니버스테스트, 자크베라테스트를 통해 피벨류를 계산할 수 있다.


- x와 잔차를 스케터플롯으로 그렸을때 분산의 크기가 비슷해야한다. x가 어떤 값으로 고정이 되었을때 분포를 구하고 기댓값을 구했을때 0이 되어야 한다.


- w의 표준 오차는 어떤 분포를 따르냐면 스튜던트 t분포를 따른다. 가우시안 정규분포를 따르지 않는 이유는 시그마를 정확하게 예측할 수 없어서 분포가 흔들리기 때문이다. w와 스텐다드 에러를 나누어서 구한 통계량은 스튜던트 t분포를 따른다는 것이다. 만약에 Wi가 0이라고 귀무가설을 설정했을때 귀무가설이 성립한다는 것은 i번째 독립변수는 y의 예측에 아무런 도움이 되지 않는다는 의미이다.


- 실제값으로 wi가 0이 되어야 한다는 귀무가설로 시작했는데 우리가 회귀분석을 하고 나면 우리가 얻은 w헷값은 0이 아닐 수 있다는 것이고 그것의 분포가 스튜던트 t분포라는 것이다. 그래서 wi가 0으로부터 멀리 떨어져 있는 경우 귀무가설이 성립하지 않을 수도 있는데 이런 경우는 원래 w가 0이 아닐 수도 있다는 말이고 y의 예측에 영향을 주는 요소라고 할 수 있다. 이것을 테스트하는것이 단일계수 t검정이다.


- 선형회귀분석 summary에서 Standard Errors assume that the covariance matrix of the errors is correctly specified는 이분산성이 있는지 없는지 체크를 하지 않았다는 이미이다.


- 확률론적 선형회귀모델에서 모든 w에 대해 

$$\ H_0 : w_0  = w_1 = \cdots = w_{K-1} = 0 $$

라는 귀무가설을 세울 수 있다. 이 귀무가설이 성립한다는 것은 모든 독립변수들이 y를 예측하는데 쓸모가 없다는 것이다. 이런 테스트를 회귀분석 F테스트라고 한다. 회귀분석 summary에도 나와있다. 모델 전체가 쓰레기 데이터인가 아닌가를 테스트하는 것이다. 통상 회귀분석을 하면 Prob (F-statistic)가 엄청나게 낮은 값이 나온다. 그러면 이런걸 왜하느냐? 거의 항상 reject될텐데.. 예를 들어 두명이 분석을 했는데 누가 더 좋은 모델이냐 라고 판단할 수 있는 근거로 쓸 수 있다. Prob (F-statistic)가 작을수록 더 좋은 모델이라고 할 수 있다.
