---
layout: post
title: "타계정간 대규모 s3 데이터 마이그레이션 방안"
tags: [Data Engineering]
comments: true
---

.

#### 1. 목적

AS-IS 계정에 있는 S3의 모든 버킷과 데이터를 같은 리전의 TO-BE계정으로 데이터 마이그레이션

(S3 to S3 Migration)

#### 2. 전제조건


- S3 총 데이터 규모가 40TB가 넘음


- AS-IS 계정은 현재 서비스 중이기 때문에 AS-IS 계정의 네트워크나 config 설정을 건드는 것을 최소화해야함


- 1차 베이스라인 데이터 백업 후 디데이까지 일배치로 AS-IS 계정의 s3 데이터와 TO-BE 계정의 s3 데이터 간 sync를 맞추는 작업 필요

#### 3. 가능한 방안

- 결론적으로 [3안] S3 Sync 기능을 이용한 마이그레이션 방안 채택

![123123](https://user-images.githubusercontent.com/41605276/76311265-e40ff180-6313-11ea-9dd6-5d677bb227e3.png)

- [참고사항] s3 Replication 기능

![s3 mig2](https://user-images.githubusercontent.com/41605276/78541271-59e27c80-7830-11ea-9b70-a2cb6b6129dd.png)

위에 그림과 같이 타계정(B계정 <-> C계정) s3 간에 데이터 복제가 가능한지 궁금할 수도 있는데 관련해서 사용할 수 있는 옵션이 s3 Replication이라는 기능이 있다.

말그대로 s3 데이터를 복제해주는 기능이다. 
Source 버킷 콘솔에서 '복제규칙' 을 지정해주면 작동하는 기능이다.
Destination 등 설정값을 지정하고 활성화를 하면 그 시점부터 새로 변동되는 파일에 대해 replication을 해준다.
(기존에 저장되어 있던 파일들까지 반영하고자 한다면 S3 Sync 명령 등을 활용하여 사전에 source와 destination을 동기화하고, s3 replication을 적용하면 된다.)

AWS 공식문서는 아래의 URL을 참고하면 된다.

- 공식 도큐먼트 : https://docs.aws.amazon.com/ko_kr/AmazonS3/latest/user-guide/enable-replication.html#enable-replication-cross-account-destination


- AWS 블로그 참고자료 : https://aws.amazon.com/ko/blogs/korea/amazon-s3-introduces-same-region-replication/


원하는 폴더&파일만 복제가 가능하도록 '접두사 지정'이 가능하다. 

#### 4. [3안] S3 Sync 기능 테스트

STEP 1) TO-BE 계정의 Destination 버킷에 대하여 AS-IS 계정이 접근할 수 있도록 버킷권한 설정

- TO-BE 계정의 Destination 버킷에서 아래와 같이 버킷권한 설정

![22](https://user-images.githubusercontent.com/41605276/76311599-abbce300-6314-11ea-8030-31f2abf3b99c.png)


```python
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "DelegateS3Access",
            "Effect": "Allow",
            "Principal": {
                "AWS": "arn:aws:iam::111111111111:user/pms-s3synctest-user"
            },
            "Action": [
                "s3:ListBucket",
                "s3:GetObject",
                "s3:PutObject",
                "s3:DeleteObject",
                "s3:GetObjectAcl"
            ],
            "Resource": [
                "arn:aws:s3:::minman2",
                "arn:aws:s3:::minman2/*"
            ]
        }
    ]
}
```

STEP 2) AS-IS 계정에서 아래와 같이 사용자계정 inline 정책을 만들어서 부여

![asis](https://user-images.githubusercontent.com/41605276/77998683-6ec09b00-736c-11ea-8a5a-440b34ea4bb6.png)

** STEP 1 ~ 2) 참고자료 : https://aws.amazon.com/ko/premiumsupport/knowledge-center/copy-s3-objects-account/

STEP 3) TO-BE 계정에서 EC2(amazon linux2) 생성 후 task 부여환경 구성

step 3-1) ec2 생성 및 ssh 접속

step 3-2) 파이썬 3.7 및 nmon 설치


```python
sudo yum update -y
sudo yum install python37 python37-pip -y
sudo amazon-linux-extras install epel -y
sudo yum update -y
sudo yum install nmon -y
sudo pip3 install boto3
```

step 3-3) os 파이썬 기본버전 변경 (2.7 -> 3.7)


```python
sudo alternatives --install /usr/bin/python python /usr/bin/python2.7 1
sudo alternatives --install /usr/bin/python python /usr/bin/python3.7 2
```

step 3-4) 주피터 설치


```python
sudo pip3 install jupyter
```

step 3-5) 주피터 설정파일 생성


```python
jupyter notebook --generate-config
```

step 3-6) 설정파일 내 설정 변경


```python
sudo vi /home/ec2-user/.jupyter/jupyter_notebook_config.py
```

step 3-6-1) 외부 접속 허용


```python
##  Takes precedence over allow_origin_pat.
c.NotebookApp.allow_origin = '*'
```

step 3-6-2) 작업 경로 설정


```python
## The directory to use for notebooks and kernels.
c.NotebookApp.notebook_dir = '/home/ec2-user'
```

step 3-6-3) 시작시 브라우저 실행 안함설정


```python
## Whether to open in a browser after starting.
c.NotebookApp.open_browser = False
```

step 3-6-4) 접속 암호 설정

암호 설정을 위해 아래와 같이 암호 생성

리눅스 콘솔에서 python 실행

아래와 같이 입력


```python
Python 3.6.1 |Continuum Analytics, Inc.| (default, May 11 2017, 13:25:24) [MSC v.1900 64 bit (AMD64)] on win32
Type "help", "copyright", "credits" or "license" for more information.
>>> from notebook.auth import passwd
>>> passwd()
Enter password: 1234
Verify password: 1234
'sha1:**********************'
```

step 3-6-5) 다시 설정파일로 들어가서 비밀번호 설정부분 변경


```python
#  The string should be of the form type:salt:hashed-password.
c.NotebookApp.password = 'sha1:***********************'
```

step 3-6-6) 접속포트 설정

- 포트설정과 동시에 ec2 보안그룹에서도 8800 포트를 열어준다.


```python
## The port the notebook server will listen on.
c.NotebookApp.port = 8800
```

step 3-7) 리눅스 콘솔에서 aws configure 설정

step 3-8) 시스템 타임존 변경

tzselect 명령어를 사용하여 타임존을 서울 시간으로 설정

(이 방법은 로컬에만 적용된다는 것을 주의한다)


```python
echo "TZ='Asia/Seoul'; export TZ" >> .profile
. .profile
date
```

step 3-9) 아래와 같이 리눅스 콘솔에서 주피터 실행


```python
jupyter notebook --ip=0.0.0.0 --port=8800
```

아래와 같이 브라우저에서 IP주소:8800으로 접속

![JYPY](https://user-images.githubusercontent.com/41605276/76313004-78c81e80-6317-11ea-9628-fe8a84e9748e.png)

STEP 4) jupyter에서 노트북 생성 후 아래와 같은 코드를 작성하여 코드 실행


```python
import subprocess
from datetime import datetime
import time
import boto3
import math

start = time.time()

timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')+'\n'
start_timestamp = timestamp[0:4]+timestamp[5:7]+timestamp[8:10]

source_bucket = "k"
source_prefix = "m"
source_s3_location = source_bucket+'/'+source_prefix

destination_bucket = 'a'
destination_prefix = 'b/d'
destination_s3_location = destination_bucket +'/'+ destination_prefix
task_name = "task 3"

sync_command = f"aws s3 sync s3://{source_s3_location}/ s3://{destination_s3_location}/ --delete --acl bucket-owner-full-control"

file_name = start_timestamp+" "+task_name

logfile = open("{}.txt".format(file_name), 'wb')
start_time = "start time : {}".format(timestamp).encode()
logfile.write(start_time)

try :
    log_record = subprocess.check_output(sync_command, stderr=subprocess.STDOUT, shell=True)
    print("job success")
    logfile.write(log_record)
    runtime = "Runtime: {} sec \n".format(time.time()-start).encode()
    logfile.write(runtime)
    print(runtime)
    
    # check data integrity
    client = boto3.client('s3')
    paginator = client.get_paginator('list_objects_v2')
    
    operation_parameters = {'Bucket': '{}'.format(source_bucket),'Prefix': '{}/'.format(source_prefix)}
    response_iterator = paginator.paginate(**operation_parameters)
    source_object_list = []
    source_folder_list = []

    for page in response_iterator:
        for content in page['Contents']:
            if (content['Key']+','+str(content['Size']))[-3:] == '/,0':
                source_folder_list.append(content['Key']+','+str(content['Size']))
            else : 
                source_object_list.append(content['Key'].split('/').pop()+','+str(content['Size']))
  

    operation_parameters = {'Bucket': '{}'.format(destination_bucket),'Prefix': '{}/'.format(destination_prefix)}
    response_iterator = paginator.paginate(**operation_parameters)
    destination_object_list = []
    destination_folder_list = []

    for page in response_iterator:
        for content in page['Contents']:
            if (content['Key']+','+str(content['Size']))[-3:] == '/,0':
                destination_folder_list.append(content['Key']+','+str(content['Size']))
            else : 
                destination_object_list.append(content['Key'].split('/').pop()+','+str(content['Size']))

    if source_object_list == destination_object_list :
# 실시간 데이터가 유입되는 버킷은 아래와 같이 정합성 테스트
#     if source_object_list[0:math.floor(len(source_object_list)*0.95)] == destination_object_list[0:math.floor(len(source_object_list)*0.95)] : 

        logfile.write("data integrity check success \n".encode())
        logfile.write("{} to {} complete \n".format(source_s3_location,destination_s3_location).encode())
        
        source_folder_list_str = "source_folder_list:" + str(source_folder_list)+'\n'
        logfile.write(source_folder_list_str.encode())
        source_object_list_str = "source_object_list:" + str(source_object_list)+'\n'
        logfile.write(source_object_list_str.encode())
        
        destination_folder_list_str = "destination_folder_list:"+str(destination_folder_list)+'\n'
        logfile.write(destination_folder_list_str.encode())
        destination_object_list_str = "destination_object_list:"+str(destination_object_list)+'\n'
        logfile.write(destination_object_list_str.encode())
        
        source_object_list_str = "number of source_object_list:" + str(len(source_object_list))+'\n'
        logfile.write(source_object_list_str.encode())
        destination_object_list_str = "number of destination_object_list:" + str(len(destination_object_list))+'\n'
        logfile.write(destination_object_list_str.encode())
        
        print("data integrity check success \n")
    else:
        logfile.write("data integrity check fail \n".encode())
        
        source_folder_list_str = "source_folder_list:" + str(source_folder_list)+'\n'
        logfile.write(source_folder_list_str.encode())
        source_object_list_str = "source_object_list:" + str(source_object_list)+'\n'
        logfile.write(source_object_list_str.encode())
        
        destination_folder_list_str = "destination_folder_list:"+str(destination_folder_list)+'\n'
        logfile.write(destination_folder_list_str.encode())
        destination_object_list_str = "destination_object_list:"+str(destination_object_list)+'\n'
        logfile.write(destination_object_list_str.encode())
        
        source_object_list_str = "number of source_object_list:" + str(len(source_object_list))+'\n'
        logfile.write(source_object_list_str.encode())
        destination_object_list_str = "number of destination_object_list:" + str(len(destination_object_list))+'\n'
        logfile.write(destination_object_list_str.encode())
        
        print("data integrity check fail")
    
except subprocess.CalledProcessError as e:
    print("job fail")
    logfile.write(e.output)
    runtime = "Runtime: {} sec".format(time.time()-start).encode()
    logfile.write(runtime)
    print(runtime)
    
logfile.close()
```

    job success
    b'Runtime: 1.083536148071289 sec \n'
    data integrity check success 
    
    

로그파일 저장된 내용 예시

![aa](https://user-images.githubusercontent.com/41605276/76387930-410ab680-63ab-11ea-9c6c-6cea8389d1e7.png)

#### 5.  [참고사항] S3 Replication 기능 테스트

[구현하고자 하는 아키텍처]

![3](https://user-images.githubusercontent.com/41605276/79062783-05c81400-7cd8-11ea-8447-6c51c072a3a4.png)

[가이드]

STEP 1) Source 및 Destination Bucket에 versioning 설정

![1](https://user-images.githubusercontent.com/41605276/79062977-96ebba80-7cd9-11ea-9794-aaf9ea6dcd27.PNG)

STEP 2) Source Bucket에 접속해서 아래와 같이 Replication Rule 설정

![4](https://user-images.githubusercontent.com/41605276/79063539-46765c00-7cdd-11ea-8b7e-be9f73f760ff.png)

* 참고로 위에 그림과 같이 replication 설정 시 IAM Role을 새로 생성한다고 지정하면 아래와 같이 생성된다.

IAM Role : s3crr_for_pms-s3synctest-bucket_to_minman2


```python
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Action": [
                "s3:Get*",
                "s3:ListBucket"
            ],
            "Effect": "Allow",
            "Resource": [
                "arn:aws:s3:::[Source Bucket Name]",
                "arn:aws:s3:::[Source Bucket Name]/*"
            ]
        },
        {
            "Action": [
                "s3:ReplicateObject",
                "s3:ReplicateDelete",
                "s3:ReplicateTags",
                "s3:GetObjectVersionTagging",
                "s3:ObjectOwnerOverrideToBucketOwner"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:s3:::[Destination Bucket Name]/*"
        }
    ]
}
```

Destination Bucket에 가서 아래와 같이 설정해준다.

![5](https://user-images.githubusercontent.com/41605276/79063403-87ba3c00-7cdc-11ea-812c-250dc54ce90c.png)

STEP 3) 정상적으로 Replication 설정이 되었는지 테스트

아래 그림과 같이 Source Bucket에 testtest.txt 파일을 업로드 하면 그대로 Destination Bucket에도 똑같은 prefix에 똑같은 파일로 업로드가 된 것을 확인 할 수 있다.

![6](https://user-images.githubusercontent.com/41605276/79063595-9e14c780-7cdd-11ea-85c6-d17e58aa9e67.png)

여기서 테스트를 더 해보면 위에 testtext.txt 파일의 내용이 'version 2'이라는 문자열이 들어가 있는 파일이었는데 내용을 'version 3'로 바꿔서 Source Bucket에 업로드 했을때 똑같이 Destination Bucket에도 반영이 되는지 보자. 아래 그림과 같이 소스버킷의 파일의 내용을 바꿔서 업로드해서 Destination 버킷에도 그대로 반영되는 것을 확인할 수 있다.

![7](https://user-images.githubusercontent.com/41605276/79063804-db2d8980-7cde-11ea-873f-7bb21f624cf8.png)

열어보면 내용이 testtest.txt의 내용이 version 3 인것을 확인할 수 있다.

QA) Souce Bucket의 방금 업로드한 testtest.txt 파일을 삭제하면 Destination Bucket에도 있는 testtest.txt도 지워지는 것인지?

- Answer : 아니다 Destination Bucket에도 있는 testtest.txt은 지워지지 않는다. 위와 같이 똑같은 네임의 객체의 내용이 바뀌어서 새로 업로드 된 경우에는 반영이 되지만 아예 있는 객체를 삭제해도 Destination Bucket의 그 객체도 같이 삭제되지 않는다.

참고자료 : https://docs.aws.amazon.com/ko_kr/AmazonS3/latest/dev/replication-what-is-isnot-replicated.html / S3 개발자 안내서 '삭제 작업이 복제에 미치는 영향'

The official text - "If you specify an object version ID to delete in a DELETE request, Amazon S3 deletes that object version in the source bucket, but it doesn't replicate the deletion in the destination bucket. In other words, it doesn't delete the same object version from the destination bucket. This protects data from malicious deletions. "
