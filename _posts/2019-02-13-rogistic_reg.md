---
layout: post
title: "로지스틱 회귀분석 기초이론 요약"
tags: [머신러닝]
comments: true
---

.

#### # '패스트캠퍼스'에서 공부한 내용을 정리한 것으로 일부 주관적이며 오류가 있을 수 있습니다.

- 패스트캠퍼스 : https://www.fastcampus.co.kr

- 참고자료 출처 : https://datascienceschool.net


### # 로지스틱 회귀분석 기초이론


- 이름이 회귀분석인데 classification에도 쓸 수 있다.


- 선형회귀분석은 확률분포가 y값이 가우시안 노멀을 따르는데 x에 대한 뮤값이 선형적으로 변하는 값들이었다.


- 선형회귀분석의 문제점은 노멀분포라는게 +무한대로 가거나 -무한대로 가도 확률이 0으로 되지는 않는다. 예를들어서 집값을 예측하는 문제인데 y값은 양수지만 y헷값이 음수가 발생하는 넌센스한 문제점이 발생한다는 것이다.


- 선형회귀분석의 다른문제점은 y값이 음수여야 하는 이런조건에서는 우리가 알고있는 선형회귀분석모형에서 동작이 잘 안되는 문제점이 있다. 


- 또다른 문제점은 y값이 정수인 경우는 잘 예측하지 못하는 문제가 있다. 왜냐하면 연속확률변수값이기 때문이다. 예를들어서 우리는 프리미어리그 구단의 순위를 예측한다고 목표를 정했는데 15등이면 15등이라는 결과가 정확하게 나와야 하는데 15.2342등 이런 수치의 결과가 나올 수 있다. 


- 이런 문제점들을 해결하기 위해서 수학자들이 생각해낸 것이 x에 대한 y의 분포를 왜 굳이 가우시만 노멀만 사용해야하냐 다른 분포들도 사용해보자라고 아이디어를 구상하였다.


- 주로 로지스틱 회귀분석 모델에서는 종속변수가 이항분포를 따르고(y값이 0 ~ N값을 가지는) 그 모수 뮤가 독립변수 x에 의존하는 다음과 같은 형태로 가정한다.

$$\ p(y \mid x) = \text{Bin} (y \mid \mu(x), N) $$


- 아니면 아래와 같이 y가 베르누이 확률분포인 경우도 있다.

$$\ p(y \mid x) = \text{Bern} (y \mid  \mu(x) ) $$

- 위와 같이 베르누이 확률분포인 경우의 로지스틱 회귀분석을 풀게되면 y헷값을 구할때 x에 따른 뮤를 예측하여 확률분포를 추정하는 것인데 y를 낼때 뮤(x)값이 0.5보다 크면 1로, 0.5보다 작으면 0으로 y를 예측하는 형태가 된다. 또는 y가 1이 될 확률을 뮤(x)로 다이렉트로 출력할 수도 있다.


- 위와 같은 방법으로 classification을 하겠다는 것이 로지스틱 회귀분석이다.


- 로지스틱 회귀분석은 statsmodels나 scikit-learn 둘다 구현이 되어 있다.


- 로지스틱 회귀분석에서 뮤는 독립변수 x에 따라서 달라지게 되는데 가우시안 노멀분포인 경우에는  $$\ w^Tx $$ 수식과 같이 단순하게 함수를 설정하면 되는데 이항분포나 베르누이 확률분포인 경우에는 위와 같은 수식을 사용하면 안된다.


- 왜냐하면 x가 점점 커지면 뮤가 1보다 커지거나 1보다 작아지는 경우가 발생하기 떄문이다.


- 그래서 수학자들이 낸 아이디어가 x가 커지면 뮤가 커지고 x가 작아지면 뮤도 작아지는건 당연한 전제로 받아들이는데 0과 1은 벗어나지 않게 수학적인 장치를 만들어보자는 것이었다.


- 그래서 나온개념이 시그모이드 함수이다. 시그모이드 함수는 대략 특정 유한한 구간을 설정하고 항상 0 또는 양의 기울기를 가지는 함수의 집합을 칭한다.


- 시그모이드 함수에도 여러 종류의 함수들이 있는데 로지스틱 함수, 하이퍼볼릭 탄젠트함수, 오차함수 이 세개를 가장 많이 사용한다.


- 참고로 오차함수는 가우시안 노멀분포의 CDF에 해당하는 함수라고 할 수 있다. 만약에 오차함수를 사용한 회귀분석을 하는 경우는 probit regression이라고 한다.


- 로지스틱 함수는 로그함수와 유사하다. 로그함수는 입력으로 양수만 받는 함수이고 만약에 1보다 큰수가 들어가게되면 양수, 0 ~ 1 사이의 숫자가 들어가면 음수가 출력된다.


- 로지스틱 함수는 수식이 다음과 같다. $$\ \text{logitstic}(z) = \sigma(z) = \dfrac{1}{1+\exp{(-z)}} $$

$$\ z = w^Tx $$

- 원래 뮤라는 것은 0부터 1사이만 있을 수 있다. 반면에 x는 -무한대 ~ +무한대까지 있을 수 있다. 특히 뮤의 범위를 원하는 범위로 조절하려고 하는데


- 그래서 처음 생각한것이 odds ratio이다. 쉽게 말하면 '동전 뒷면이 나올 확률 / 동전 앞면이 나올 확률'과 같다. 수식은 'odds ratio = 뮤 / 1 - 뮤'이다. 뮤가 0이면 odds ratio는 0이 되고 뮤가 1이 되면 무한대로 가게된다.


- 우리가 뮤를 odds ratio로 바꾸면 0 ~ 1사이였던 숫자가 0 ~ 무한대로 범위가 확장이 된다.


- 그 다음에 이 odds ratio에 log를 취하게 된다. 그것을 로지트 함수라고 한다.
$$\ z = \text{logit}(\text{odds ratio}) = \log \left(\dfrac{\mu}{1-\mu}\right) $$


- 그러면 로그라는 것은 0 ~ 무한대였던 범위를 -무한대 ~ +무한대로 변환시켜주게 된다.


- 이 뮤 0~1을 -무한대 ~ +무한대로 확장을 시키고, 순서가 바뀌지 않게 mapping을 시킬 수 있게되었다. 따라서 뮤 -> x로 바꾼셈이다. 그런데 우리가 원하는 것은 그 반대이다. 즉 x가 들어갔을때 뮤가 나와야 한다. 그러면 우리가 구한 이 로지트 함수의 역함수를 구하면 되는 것이다. 그 함수가 바로 로지스틱 함수이다.


- 로지스틱 함수는 -무한대 ~ +무한대의 값을 0과 1사이로 변환해주는 함수이다.  $$\ \text{logitstic}(z) = \mu(z) = \dfrac{1}{1+\exp{(-z)}} $$

$$\ z = w^Tx $$

- 로지스틱함수에서 z = 0 인 경우에는 뮤가 0.5가 되고 z가 0보다 크면 뮤도 0.5보다 커지며, z가 0보다 작은 경우 뮤도 0.5보다 작아지게 된다. 즉 z가 판별함수의 역할을 하게 되는 것이다.


- 다시말해 로지스틱 회귀분석을 한다는 것은 $$\ w^Tx $$를 판별함수로 갖는 판별모형을 만드는 것이라고 할 수 있다.


- 로지스틱 회귀분석을 우리가 풀고나서 x공간상에서 보게되면 직선 경계선을 갖는 모형이 나오게 된다.


- 우리는 그러면 이제 로지스틱 회귀모형을 만들때 w값을 찾게 되면 끝나게 된다. 이 w값은 최대가능도 방법으로 추정할 수 있다. 만약에 우리가 구하는 분포가 베르누이라고 할떄 수식은 다음과 같게 된다. $$\ p(y \mid x) = \text{Bern} (y \mid  \mu(x;w) ) = \mu(x;w)^y ( 1 - \mu(x;w) )^{1-y} $$,

$$\ \mu(x;w) = \dfrac{1}{1 + \exp{(-w^Tx)}} $$


- 위의 수식은 x데이터와 y데이터가 각각 하나씩 있는 경우고 우리는 이 여러개의 데이터를 전부구해야한다. 이 개별 데이터들은 서로 독립적이기 때문에 이 수식을 데이터 수만큼 곱하면 되는데 이걸 그냥 곱하게 되면 복잡하고 불편하기 때문에 앞에 로그를 씌워줘서 log likelihood를 구하면 된다. x하고 y는 이미 있기때문에 대입하면 되는 것이고 우리는 이 log likelihood를 가장 크게하는 w값을 찾으면 된다.


- 따라서 위에 우리가 구하려고 하는 식을 w로 미분하게 되면 값이 0이 되는 위치가 log likelihood가 제일 커지는 위치인 것이다. 따라서 w로 미분해서 0을 만들면 된다.


- $$\ \begin{eqnarray}
\dfrac{\partial \text{LL}}{\partial w} 
&=& \sum_{i=1}^N \big( y_i  - \mu(x_i;w) \big) x_i \\
\end{eqnarray} $$ 결론적으로 다음과 같은 식이 나오게 되는데 이 값이 행렬의 형태로 간단하게 계산이 안된다. 이 값은  w 에 대한 비선형 함수이므로 선형모형 처럼 행렬의 형태로 w 값에 대한 수식을 구할 수 없으며 수치적최적화를 통해 최적 w의 값을 구해야 한다.


- w를 0으로 만들기 위해 w를 계속해서 넣어주고 그레디언트 백터 구해서 그방향으로 옮기는 과정을 반복해줘야 한다.


- 수치적 최적화를 위해 Steepest Gradient decent 방법을 쓰면 된다. 로그가능함수 LL을 최대화한다는 것은 다음 목적함수를 최소화하는 것과 같다. J = -LL .즉, 그레디언트에 -를 취한방향, 즉 반대방향으로 계속 내려가면된다.


- 수식으로는 다음과 같이 표현할 수 있다.$$\ \begin{eqnarray}
w_{k+1} 
&=& w_{k} - \eta_k g_k \\
&=& w_{k} + \eta_k \sum_{i=1}^N \big( y_i  - \mu(x_i; w_k) \big) x_i\\
\end{eqnarray} $$

그레디언트 벡터 : $$\ g_k = \dfrac{d}{dw}(-LL) $$


- 로지스틱 회귀분석도 x가 많아지게 되면 오버피팅 현상이 발생하게 되는데 이를 방지하기위해 ridge, lasso, elastic net 방식의 정규화 페널티를 목적 함수인 로그 라이클리후드에 추가할 수 있다.


- 로지스틱 회귀분석의 성능은 통상 McFadden pseudo R square 값으로 측정한다.
