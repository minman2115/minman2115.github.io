---
layout: post
title: "서포트 백터머신 기초이론 요약 및 구현실습"
tags: [머신러닝]
comments: true
---

.

#### # '패스트캠퍼스'에서 공부한 내용을 블로거 입장에서 정리한 것으로 일부 주관적이며 오류가 있을 수 있습니다.

- 패스트캠퍼스 : https://www.fastcampus.co.kr

- 참고자료 출처 : https://datascienceschool.net

### # 서포트 백터머신 기초이론 및 구현실습

- 퍼셉트론을 조금 더 발전시킨 것이 서포트 벡터머신이다.


- 퍼셉트론의 문제점은 linearly separable 데이터 집합이 있을때 실제로 그 데이터들을 분리하는 선이 하나만 존재하는 것이 아니라 무한하게 많이 존재할 수 있다는 점이다.


- 그렇다면 그 수많은 구분선이 있다면 어떤 선이 가장 좋은 것인가 라는 것을 생각해야 한다.


- 모델 학습 시 바운더리가 구분선 부근에 바싹 붙어서 설정이 되어 있다면 추후 test 시 miss classification을 할 가능성이 높아진다.


- 이런 경우를 막기위해서는 데이터 간 구분하는 선을 학습데이터에서 가장 멀리 긋는 것이 필요하다.


- 상대편 클래스 근처에 있는 최전방의 데이터 들에 대해서 바운더리를 멀리 두는 것이 포인트다.


- 그래서 나온 개념이 최전방에 있는 데이터들이 서포트 벡터이다.


- 이 서포트 벡터를 기준으로 바운더리를 두게 된다.


- 사실상 서포트 벡터 뒤에 있는 데이터 들은 바운더리를 긋는데 전혀 영향을 주지 않게된다.


- 이 바운더리, 경계선은 양쪽의 서포트 벡터에서 멀리떨어지도록 해야하기 때문에 경계선과 서포트 벡터 사이에 정확하게는 음의 서포트벡터와 양의 서포트벡터 사이에 거리를 마진이라고 칭하고 설정하게 된다.


- 그래서 가능한한 이 마진을 가장 크게해야하는 것이 중요하다. 서포트 벡터를 기준으로 마진을 설정하고 그 마진과 경계선이 직각이 되게 해야한다.


- 이 말을 기하학적으로 보면 선과 점의 거리를 계산해야한다.


- 직선의 방정식은 $$\ f(x) = w^Tx-w_0 $$ 이렇게 쓸 수 있고 이 함수가 판별함수가된다. w라는 벡터에 수직이면서 위치는 어느곳이나 상관없다. 


- 따라서 판별경계선 $$\ w^T x - w_0=0 $$ 과 최전방 서포트 벡터들간의 거리를 구하여 마진값이 최대가 되도록 목적함수를 설정하여 최적화하면 된다.


- 마진은 $$\ \dfrac{w^T x^{+} - w_0}{\| w \|}  -\dfrac{w^T x^{-} - w_0}{\| w \|} = \dfrac{2}{\| w \|} $$가 되며 이 마진값이 최대가 되는 경우는 $$\ \| w \| $$가 최소가 되야 한다.


- 따라서 다음과 같은 목적함수를 최소화 시키면된다.
$$\ L = \dfrac{1}{2} ||w||^2 = \dfrac{1}{2} w^T w $$


- ||w||와 같이 L1 norm은 piecewise linear이기 때문에 그레디언트 속도조절이 안된다.


- 그래서 위의 목적함수는 L2 norm 즉 ||w||를 제곱하였다.


- 분류가 되는 선이 무한하게 많은 상황중에 분류가 제대로 되는 것중에서 위의 목적함수를 최소화시켜야 하기 때문에 최적화문제이긴 최적화 문제이지만 제한조건이 있는 최적화 문제가 된다.


- 제한조건은 여기서 분류가 제대로 되야한다는 것이다.


- 데이터가 N개가 있다면 어떤 클래스가 되던간에 y값은 플러스 그룹에서는 +1 보다 커야하고 마이너스 그룹에서는 -1보다 작아야 한다. 그러면 전체 데이터를 봤을때 다음과 같은 수식으로 표현할 수 있다.  $$\ y_i \cdot f(x_i) = y_i \cdot( w^Tx_i - w_o) \geq 1 \;\;\; ( i = 1, \ldots, N ) $$가 된다.


- y가 플러스면 판별함수도 플러스니 전체식도 플러스, y가 마이너스면 판별함수도 마이너스니 전체식도 플러스가 된다.  위의 수식은 데이터가 n개 있으므로 그 수식도 n개가 있는 것이다.


- 따라서 라그랑지 승수법을 사용하여 다음과 같은 목적함수를 최소화 하면 된다. $$\ L = \dfrac{1}{2} w^T w - \sum_{i=1}^N a_i \{ y_i \cdot ( w^Tx_i - w_o) - 1 \} $$


- 이 최적화 문제를 풀어 w, w0, a를 구하면 최종적인 판별함수를 구할 수 있다.


- 결론적으로 예측모형을 다음과 같이 수식으로 나타낼 수 있다. 
$$\ f(x) = w^T x - w_0 $$

- 서포트벡터머신은 상당히 큰 제한조건이 있다. 


- 그게 뭐냐면 linear separable해야 한다는 점이다. 만약에 그게 안되는 경우에는 어떻게 해야하나라는 고민에서 나온 개념이 슬랙변수이다.


- 이 슬랙변수를 서포트백터머신에 추가할 수 있다.


- 슬랙변수는 원래 부등식제한조건을 등식제한조건으로 바꾸는데 사용되는 개념이다. 제한조건을 완화한다는 의미가 있다.


- 어떤식으로 완화하는가. $$\ w^Tx_+ - w_0 \geq +1-\xi_i $$ , $$\ w^Tx_- - w_0 \leq -1+\xi_i $$ 처럼 +1, -1 딱 이렇게 제한조건을 주는게 아니라 데이터 하나하나에 대해 약간의 융통성을 준다는 의미이다. 수식에서 크사이가 이를 나타내주고 있다.


- 크사이가 0이면 융통성을 주지 않는다는 의미이고, 크사이가 1이라면 경계선까지 융통성을 늘리겠다는 의미이다. 만약에 1보다 더 크다 그러면 경계선을 넘어서겠다는 의미이다. 일부 데이터에 따라 이런 허용치를 융통성으로 허용하겠다는 것이다.


- 사실상 1보다 크다는 것은 얘는 그냥 오분류를 해도 상관이 없다는 의미이다.


- 크사이는 데이터마다 하나씩 주어지기 때문에 데이터마다 융통성이 다르게 적용된다는 것을 알 수 있다. 또한 0보다 같거나 클때 의미가 있으므로 모든 슬랙변수는 0보다 크거나 같다.


- 크사이는 어쨌든 융통성이기 때문에 가능한한 최소화해야한다. 


- 따라서 이런 슬랙변수까지 고려하게되면 목적함수는 다음과 같아진다.
$$\ L = \dfrac{1}{2} ||w||^2 - \sum_{i=1}^N a_i (y_i \cdot ( w^Tx_i - w_o) - 1 + \xi_i ) - \sum_{i=1}^N \mu_i \xi_i  + C \sum_{i=1}^N \xi_i $$

- 여기서 $$\ C \sum_{i=1}^N \xi_i $$는 슬랙변수 합이 너무 커지지않도록 제한한다. 또한 위식에서 시그마 뮤i는 크사이가 0보다 같거나 크다는 부등식제한조건에 대한 라그랑지 멀티플라이어다.

- $$\ C \sum_{i=1}^N \xi_i $$는 슬랙변수가 지나치게 커지는 것을 막아주는 패널티 조건이다. 다시말해 오분류가 되는 데이터를 최소한으로 줄이기 위한 장치라고 할 수 있다. 그 패널티를 강경하게 주느냐 약하게 주느냐를 결정하는 것이 C다. 이 C가 커지면 커질 수록 오분류가 되는 것을 막겠다는 의도이다.

- 슬랙변수의 가중치 C가 커지면 서포트 벡터의 수가 줄어들고, 반면에 작아지면 서포트 벡터의 수가 증가하는 영향이 있다.


- 이렇게 서포트 벡터가 여러개가 있는 경우 prediction을 할때 이 서포트 벡터의 평균을 내서 유사도를 구하는 방식으로 한다.


- 슬랙변수의 가중치를 작게 두면 서포트 벡터의 수가 늘어나게 되서 서포트 벡터가 혹시나 아웃라이어일 경우를 대비하여 위와 같이 평균을 내서 유사도를 구하는 방식으로 안정적인 방안을 취할 수 있다.


- 서포트 벡터머신은 이미지 classification할때 많이 쓰이는 모델이다. 이미지는 상당히 고차원이기 때문에 사람이 볼때는 모르겠지만 이런 상당히 고차원에서는 이 이미지들이 상당히 떨어져 있는 linearly separable한 데이터이다. 서포트 벡터 머신은 이 떨어진 차원의 데이터에 대해 경계선을 잘 그을 수 있다.


- 아래는 scikit-learn 패키지 내장 데이터인 '올리베티 페이스' 이미지를 서포트벡터 머신으로 모델링하여 테스트한 성능을 구현한 실습코드이다. classification report를 보면 알 수 있듯이 90% 이상의 뛰어난 성능을 보여준다.


```python
## 올리베티 이미지 임포트
from sklearn.datasets import fetch_olivetti_faces
faces = fetch_olivetti_faces()

## train & test 데이터 분리
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(faces.data, faces.target, test_size=0.3, random_state=0)

## 서포트벡터 머신을 이용한 모델학습
from sklearn.svm import SVC
svc = SVC(kernel='linear').fit(X_train, y_train)

## 모델테스트
from sklearn.metrics import classification_report, accuracy_score
y_pred_test = svc.predict(X_test)

## 결과전시
print(classification_report(y_test, y_pred_test))
```

                  precision    recall  f1-score   support
    
               0       0.86      1.00      0.92         6
               1       1.00      1.00      1.00         4
               2       1.00      1.00      1.00         3
               3       0.33      1.00      0.50         1
               4       1.00      1.00      1.00         2
               5       1.00      1.00      1.00         5
               6       1.00      0.80      0.89         5
               7       1.00      0.67      0.80         3
               8       1.00      1.00      1.00         2
               9       1.00      1.00      1.00         2
              10       1.00      1.00      1.00         4
              11       1.00      1.00      1.00         1
              12       1.00      1.00      1.00         2
              13       1.00      1.00      1.00         3
              14       1.00      1.00      1.00         5
              15       1.00      0.60      0.75         5
              16       0.00      0.00      0.00         0
              17       1.00      1.00      1.00         6
              19       1.00      1.00      1.00         4
              20       1.00      1.00      1.00         1
              21       1.00      1.00      1.00         2
              22       1.00      1.00      1.00         2
              23       1.00      1.00      1.00         2
              24       1.00      1.00      1.00         2
              25       1.00      1.00      1.00         4
              26       1.00      1.00      1.00         4
              27       1.00      1.00      1.00         1
              28       1.00      1.00      1.00         3
              29       1.00      1.00      1.00         5
              30       1.00      1.00      1.00         4
              31       1.00      1.00      1.00         4
              32       1.00      1.00      1.00         3
              33       1.00      1.00      1.00         2
              34       1.00      0.83      0.91         6
              35       1.00      1.00      1.00         1
              36       1.00      1.00      1.00         4
              37       1.00      1.00      1.00         3
              38       1.00      1.00      1.00         1
              39       0.75      1.00      0.86         3
    
       micro avg       0.96      0.96      0.96       120
       macro avg       0.95      0.95      0.94       120
    weighted avg       0.98      0.96      0.96       120
    
    
- 과거에는 그래서 이미지 분석 시 서포트 벡터머신이 가장 많이 쓰였고, 참고로 피쳐가 여러가지가 섞여있을때는 랜덤포레스트 또는 부스팅방법이 과거에 또한 많이 쓰였다.
