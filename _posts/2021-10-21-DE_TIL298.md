---
layout: post
title: "Airflow 클러스터 구성과정 요약"
tags: [Data Engineering]
comments: true
---

.

Data_Engineering_TIL(20211021)

[참고자료]

1) airflow 시스템 복원을 위한 주요정보 백업 및 복원 스크립트 예시

URL : https://minman2115.github.io/DE_TIL294

2) Airflow 운영시 서비스 on&off 명령어 스크립트 예시

URL : https://minman2115.github.io/DE_TIL161

[내용]

celery executor 엔진의 airflow 클러스터 구성과정은 아래와 같다.

STEP 1) Airflow 인프라 생성
- 서버 : EC2
- dag, plugins, log 등 공통저장소 : EFS
- 메타디비 : RDS
- 메세지 브로커 : elasticsearch redis

STEP 2) 모든 EC2 노드에 EFS 마운트
- 추가로 /etc/fstab에 efs 마운트 설정값을 추가하여 EC2를 재부팅 했을시도 마운트 되도록 설정

STEP 3) 모든 EC2 노드에 대해서 airflow 설치 및 관련 라이브러리 일치화

STEP 4) 모든 EC2 노드에 대해서 airflow.cfg 설정
- celery executor 설정
- elasticsearch redis 주소설정
- 메타디비 주소설정
- 등등

STEP 5) 마스터 EC2 노드에서 airflow db init

STEP 6) 마스터 노드에서 restore 복원작업 수행 (나머지 테스크 노드에서는 restore 작업 불필요)
- dags : EFS
- plugins : EFS
- connections : metaDB 
- variables : metaDB

STEP 7) DAG 에러여부 확인
- dag list 등 명령어를 이용해서 확인한다.

STEP 8) airflow 구동
- 마스터 노드 : 웹서버, 스케쥴러, 플라워
- 나머지 테스크 노드 : 워커