---
layout: post
title: "intro to DeepLearning"
tags: [딥러닝]
comments: true
---

.

Deep_Learning_Studynotes_(20190622)

study program : https://www.fastcampus.co.kr/data_camp_deeplearning


#### [학습목표]


- 딥러닝에 대한 전반적인 개요 이해


#### [학습기록]

![1](https://user-images.githubusercontent.com/41605276/59994015-4b561a80-968d-11e9-930f-8f72795a2489.png)

- 1차 연립방정식이고, 1차식이고 미지수가 두개인데 식이 4개이기 때문에 아주 특별한 경우가 아니라면 미지수를 무난하게 구할 수 있다. 이 미지수를 찾는 것이 결국은 뉴런을 이용해서 하고자 하는 것이다. 고양이 사진이 들어오면 고양이라고 인식하는거는 이런 수학적 공식이 베이스가 되는 것이다.


- 딥러닝은 아주 복잡하지만 가장 기본적인 연산으로 들어가면 그렇게 어려운것도 아니다. 어떤 데이터가 들어오면 어떤 가중치를 곱하고 더해주는 원리가 깔려있는 것이다.


- 가중치로 무엇을 곱하는지 찾는 과정이 바로 training(학습)인 것이다. 고양이 이미지에 각각의 숫자를 곱한다음에 다 더해서 어떤 함수를 통과시키면 어떤 숫자 하나가 아웃풋으로 나오는데 이게 1이면 고양이 0이면 개로 만들고 싶은 것이다.


- 그렇다면 어떻게 가중치를 찾을것이냐. 처음에는 아무런 정보가 없기 때문에 가중치를 랜덤값으로 채워넣는다. 그 다음에 최초로 연산된 값이랑 정답이랑 차이를 줄이는 방식으로 가중치를 조금씩 조정하고 결국에는 정답과 가장 가깝게 되도록 하게 된다.

![2](https://user-images.githubusercontent.com/41605276/59994023-5315bf00-968d-11e9-84ee-2e20f1ea363d.png)

- 생물의 뉴런과 인공신경망을 비교해보자. 생물의 신경망은 이전뉴런으로부터 신호를 받아서 축삭돌기로 신호를 뱉는 형식이다. 생물의 신경망이 학습을 한다는 것은 파블로프의 개의 실험으로부터 알 수 있다. 개한테 먹이를 주려고 하면 개가 침을 흘리고, 개한테 먹이를 주면서 종을 치면 나중에는 먹이를 주지 않아도 종만 치게되어도 개가 침을 흘리는 것을 관찰 할 수 있었던 실험이다. 


- 위에 그림에서 볼 수 있듯이 먹이를 보는 신경과 침을 흘리는 신경을 애초에 강하게 연결되어 있었음을 알 수 있다. 그리고 종을 치는 것을 듣는 청각신경은 기존에는 침을 흘리는 신경과는 약하게 연결되어 있었다. 그런데 저렇게 먹이를 줄때마다 종을 울려서 학습을 시키면 나중에는 청각신경과 침샘분비 신경도 강하게 연결이 된다는 것이다. 


- 그래서 뉴런사이의 연결강도가 변하는 현상을 학습이라고 생물학적으로 정의한다. 이 개념을 그대로 인공신경망으로 가져온것이 퍼셉트론이다.

![3](https://user-images.githubusercontent.com/41605276/59994035-59a43680-968d-11e9-947b-8463f370d896.png)

- 신경간의 연결강도를 표현한 것이 바로 가중치이다. 최초에는 이 연결강도에 대한 정보가 없으니까 랜덤값으로 두고 정답과 비교하면서 올바른 강도를 찾아가는 것이다.


- 그리고 활성함수로는 여러가지 함수를 쓰는데 대부분은 non-linear한 함수를 쓰고, 대표적으로 시그모이드 함수가 있다.

![4](https://user-images.githubusercontent.com/41605276/59994042-6032ae00-968d-11e9-9171-c8fad4c2996d.png)

- 뉴럴네트워크는 기본적으로 위와 같은 그래프로 구현할 수 있다.

![5](https://user-images.githubusercontent.com/41605276/59994048-66288f00-968d-11e9-886e-9fe14e1fd977.png)

- 컴퓨터는 0 아니면 1의 데이터를 활용하는 기계이기 때문에 0과 1을 받는 다음과 같은 논리회로를 뉴럴네트워크로 구현할 수 있다.

<img src="6.png" width="500" />

- 뉴럴네트워크가 하나의 레이어가 있으면 직선의 형태는 전부 표현이 가능하나 곡선의 형태의 표현은 레이어 하나만으로는 불가능하다. 대표적인 것이 exclusive nor 같은 것이다.


- 결론적으로 두개이상의 레이어를 쌓아서 non-linear한 활성화 함수를 적용하면 어떠한 형태의 곡선도 전푸 풀 수 있다. 이런 결론을 Universal Function Approximation이라고 한다.

![7](https://user-images.githubusercontent.com/41605276/59994054-6e80ca00-968d-11e9-9074-2374c2a87ec6.png)

- 머신러닝을 사용할 때는 자동차 사진을 넣어서 자동차인지 아닌지를 판별할때 중간에 사람이 바퀴, 차체 등 자동차의 특징을 잘 뽑아내는 필터같은 것을 디자인해서 classification하는 머신에 잘 넣어줘야 했다. 딥러닝 시대에서는 컴퓨팅 자원들이 비약적으로 좋아지면서 저런 특징을 뽑아내는 것도 기계한테 시키게 되었다. 그냥 데이터를 던져주면 기계가 알아서 전부 뽑아주게 되었다. 


- 이렇게 되니까 도메인 지식에 대한 경계가 무너지게 되었다. 머신러닝 시대에서는 자동차의 특징을 잘 뽑는 엔지니어의 도메인 지식이 중요했는데 딥러닝 시대에서는 그럴 필요가 없어졌기 때문이다.


#### 딥러닝을 어렵게 하는 것들

1) vanishing gradient problem

시그모이드 함수를 사용했을때 시그모이드의 수는 0 ~ 1사이의 값이 되는데 이놈들을 계속해서 미분해주게 되면 0으로 점점 작아지게 되는 것이다. 다시말해서 그레디언트 값이 뉴럴네트워크의 뒤로 전달 될수록 점점 작아지는 현상이다. 이는 뒤쪽 레이어(input에 가까운 레이어)에서는 학습이 제대로 이루어지지 않는다는 말이다.

솔루션 : 시그모이드 함수말고 ReLU를 쓰자.

구현도 쉽고 그레디언트 베니싱 현상을 잡아 줄 수 있다.

아래 그림에서 파란색 선이 랠루 함수이다. 오른쪽에는 랠루 함수를 적용한 것을 도식화 한 것으로 하얀색 박스가 음수일 경우라 없어질 것이다.

랠루함수를 미분했을때 음수 쪽은 그냥 날아가는 것이고, 차피 포워드 방향에서도 0으로 바뀌기 때문에 날아갈 것이다.

![9](https://user-images.githubusercontent.com/41605276/59994064-7476ab00-968d-11e9-957d-c6aefe610a47.png)

2) overfitting problem

솔루션 : dropout 기법같은 정규화 방법들을 적용하자.

3) get stuck in local minima

![8](https://user-images.githubusercontent.com/41605276/59994068-79d3f580-968d-11e9-98d2-f1f7752b4f56.png)
