---
layout: post
title: "Keras ì°½ì‹œìë¡œë¶€í„° ë°°ìš°ëŠ” TensorFlow 2.0 + Keras ì˜¤ë²„ë·°ìë£Œ ì‹¤ìŠµ"
tags: [ë”¥ëŸ¬ë‹]
comments: true
---

.

Deep_Learning_TIL_(20200228)

- í•™ìŠµ ì‹œ ì°¸ê³ ìë£Œ(ì¶œì²˜) 

ìë£Œ : Keras ì°½ì‹œìë¡œë¶€í„° ë°°ìš°ëŠ” TensorFlow 2.0 + Keras íŠ¹ê°•

URL : https://colab.research.google.com/drive/1p4RhSj1FEuscyZP81ocn8IeGD_2r46fS?fbclid=IwAR0qED1D3sAQk4oEVC0IolOopC9ur3LlnUsHFLl-YyFHuPUPtUxk4YUBVa0

- ì‹¤ìŠµí™˜ê²½ : Google Colab


```
!pip install tensorflow==2.0.0
```

    Collecting tensorflow==2.0.0
    [?25l  Downloading https://files.pythonhosted.org/packages/46/0f/7bd55361168bb32796b360ad15a25de6966c9c1beb58a8e30c01c8279862/tensorflow-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (86.3MB)
    [K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 86.3MB 52kB/s 
    [?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.34.2)
    Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.27.1)
    Collecting tensorboard<2.1.0,>=2.0.0
    [?25l  Downloading https://files.pythonhosted.org/packages/76/54/99b9d5d52d5cb732f099baaaf7740403e83fe6b0cedde940fabd2b13d75a/tensorboard-2.0.2-py3-none-any.whl (3.8MB)
    [K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.8MB 58.5MB/s 
    [?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.1.0)
    Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.9.0)
    Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.8.1)
    Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.1.0)
    Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.12.0)
    Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (3.10.0)
    Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.0.8)
    Collecting tensorflow-estimator<2.1.0,>=2.0.0
    [?25l  Downloading https://files.pythonhosted.org/packages/fc/08/8b927337b7019c374719145d1dceba21a8bb909b93b1ad6f8fb7d22c1ca1/tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449kB)
    [K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 450kB 70.5MB/s 
    [?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.17.5)
    Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.1.8)
    Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.2.2)
    Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.11.2)
    Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (3.1.0)
    Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (45.1.0)
    Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.0.0)
    Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.7.2)
    Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2.21.0)
    Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.2.1)
    Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.4.1)
    Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==2.0.0) (2.8.0)
    Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.1.1)
    Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (4.0)
    Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.2.8)
    Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2.8)
    Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2019.11.28)
    Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.0.4)
    Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.24.3)
    Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.3.0)
    Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.4.8)
    Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.1.0)
    Installing collected packages: tensorboard, tensorflow-estimator, tensorflow
      Found existing installation: tensorboard 1.15.0
        Uninstalling tensorboard-1.15.0:
          Successfully uninstalled tensorboard-1.15.0
      Found existing installation: tensorflow-estimator 1.15.1
        Uninstalling tensorflow-estimator-1.15.1:
          Successfully uninstalled tensorflow-estimator-1.15.1
      Found existing installation: tensorflow 1.15.0
        Uninstalling tensorflow-1.15.0:
          Successfully uninstalled tensorflow-1.15.0
    Successfully installed tensorboard-2.0.2 tensorflow-2.0.0 tensorflow-estimator-2.0.1
    


```
import tensorflow as tf
print(tf.__version__)
```

    2.0.0
    

#### TensorFlow 2.0 + Keras, ë”¥ëŸ¬ë‹ ì—°êµ¬ìë“¤ì„ ìœ„í•œ ì˜¤ë²„ë·°

*@fchollet, October 2019 (ë²ˆì—­ @chansung)*
- ì›ë³¸ì€ [TensorFlow 2.0 + Keras Overview for Deep Learning Researchers](https://colab.research.google.com/drive/1UCJt8EYjlzCs1H1d1X0iDGYJsHKwu-NO?fbclid=IwAR269Y-3J1DuZL01L6GBCC4dg6RSAmJXHnRfztL454dZ5SqKLRxCAZcxzgY)ì…ë‹ˆë‹¤.
---

**ì´ ë¬¸ì„œëŠ” ì…ë¬¸, íŠ¹ê°•, ê·¸ë¦¬ê³  TensorFlow 2.0ì˜ APIë¥¼ ë¹ ë¥´ê²Œ ì°¸ì¡°í•˜ëŠ” ëª©ì ì„ ìœ„í•´ ì œê³µë©ë‹ˆë‹¤.**

---

TensorFlowì™€ KerasëŠ” ëª¨ë‘ ì•½ 4ë…„ì „ì¯¤ ë¦´ë¦¬ì¦ˆ ë˜ì—ˆìŠµë‹ˆë‹¤ (KerasëŠ” 2015ë…„ 3ì›”, TensorFlowëŠ” 2015ë…„ 11ì›”). ì´ëŠ” ë”¥ëŸ¬ë‹ ì„¸ê³„ì˜ ê´€ì ì—ì„œ ë³¼ ë•Œ, ê½¤ ì˜¤ëœì‹œê°„ì´ë¼ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤!

ê³¼ê±°ì— TensorFlow 1.x + KerasëŠ” ì—¬ëŸ¬ê°€ì§€ ì•Œë ¤ì§„ ë¬¸ì œì ì„ ê°€ì§€ê³  ìˆì—ˆìŠµë‹ˆë‹¤:
- TensorFlowë¥¼ ì‚¬ìš©í•œë‹¤ëŠ”ê²ƒì€ ì •ì ì¸ ê³„ì‚° ê·¸ë˜í”„ë¥¼ ì¡°ì‘í•¨ì„ ì˜ë¯¸í•˜ëŠ”ê²ƒìœ¼ë¡œ, Imperative ì½”ë”© ìŠ¤íƒ€ì¼ì„ ì‚¬ìš©í•˜ëŠ” í”„ë¡œê·¸ë˜ë¨¸ë¡œ í•˜ì—¬ê¸ˆ ì–´ë µê³ , ë¶ˆí¸í•œ ëŠë‚Œì„ ë°›ê²Œ í–ˆì—ˆìŠµë‹ˆë‹¤.
- TensorFlow APIê°€ ë§¤ìš° ê°•ë ¥í•˜ë©´ì„œë„ ìœ ì—°í•˜ì§€ë§Œ, ë¹ ë¥¸ ì½”ë“œì˜ ì‘ì„±ì˜ ê°€ëŠ¥ì„±ì´ ê²°ì—¬ë˜ì–´ ìˆì—ˆìœ¼ë©° ì¢…ì¢… ì‚¬ìš©ë²•ì€ ì–´ë µê³  í˜¼ë€ìŠ¤ëŸ¬ì› ìŠµë‹ˆë‹¤.
- KerasëŠ” ë§¤ìš° ìƒì‚°ì ì´ê³  ì‚¬ìš©ì´ ì‰½ì§€ë§Œ, ì—°êµ¬ì— ì‚¬ìš©ëœ ì‚¬ë¡€ì—ì„œ ì¢…ì¢… ìœ ì—°ì„±ì´ ê²°ì—¬ë˜ì—ˆì—ˆìŠµë‹ˆë‹¤.

---
### TensorFlow 2.0ì€ TensorFlowì™€ Kerasë¥¼ ëŒ€ëŒ€ì ìœ¼ë¡œ ìƒˆë¡œì´ ë””ìì¸í•œ ê²ƒìœ¼ë¡œ, ì§€ë‚œ 4ë…„ê°„ì˜ ì‚¬ìš©ì í”¼ë“œë°±ê³¼ ê¸°ìˆ ì˜ ì§„ë³´ê°€ ëª¨ë‘ ê³ ë ¤ë˜ì—ˆìŠµë‹ˆë‹¤. ìœ„ì—ì„œ ì–¸ê¸‰ëœ ë¬¸ì œì ë“¤ì„ ëŒ€ê·œëª¨ë¡œ ìˆ˜ì •í•©ë‹ˆë‹¤.

### ë¯¸ë˜ì—ì„œì˜¨ ì°¨ì„¸ëŒ€ ë¨¸ì‹ ëŸ¬ë‹ í”Œë«í¼ì…ë‹ˆë‹¤

---

TensorFlow 2.0ì€ ì•„ë˜ì™€ ê°™ì€ ì£¼ìš” ì•„ì´ë””ì–´ì— ê¸°ë°˜í•˜ê³  ìˆìŠµë‹ˆë‹¤:

- ì‚¬ìš©ìë“¤ì´ ê³„ì‚°ì„ eagerlyí•˜ê²Œ ìˆ˜í–‰í•  ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤. ì´ëŠ” Numpyë¥¼ ì‚¬ìš©í•˜ëŠ”ë²•ê³¼ ìœ ì‚¬í•©ë‹ˆë‹¤. ì´ëŠ” TensorFlow 2.0ì„ ì´ìš©í•œ í”„ë¡œê·¸ë˜ë°ì´ ì§ê´€ì ì´ë©° ë™ì‹œì— íŒŒì´í† ë‹‰í•  ìˆ˜ ìˆê²Œë” í•´ ì¤ë‹ˆë‹¤.
- ì»´íŒŒì¼ëœ ê·¸ë˜í”„ì˜ ì—„ì²­ë‚œ ì´ì ì„ ê·¸ëŒ€ë¡œ ë³´ì¡´í•˜ëŠ”ë°, ì´ëŠ” ì„±ëŠ¥, ë¶„ì‚°, ê·¸ë¦¬ê³  ë°°í¬ë¥¼ ìœ„í•¨ì…ë‹ˆë‹¤. ì´ ë‚´ìš©ì€ TensorFlowë¥¼ ë¹ ë¥´ê³ , ë¶„ì‚° êµ¬ì¡°ì—ì„œì˜ í™•ì¥ ê°€ëŠ¥í•˜ë©°, ìƒìš©í™”ì— ì¤€ë¹„ë  ìˆ˜ ìˆë„ë¡ í•´ ì¤ë‹ˆë‹¤.
- Kerasë¥¼ ë”¥ëŸ¬ë‹ì˜ ê³ ìˆ˜ì¤€ APIë¡œ ì±„íƒí•˜ì—¬, TensorFlowë¥¼ ì´í•´í•˜ê¸° ì‰¬ìš°ë©´ì„œë„ ë†’ì€ ìƒì‚°ì„±ì„ ê°€ì§ˆ ìˆ˜ ìˆê²Œ ë§Œë“¤ì–´ ì¤ë‹ˆë‹¤.
- ë§¤ìš° ê³ ìˆ˜ì¤€(ë” ì‰¬ìš´ ì‚¬ìš©ì„±, ì•½ê°„ ë¶€ì¡±í•œ ìœ ì—°ì„±) ì—ì„œë¶€í„° ë§¤ìš° ì €ìˆ˜ì¤€(ë” ê¹Šì€ ì „ë¬¸ì„±, ë§¤ìš° ë›°ì–´ë‚œ ìœ ì—°ì„±)ì˜ ë‹¤ì–‘í•œ ë²”ìœ„ì˜ ì‘ì—…ìœ¼ë¡œê¹Œì§€ Kerasë¥¼ í™•ì¥í•©ë‹ˆë‹¤.


# íŒŒíŠ¸ 1: TensorFlowì˜ ê¸°ë³¸

## Tensors (í…ì„œ)

ë‹¤ìŒì€ [ìƒìˆ˜í˜•](https://www.tensorflow.org/api_docs/python/tf/constant) í…ì„œ ì…ë‹ˆë‹¤:


```
x = tf.constant([[5, 2], [1, 3]])
print(x)
```

    tf.Tensor(
    [[5 2]
     [1 3]], shape=(2, 2), dtype=int32)
    

í•´ë‹¹ í…ì„œì˜ ê°’ì„ Numpy ë°°ì—´í˜•íƒœë¡œ ê°€ì ¸ì˜¤ê³  ì‹¶ë‹¤ë©´ `.numpy()`ë¥¼ í˜¸ì¶œí•˜ë©´ ë©ë‹ˆë‹¤:


```
x.numpy()
```




    array([[5, 2],
           [1, 3]], dtype=int32)



Numpy ë°°ì—´ê³¼ *ê½¤ë‚˜* ìœ ì‚¬í•œ ì ìœ¼ë¡œ `dtype`ê³¼ `shape`ì´ë¼ëŠ” ì†ì„±ì„ ê°€ì§‘ë‹ˆë‹¤:


```
print('dtype:', x.dtype)
print('shape:', x.shape)
```

    dtype: <dtype: 'int32'>
    shape: (2, 2)
    

ìƒìˆ˜í˜• í…ì„œë¥¼ ìƒì„±í•˜ëŠ” ë³´í¸ì ì¸ ë°©ë²•ì€ `tf.ones`ê³¼ `tf.zeros`ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤(ì´ëŠ” Numpyì˜ `np.ones` ë° `np.zeros`ì™€ ìœ ì‚¬í•©ë‹ˆë‹¤):


```
print(tf.ones(shape=(2, 1)))
print(tf.zeros(shape=(2, 1)))
```

    tf.Tensor(
    [[1.]
     [1.]], shape=(2, 1), dtype=float32)
    tf.Tensor(
    [[0.]
     [0.]], shape=(2, 1), dtype=float32)
    

## ëœë¤í•œ ìƒìˆ˜í˜• í…ì„œ

ë‹¤ìŒì€ ëœë¤í•œ [ì •ê·œë¶„í¬](https://www.tensorflow.org/api_docs/python/tf/random/normal)ë¡œë¶€í„° ìƒìˆ˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤:


```
tf.random.normal(shape=(2, 2), mean=0., stddev=1.)
```




    <tf.Tensor: id=12, shape=(2, 2), dtype=float32, numpy=
    array([[-1.0932361,  2.228664 ],
           [-0.8287051, -0.9424461]], dtype=float32)>



*ê·¸ë¦¬ê³ * ë‹¤ìŒì€ ëœë¤í•œ [ê· ë“±ë¶„í¬](https://www.tensorflow.org/api_docs/python/tf/random/uniform)ë¡œë¶€í„° ê°’ì´ ì±„ì›Œì§€ëŠ” ì •ìˆ˜í˜• í…ì„œë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤:


```
tf.random.uniform(shape=(2, 2), minval=0, maxval=10, dtype='int32')
```




    <tf.Tensor: id=16, shape=(2, 2), dtype=int32, numpy=
    array([[0, 3],
           [5, 1]], dtype=int32)>



## Variables (ë³€ìˆ˜)

[Variables](https://www.tensorflow.org/guide/variable)ëŠ” ë³€í•  ìˆ˜ ìˆëŠ” ìƒíƒœ(ë‰´ëŸ´ë„·ì˜ ê°€ì¤‘ì¹˜ì™€ ê°™ì€)ë¥¼ ì €ì¥í•˜ëŠ”ë° ì‚¬ìš©ë˜ëŠ” íŠ¹ë³„í•œ í…ì„œ ì…ë‹ˆë‹¤. ì´ˆê¸°ê°’ì„ ì‚¬ìš©í•´ì„œ Variableì„ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:


```
initial_value = tf.random.normal(shape=(2, 2))
a = tf.Variable(initial_value)
print(a)
```

    <tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=
    array([[-1.4263921 ,  0.49103293],
           [-0.36253545,  1.8493237 ]], dtype=float32)>
    

`.assign(value)`, `.assign_add(increment)`, ë˜ëŠ” `.assign_sub(decrement)`ì™€ ê°™ì€ ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•´ì„œ Variableì˜ ê°’ì„ ê°±ì‹ í•©ë‹ˆë‹¤:


```
new_value = tf.random.normal(shape=(2, 2))
a.assign(new_value)
for i in range(2):
  for j in range(2):
    assert a[i, j] == new_value[i, j]

print(a)
```

    <tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=
    array([[ 1.5460566 ,  1.1849345 ],
           [ 0.24738911, -0.30803028]], dtype=float32)>
    


```
added_value = tf.random.normal(shape=(2, 2))
a.assign_add(added_value)
for i in range(2):
  for j in range(2):
    assert a[i, j] == new_value[i, j] + added_value[i, j]

print(a)
```

    <tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=
    array([[ 0.83296186,  0.58070505],
           [ 1.7173103 , -0.678332  ]], dtype=float32)>
    

## TensorFlowì—ì„œ ìˆ˜í•™ì„ í•˜ëŠ”ê²ƒ

TensorFlowëŠ” Numpyë¥¼ ì‚¬ìš©í•˜ëŠ”ê²ƒê³¼ ì •í™•íˆ ë˜‘ê°™ì€ ë°©ë²•ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ë‘˜ì˜ ì£¼ìš” ë‹¤ë¥¸ì ì€ ì‘ì„±í•œ TensorFlowì˜ ì½”ë“œëŠ” GPUì™€ TPU ìƒì—ì„œ ì‹¤í–‰ë  ìˆ˜ ìˆë‹¤ëŠ” ì ì…ë‹ˆë‹¤:


```
a = tf.random.normal(shape=(2, 2))
b = tf.random.normal(shape=(2, 2))

c = a + b
d = tf.square(c)
e = tf.exp(d)

print(a,'\n',b,'\n',c,'\n',d,'\n',e)
```

    tf.Tensor(
    [[-1.7188389  -0.715927  ]
     [-0.14043556  0.7175207 ]], shape=(2, 2), dtype=float32) 
     tf.Tensor(
    [[-0.36652637  0.55603373]
     [ 0.89047074  0.614373  ]], shape=(2, 2), dtype=float32) 
     tf.Tensor(
    [[-2.0853653  -0.15989327]
     [ 0.75003517  1.3318937 ]], shape=(2, 2), dtype=float32) 
     tf.Tensor(
    [[4.348748   0.02556586]
     [0.56255275 1.7739408 ]], shape=(2, 2), dtype=float32) 
     tf.Tensor(
    [[77.38154    1.0258955]
     [ 1.7551472  5.894035 ]], shape=(2, 2), dtype=float32)
    

## `GradientTape`ì„ ì‚¬ìš©í•´ì„œ ê²½ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ëŠ”ê²ƒ

í•œ ê°€ì§€ ë” Numpyì™€ì˜ í° ì°¨ì´ì ì´ ìˆìŠµë‹ˆë‹¤: ëª¨ë“  ë¯¸ë¶„ê°€ëŠ¥í•œ í‘œí˜„ì— ëŒ€í•´ì„œ, ìë™ìœ¼ë¡œ ê²½ì‚¬ë„ë¥¼ êµ¬í•˜ëŠ” ê²ƒì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.

ë‹¨ìˆœíˆ [`GradientTape`](https://www.tensorflow.org/api_docs/python/tf/GradientTape)ë¥¼ ì—´ê²Œë˜ë©´, ê·¸ë•Œë¶€í„´ `tape.watch()`ë¥¼ í†µí•´ í…ì„œë¥¼ í™•ì¸í•˜ê³ , ì´ í…ì„œë¥¼ ì…ë ¥ìœ¼ë¡œì¨ ì‚¬ìš©í•˜ëŠ” ë¯¸ë¶„ê°€ëŠ¥í•œ í‘œí˜„ì„ êµ¬ì„±í•˜ëŠ”ê²ƒì´ ê°€ëŠ¥í•©ë‹ˆë‹¤:


```
a = tf.random.normal(shape=(2, 2))
b = tf.random.normal(shape=(2, 2))

with tf.GradientTape() as tape:
  tape.watch(a)  # `a`ì— ì ìš©ë˜ëŠ” ì—°ì‚°ì˜ íˆìŠ¤í† ë¦¬ì— ëŒ€í•œ ê¸°ë¡ì„ ì‹œì‘
  c = tf.sqrt(tf.square(a) + tf.square(b))  # `a`ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª‡ ê°€ì§€ ìˆ˜í•™ì„ ìˆ˜í–‰
  # `a`ì— ëŒ€í•œ `c`ì˜ ê²½ì‚¬ë„ëŠ” ë¬´ì—‡ì¸ê°€?
  dc_da = tape.gradient(c, a)
  print(dc_da)
```

    tf.Tensor(
    [[ 0.94430184  0.04223739]
     [-0.860221    0.6537137 ]], shape=(2, 2), dtype=float32)
    

ë””í´íŠ¸ë¡œëŠ” Variableë“¤ì€ ìë™ìœ¼ë¡œ watchê°€ ì ìš©ë˜ì–´ ìˆê¸° ë•Œë¬¸ì—, ìˆ˜ë™ìœ¼ë¡œ `watch`ë¥¼ í•´ ì¤„ í•„ìš”ëŠ” ì—†ìŠµë‹ˆë‹¤:


```
a = tf.Variable(a)

with tf.GradientTape() as tape:
  c = tf.sqrt(tf.square(a) + tf.square(b))
  dc_da = tape.gradient(c, a)
  print(dc_da)
```

    tf.Tensor(
    [[ 0.94430184  0.04223739]
     [-0.860221    0.6537137 ]], shape=(2, 2), dtype=float32)
    

GradientTapeì„ ì¤‘ì²©ì‹œì¼œì„œ ê³ ì°¨ì›ì˜ ë¯¸ë¶„ì„ ê³„ì‚°í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤:


```
with tf.GradientTape() as outer_tape:
  with tf.GradientTape() as tape:
    c = tf.sqrt(tf.square(a) + tf.square(b))
    dc_da = tape.gradient(c, a)
  d2c_da2 = outer_tape.gradient(dc_da, a)
  print(d2c_da2)
```

    tf.Tensor(
    [[0.14804387 0.35640335]
     [0.40593112 2.9766662 ]], shape=(2, 2), dtype=float32)
    

## end-to-end ì˜ˆì œ: ì„ í˜• íšŒê·€

ì§€ê¸ˆê¹Œì§€ TensorFlowëŠ” Numpyì™€ ë¹„ìŠ·í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì¸ë°, ì¶”ê°€ì ìœ¼ë¡œ GPU ë˜ëŠ” TPUë¥¼ í†µí•´ ê°€ì†ë  ìˆ˜ ìˆê³ , ìë™ìœ¼ë¡œ ë¯¸ë¶„ì´ ê³„ì‚°ëœë‹¤ëŠ” ë‚´ìš©ì„ ë°°ì› ìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë©´ ì´ì œëŠ” end-to-end ì˜ˆì œë¥¼ ì•Œì•„ë³¼ ì‹œê°„ì…ë‹ˆë‹¤: ë¨¸ì‹ ëŸ¬ë‹ì˜ í”¼ì¦ˆë²„ì¦ˆì¸, ì„ í˜• íšŒê·€ë¥¼ êµ¬í˜„í•´ ë´…ì‹œë‹¤. 

ì´ë¥¼ ë³´ì—¬ì£¼ê¸° ìœ„í•´ì„œ, `Layer` ë˜ëŠ” `MeanSquaredError`ì™€ ê°™ì€ Kerasì˜ ê³ ìˆ˜ì¤€ ì»´í¬ë„ŒíŠ¸ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šì„ ê²ƒì…ë‹ˆë‹¤. ë‹¨ì§€ ê¸°ë³¸ì ì¸ ì—°ì‚°ìë§Œì„ ì‚¬ìš©í•©ë‹ˆë‹¤.


```
input_dim = 2
output_dim = 1
learning_rate = 0.01

# ê°€ì¤‘ì¹˜ í–‰ë ¬ì…ë‹ˆë‹¤
w = tf.Variable(tf.random.uniform(shape=(input_dim, output_dim)))
# í¸í–¥ ë²¡í„°ì…ë‹ˆë‹¤
b = tf.Variable(tf.zeros(shape=(output_dim,)))

def compute_predictions(features):
  return tf.matmul(features, w) + b

def compute_loss(labels, predictions):
  return tf.reduce_mean(tf.square(labels - predictions))

def train_on_batch(x, y):
  with tf.GradientTape() as tape:
    predictions = compute_predictions(x)
    loss = compute_loss(y, predictions)
    dloss_dw, dloss_db = tape.gradient(loss, [w, b])
  w.assign_sub(learning_rate * dloss_dw)
  b.assign_sub(learning_rate * dloss_db)
  return loss
```

ì‘ì„±í•œ ëª¨ë¸ì„ ê²€ì¦í•˜ê¸° ìœ„í•œ, ì¸ê³µì ì¸ ë°ì´í„°ë¥¼ ìƒì„±í•´ ë³´ê² ìŠµë‹ˆë‹¤:


```
import numpy as np
import random
import matplotlib.pyplot as plt
%matplotlib inline

# ë°ì´í„°ì…‹ì„ ì¤€ë¹„í•©ë‹ˆë‹¤
num_samples = 10000
negative_samples = np.random.multivariate_normal(
    mean=[0, 3], cov=[[1, 0.5],[0.5, 1]], size=num_samples)
positive_samples = np.random.multivariate_normal(
    mean=[3, 0], cov=[[1, 0.5],[0.5, 1]], size=num_samples)
features = np.vstack((negative_samples, positive_samples)).astype(np.float32)
labels = np.vstack((np.zeros((num_samples, 1), dtype='float32'),
                    np.ones((num_samples, 1), dtype='float32')))

plt.scatter(features[:, 0], features[:, 1], c=labels[:, 0])
```




    <matplotlib.collections.PathCollection at 0x7ff0b5880550>




![1](https://user-images.githubusercontent.com/41605276/75503945-6a762a80-5a1a-11ea-9ad1-f20581707e29.png)


ê·¸ëŸ¬ë©´, ë°ì´í„°ì˜ ë°°ì¹˜í¬ê¸° ë‹¨ìœ„ë¡œ ëŒë©´ì„œ, `train_on_batch` í•¨ìˆ˜ë¥¼ ë°˜ë³µì ìœ¼ë¡œ í˜¸ì¶œí•˜ì—¬ ì„ í˜• íšŒê·€ ëª¨ë¸ì„ í•™ìŠµì‹œì¼œ ë´…ì‹œë‹¤:


```
# ë°ì´í„°ë¥¼ ë¬´ì‘ìœ„ë¡œ ì„ìŠµë‹ˆë‹¤
random.Random(1337).shuffle(features)
random.Random(1337).shuffle(labels)

# ì†ì‰½ê²Œ ë°°ì¹˜í™”ëœ ë°˜ë³µì„ ìœ„í•´, tf.data.Dataset ê°ì²´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤
dataset = tf.data.Dataset.from_tensor_slices((features, labels))
dataset = dataset.shuffle(buffer_size=1024).batch(256)

for epoch in range(10):
  for step, (x, y) in enumerate(dataset):
    loss = train_on_batch(x, y)
  print('Epoch %d: ë§ˆì§€ë§‰ ë°°ì¹˜ì˜ ì†ì‹¤ê°’ = %.4f' % (epoch, float(loss)))
```

    Epoch 0: ë§ˆì§€ë§‰ ë°°ì¹˜ì˜ ì†ì‹¤ê°’ = 0.0537
    Epoch 1: ë§ˆì§€ë§‰ ë°°ì¹˜ì˜ ì†ì‹¤ê°’ = 0.0898
    Epoch 2: ë§ˆì§€ë§‰ ë°°ì¹˜ì˜ ì†ì‹¤ê°’ = 0.0370
    Epoch 3: ë§ˆì§€ë§‰ ë°°ì¹˜ì˜ ì†ì‹¤ê°’ = 0.0332
    Epoch 4: ë§ˆì§€ë§‰ ë°°ì¹˜ì˜ ì†ì‹¤ê°’ = 0.0399
    Epoch 5: ë§ˆì§€ë§‰ ë°°ì¹˜ì˜ ì†ì‹¤ê°’ = 0.0331
    Epoch 6: ë§ˆì§€ë§‰ ë°°ì¹˜ì˜ ì†ì‹¤ê°’ = 0.0174
    Epoch 7: ë§ˆì§€ë§‰ ë°°ì¹˜ì˜ ì†ì‹¤ê°’ = 0.0242
    Epoch 8: ë§ˆì§€ë§‰ ë°°ì¹˜ì˜ ì†ì‹¤ê°’ = 0.0270
    Epoch 9: ë§ˆì§€ë§‰ ë°°ì¹˜ì˜ ì†ì‹¤ê°’ = 0.0301
    

ì•„ë˜ëŠ” ìš°ë¦¬ê°€ ë§Œë“  ëª¨ë¸ì´ ì–¼ë§ˆë‚˜ ì˜ ë™ì‘í•˜ëŠ”ì§€ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤:


```
predictions = compute_predictions(features)
plt.scatter(features[:, 0], features[:, 1], c=predictions[:, 0] > 0.5)
```




    <matplotlib.collections.PathCollection at 0x7ff0b1b73080>




![1](https://user-images.githubusercontent.com/41605276/75503945-6a762a80-5a1a-11ea-9ad1-f20581707e29.png)


## `tf.function`ë¥¼ ì´ìš©í•´ì„œ ì†ë„ë¥¼ ë¹ ë¥´ê²Œ í•˜ê¸°

í˜„ì¬ì˜ ì½”ë“œëŠ” ì–¼ë§ˆë‚˜ ë¹¨ë¦¬ ìˆ˜í–‰ë ê¹Œìš”?


```
import time

t0 = time.time()
for epoch in range(20):
  for step, (x, y) in enumerate(dataset):
    loss = train_on_batch(x, y)
t_end = time.time() - t0
print('epochë‹¹ ê±¸ë¦° ì‹œê°„: %.3f ì´ˆ' % (t_end / 20,))
```

    epochë‹¹ ê±¸ë¦° ì‹œê°„: 0.117 ì´ˆ
    

í•™ìŠµ í•¨ìˆ˜ë¥¼ ì •ì  ê·¸ë˜í”„ë¡œ ì»´íŒŒì¼ í•´ ë´…ì‹œë‹¤. ì´ë¥¼ ìœ„í•´ì„œ í•´ì•¼í•  ê²ƒì€ ë¬¸ì ê·¸ëŒ€ë¡œ, `tf.function`ì´ë¼ëŠ” ë°ì½”ë ˆì´í„°ë¥¼ ìœ„ì— ë„£ì–´ì£¼ëŠ”ê²ƒ ë¿ì…ë‹ˆë‹¤:


```
@tf.function
def train_on_batch(x, y):
  with tf.GradientTape() as tape:
    predictions = compute_predictions(x)
    loss = compute_loss(y, predictions)
    dloss_dw, dloss_db = tape.gradient(loss, [w, b])
  w.assign_sub(learning_rate * dloss_dw)
  b.assign_sub(learning_rate * dloss_db)
  return loss
```

ë‹¤ì‹œí•œë²ˆ ì‹œê°„ì„ ì¸¡ì •í•´ ë´…ì‹œë‹¤:


```
t0 = time.time()
for epoch in range(20):
  for step, (x, y) in enumerate(dataset):
    loss = train_on_batch(x, y)
t_end = time.time() - t0
print('epochë‹¹ ê±¸ë¦° ì‹œê°„: %.3f ì´ˆ' % (t_end / 20,))
```

    epochë‹¹ ê±¸ë¦° ì‹œê°„: 0.078 ì´ˆ
    

ê±¸ë¦° ì‹œê°„ì´ ì•½ 40% ê°ì†Œí–ˆìŠµë‹ˆë‹¤. ì´ ê²½ìš°, ë§¤ìš° ê°„ë‹¨í•œ ëª¨ë¸ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤; ì¼ë°˜ì ìœ¼ë¡œ ëª¨ë¸ì´ í¬ë©´ í´ ìˆ˜ë¡, ì •ì  ê·¸ë˜í”„ë¥¼ í™œìš©í•œ ì†ë„ ê°œì„ ì€ ë” ë§ì´ ì´ë¤„ì§‘ë‹ˆë‹¤.

ê¸°ì–µí•´ì•¼í•  ê²ƒì´ ìˆìŠµë‹ˆë‹¤: eager ì‹¤í–‰ëª¨ë“œëŠ” ë””ë²„ê¹…ê³¼ ì½”ë“œ ë¼ì¸ë³„ ê²°ê³¼ë¥¼ ì¶œë ¥í•˜ëŠ”ë° ë§¤ìš° ìœ ìš©í•˜ì§€ë§Œ, í¬ê¸°ë¥¼ í‚¤ì›Œì•¼í•  ì‹œê¸°ê°€ ì˜¤ë©´, ì •ì  ê·¸ë˜í”„ê°€ ì—°êµ¬ìë“¤ì—ê²Œ ìµœê³ ì˜ ì¹œêµ¬ê°€ ë  ê²ƒì…ë‹ˆë‹¤.

# íŒŒíŠ¸ 2: Keras API

KerasëŠ” ë”¥ëŸ¬ë‹ì„ ìœ„í•œ íŒŒì´ì¬ API ì…ë‹ˆë‹¤. ëª¨ë‘ê°€ ì‚¬ìš©í• ë§Œí•œ ë‚´ìš©ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤:

- ì—”ì§€ë‹ˆì–´ì˜ ê²½ìš°, KerasëŠ” ê³„ì¸µ, í‰ê°€ì§€í‘œ(metrics), í•™ìŠµ ë°˜ë³µë¬¸ê³¼ ê°™ì€ ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ë¸”ë¡ì„ ì œê³µí•˜ì—¬ ì¼ë°˜ì ì€ ì‚¬ìš© ì‚¬ë¡€ë¥¼ ì§€ì›í•©ë‹ˆë‹¤. ê³ ìˆ˜ì¤€ì˜ ì‚¬ìš©ì ê²½í—˜ì„ ì œê³µí•˜ì—¬ ì ‘ê·¼ì´ ìš©ì´í•˜ê³ , ìƒì‚°ì„±ì´ ì¢‹ìŠµë‹ˆë‹¤.

- ì—°êµ¬ìì˜ ê²½ìš°, ê³„ì¸µì´ë‚˜ í•™ìŠµ ë°˜ëª©ë¬¸ê³¼ ê°™ì€ ì´ë¯¸ ì œê³µë˜ëŠ” ë¸”ë¡ì˜ ì‚¬ìš©ì„ ì„ í˜¸í•˜ì§€ ì•Šê³ , ìŠ¤ìŠ¤ë¡œ ë§Œë“  ê²ƒì„ ëŒ€ì‹  ì‚¬ìš©í•  ì§€ë„ ëª¨ë¦…ë‹ˆë‹¤. ë¬¼ë¡ , KerasëŠ” ì´ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•´ ì¤ë‹ˆë‹¤. ì´ ê²½ìš°, KerasëŠ” ì—¬ëŸ¬ë¶„ì´ ì‘ì„±í•˜ê²Œë  ë¸”ë¡ì— ëŒ€í•œ í…œí”Œë¦¿ì„ Layers ë° Metricsì™€ ê°™ì€ í‘œì¤€ì ì¸ APIì™€ í•¨ê»˜ ì œê³µí•©ë‹ˆë‹¤. ì´ëŸ¬í•œ êµ¬ì¡°ëŠ” ë‹¤ë¥¸ ì‚¬ëŒê³¼ ì½”ë“œë¥¼ ì‰½ê²Œ ê³µìœ í•˜ê³ , ìƒìš©ì˜ ì‘ì—… íë¦„ì—ë„ í†µí•©ë  ìˆ˜ ìˆê²Œë” í•´ ì¤ë‹ˆë‹¤.

- ì´ ê°™ì€ ë‚´ìš©ì€ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ê°œë°œí•˜ëŠ” ë¶„ë“¤ì—ê²Œë„ ì ìš©ë˜ëŠ” ì‚¬ì‹¤ì…ë‹ˆë‹¤. TensorFlowëŠ” ê±°ëŒ€í•œ ìƒíƒœê³„ì£ . ìˆ˜ ë§ì€ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì¡´ì¬í•©ë‹ˆë‹¤. ì„œë¡œë‹¤ë¥¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ìƒí˜¸ì‘ìš©í•˜ê³ , ì´ë“¤ì˜ ì»´í¬ë„ŒíŠ¸ë¥¼ ê³µìœ í•  ìˆ˜ ìˆê²Œí•˜ê¸° ìœ„í•´ì„  API í‘œì¤€ì„ ë”°ë¼ì•¼ë§Œ í•©ë‹ˆë‹¤. API í‘œì¤€ì´ ê³§ Kerasê°€ ì œê³µí•˜ëŠ” í•µì‹¬ì…ë‹ˆë‹¤.


KerasëŠ” ê²°ì •ì ìœ¼ë¡œ ê³ ìˆ˜ì¤€ì˜ UXì™€ ì €ìˆ˜ì¤€ì˜ ìœ ì—°ì„±ì„ ëª¨ë‘ í•¨ê»˜ ì™„ë§Œíˆ ë„ì…í•©ë‹ˆë‹¤. ì´ëŠ” ë”ì´ìƒ í•œí¸ìœ¼ë¡  ì‚¬ìš©ì„±ì´ ë›°ì–´ë‚˜ì§€ë§Œ ìœ ì—°ì¹˜ëŠ” ëª»í•œ ê³ ìˆ˜ì¤€ APIë¥¼, ë‹¤ë¥¸ í•œí¸ìœ¼ë¡  ë§¤ìš° ìœ ì—°í•˜ì§€ë§Œ ì „ë¬¸ê°€ë§Œì´ ì‚¬ìš©ê°€ëŠ¥í•œ ì €ìˆ˜ì¤€ APIë¥¼ ê°€ì ¸ì•¼ë§Œ í•˜ëŠ” ìƒí™©ì—ì„œ ë²—ì–´ë‚˜ê²Œ í•´ ì¤ë‹ˆë‹¤. ê·¸ ëŒ€ì‹ , ë§¤ìš° ê³ ìˆ˜ì¤€ì—ì„œë¶€í„° ë§¤ìš° ì €ìˆ˜ì¤€ ê¹Œì§€ì˜ ë‹¤ì–‘í•œ ì‘ì—… íë¦„ì˜ ë²”ìœ„ë¥¼ ê°€ì§ˆ ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤. ì´ ì‘ì—…íë¦„ì´ë€, ë™ì¼í•œ ì»¨ì…‰ê³¼ ê°ì²´ì— ê¸°ë°˜í•´ì„œ ë§Œë“¤ì–´ì¡Œê¸° ë•Œë¬¸ì— ëª¨ë“ ê²ƒì´ ìƒí˜¸ í˜¸í™˜ ê°€ëŠ¥í•œ ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

![Keras ì‘ì—… íë¦„ì˜ ë²”ìœ„](https://drive.google.com/uc?export=view&id=1bE0FiQY2XF5QzBLRHfe7-SdxvwGIO0GK)

## `Layer` ê¸°ë³¸ í´ë˜ìŠ¤

ê°€ì¥ ì²« ë²ˆì§¸ë¡œ ì•Œì•„ì•¼í•  í´ë˜ìŠ¤ëŠ” [`Layer`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer) ì…ë‹ˆë‹¤. Kerasì˜ ê±°ì˜ ëª¨ë“ ê²ƒì€ ì´ í´ë˜ìŠ¤ë¡œë¶€í„° íŒŒìƒë©ë‹ˆë‹¤.

LayerëŠ” ìƒíƒœ(ê°€ì¤‘ì¹˜, weights)ì™€ ëª‡ (`call` ë©”ì†Œë“œì— ì •ì˜ëœ)ê³„ì‚°ì„ ìº¡ìŠí™” í•©ë‹ˆë‹¤.


```
from tensorflow.keras.layers import Layer

class Linear(Layer):
  """y = w.x + b"""

  def __init__(self, units=32, input_dim=32):
      super(Linear, self).__init__()
      w_init = tf.random_normal_initializer()
      self.w = tf.Variable(
          initial_value=w_init(shape=(input_dim, units), dtype='float32'),
          trainable=True)
      b_init = tf.zeros_initializer()
      self.b = tf.Variable(
          initial_value=b_init(shape=(units,), dtype='float32'),
          trainable=True)

  def call(self, inputs):
      return tf.matmul(inputs, self.w) + self.b

# ìš°ë¦¬ê°€ ë§Œë“  Layerê°ì²´ë¥¼ ì¸ìŠ¤í„´ìŠ¤í™” í•©ë‹ˆë‹¤
linear_layer = Linear(4, 2)
```

Layer ì¸ìŠ¤í„´ìŠ¤ëŠ” ë§ˆì¹˜ í•¨ìˆ˜ì²˜ëŸ¼ ë™ì‘í•©ë‹ˆë‹¤. ëª‡ ë°ì´í„°ì— ëŒ€í•´ì„œ ì´ë¥¼ í˜¸ì¶œí•´ ë´…ì‹œë‹¤:


```
y = linear_layer(tf.ones((2, 2)))
assert y.shape == (2, 4)
```

`Layer` í´ë˜ìŠ¤ëŠ” ì†ì„±ìœ¼ë¡œì¨ ë¶€ì—¬ëœ weightsë¥¼ í†µí•´ì„œ, ê°€ì¤‘ì¹˜ë“¤ì„ ì¶”ì í•©ë‹ˆë‹¤ 


```
# ê°€ì¤‘ì¹˜ëŠ” ìë™ìœ¼ë¡œ `weights`ë¼ëŠ” ì†ì„±ìœ¼ë¡œì¨ ì¶”ì ë©ë‹ˆë‹¤.
assert linear_layer.weights == [linear_layer.w, linear_layer.b]
```

`add_weight`ë¥¼ ì´ìš©í•˜ì—¬ ê°„ë‹¨íˆ ê°€ì¤‘ì¹˜ë¥¼ ìƒì„±í•˜ëŠ” ë°©ë²•ì´ ìˆëŠ”ê²ƒë„ ì•Œì•„ë‘ì„¸ìš”. ì´ë ‡ê²Œ ì½”ë“œë¥¼ ì‘ì„±í•˜ëŠ”ê²ƒ ëŒ€ì‹ :

```python
w_init = tf.random_normal_initializer()
self.w = tf.Variable(initial_value=w_init(shape=shape, dtype='float32'))
```

ì¼ë°˜ì ìœ¼ë¡œ ì•„ë˜ì™€ ê°™ì´ ì‘ì„±í•©ë‹ˆë‹¤:

```python
self.w = self.add_weight(shape=shape, initializer='random_normal')
```

`build`ë¼ëŠ” ë³„ë„ì˜ ë©”ì†Œë“œì—ì„œ ê°€ì¤‘ì¹˜ë¥¼ ìƒì„±í•˜ëŠ”ê²ƒì´ ì¢‹ì€ ê´€ë¡€ì…ë‹ˆë‹¤. ì´ `build`ëŠ” Layerì— ì˜í•´ ì²« ë²ˆì§¸ ì…ë ¥ì˜ Shapeì´ í™•ì¸ë˜ëŠ” ìˆœê°„ í˜¸ì¶œë˜ëŠ” lazyí•œ ë©”ì†Œë“œ ì…ë‹ˆë‹¤. ì´ëŸ¬í•œ íŒ¨í„´ì€ ì…ë ¥ ì°¨ì›(input_dim)ì„ ìƒì„±ìì— ëª…ì‹œí•˜ì§€ ì•Šì•„ë„ ë˜ê²Œ í•´ ì¤ë‹ˆë‹¤:


```
class Linear(Layer):
  """y = w.x + b"""

  def __init__(self, units=32):
      super(Linear, self).__init__()
      self.units = units

  def build(self, input_shape):
      self.w = self.add_weight(shape=(input_shape[-1], self.units),
                               initializer='random_normal',
                               trainable=True)
      self.b = self.add_weight(shape=(self.units,),
                               initializer='random_normal',
                               trainable=True)

  def call(self, inputs):
      return tf.matmul(inputs, self.w) + self.b


# Lazyí•œ Layerì˜ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë§Œë“­ë‹ˆë‹¤.
linear_layer = Linear(4)

# ì´ë ‡ê²Œ í•˜ë©´, `build(input_shape)`ì´ í˜¸ì¶œë˜ì–´ ê°€ì¤‘ì¹˜ë¥¼ ìƒì„±í•˜ê²Œ ë©ë‹ˆë‹¤.
y = linear_layer(tf.ones((2, 2)))
assert len(linear_layer.weights) == 2
```

## í•™ìŠµ ê°€ëŠ¥í•œ, ê·¸ë¦¬ê³  í•™ìŠµ ë¶ˆê°€ëŠ¥í•œ ê°€ì¤‘ì¹˜

Layerì— ì˜í•´ ìƒì„±ëœ ê°€ì¤‘ì¹˜ëŠ” í•™ìŠµì´ ê°€ëŠ¥í•  ìˆ˜ë„, í•™ìŠµì´ ë¶ˆê°€ëŠ¥í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ì´ ë‘ ê²½ìš°ëŠ” ê°ê° 
`trainable_weights` ë° `non_trainable_weights`ë¡œì¨ ë…¸ì¶œë˜ì–´ ì™¸ë¶€ì—ì„œ ì ‘ê·¼ ê°€ëŠ¥í•©ë‹ˆë‹¤. ë‹¤ìŒì€ í•™ìŠµ ë¶ˆê°€ëŠ¥í•œ ê°€ì¤‘ì¹˜ë¥¼ ê°€ì§€ëŠ” Layerë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤:


```
class ComputeSum(Layer):
  """ì…ë ¥ì˜ í•©ì‚° ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ëŠ” Layer"""

  def __init__(self, input_dim):
      super(ComputeSum, self).__init__()
      # í•™ìŠµ ë¶ˆê°€ëŠ¥í•œ ê°€ì¤‘ì¹˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
      self.total = tf.Variable(initial_value=tf.zeros((input_dim,)),
                               trainable=False)

  def call(self, inputs):
      self.total.assign_add(tf.reduce_sum(inputs, axis=0))
      return self.total  

my_sum = ComputeSum(2)
x = tf.ones((2, 2))

y = my_sum(x)
print(y.numpy())  # [2. 2.]

y = my_sum(x)
print(y.numpy())  # [4. 4.]

assert my_sum.weights == [my_sum.total]
assert my_sum.non_trainable_weights == [my_sum.total]
assert my_sum.trainable_weights == []
```

    [2. 2.]
    [4. 4.]
    

## ì¬ê·€ì ìœ¼ë¡œ Layerë¥¼ ì¡°í•©í•˜ëŠ”ê²ƒ

Layerë“¤ì€ ë” í° ê³„ì‚°ì„ ìœ„í•œ ë¸”ë¡ì„ ìƒì„±í•˜ê¸° ìœ„í•´ ì¬ê·€ì ìœ¼ë¡œ ì¤‘ì²©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê°ê°ì˜ LayerëŠ” ê°ê°ì˜ (í•™ìŠµ ê°€ëŠ¥í•œê²ƒê³¼ í•™ìŠµ ë¶ˆê°€ëŠ¥í•œ)ê°€ì¤‘ì¹˜ë¥¼ ì¶”ì í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.


```
# `build` ë©”ì†Œë“œì™€ í•¨ê»˜ ì•ì„œ ì •ì˜ëœ
# Linear í´ë˜ìŠ¤ë¥¼ ì¬ì‚¬ìš© í•´ë´…ì‹œë‹¤

class MLP(Layer):
    """Linear Layerì˜ ê°„ë‹¨í•œ ì¸µì„ ìŒ“ëŠ” Layer ì…ë‹ˆë‹¤."""

    def __init__(self):
        super(MLP, self).__init__()
        self.linear_1 = Linear(32)
        self.linear_2 = Linear(32)
        self.linear_3 = Linear(10)

    def call(self, inputs):
        x = self.linear_1(inputs)
        x = tf.nn.relu(x)
        x = self.linear_2(x)
        x = tf.nn.relu(x)
        return self.linear_3(x)

mlp = MLP()

# `mlp` ê°ì²´ì— ëŒ€í•œ ì²« ë²ˆì§¸ í˜¸ì¶œì€ ê°€ì¤‘ì¹˜ë¥¼ ìƒì„±í•˜ê²Œ ë©ë‹ˆë‹¤.
y = mlp(tf.ones(shape=(3, 64)))

# ê°€ì¤‘ì¹˜ë“¤ì€ ì¬ê·€ì ìœ¼ë¡œ ì¶”ì ë©ë‹ˆë‹¤.
assert len(mlp.weights) == 6
```

## ë¯¸ë¦¬ ì •ì˜ëœ Layerì˜ ì¢…ë¥˜

KerasëŠ” [ë„“ì€ ë²”ìœ„ì˜ ë¯¸ë¦¬ ì •ì˜ëœ Layerì˜ ì¢…ë¥˜](https://www.tensorflow.org/api_docs/python/tf/keras/layers/)ë¥¼ ì œê³µí•˜ì—¬ í•­ìƒ ì—¬ëŸ¬ë¶„ ìŠ¤ìŠ¤ë¡œê°€ ëª¨ë“ ê²ƒì„ êµ¬í˜„í•˜ì§€ ì•Šì•„ë„ ë˜ë„ë¡ë” í•´ ì¤ë‹ˆë‹¤.

- Convolution layers
- Transposed convolutions
- Separateable convolutions
- Average and max pooling
- Global average and max pooling
- LSTM, GRU (with built-in cuDNN acceleration)
- BatchNormalization
- Dropout
- Attention
- ConvLSTM2D
- etc.




KerasëŠ” ë””í´íŠ¸ë¡œ ì¢‹ì€ ì„¤ì •ê°’ì„ ë…¸ì¶œì‹œí‚¤ëŠ” ì›ì¹™ì„ ë”°ë¦…ë‹ˆë‹¤. ì´ë ‡ê²Œ í•´ì„œ, í•„ìš”í•œ ì¸ìê°’ì„ ë””í´íŠ¸ê°’ìœ¼ë¡œ ë‚´ë²„ë ¤ë‘ì–´ë„ ëŒ€ë¶€ë¶„ì˜ ê²½ìš°ì—ì„œ ì˜ ë™ì‘í•  ìˆ˜ ìˆê²Œë” í•´ ì¤ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ì„œ, `LSTM` LayerëŠ” ë””í´íŠ¸ë¡œ ì§êµ ìˆœí™˜ í–‰ë ¬ ì´ˆê¸°í™”ì(orthogonal recurrent matrix intializer)ë¥¼ ì‚¬ìš©í•˜ê³ , ì´ëŠ” forget ê²Œì´íŠ¸ì˜ í¸í–¥ê°’ì„ 1ë¡œì¨ ì´ˆê¸°í™” í•©ë‹ˆë‹¤.

## `call` ë©”ì†Œë“œì˜ `training` ì¸ì


ëª‡ Layer, íŠ¹íˆ `BatchNormalization`ê³¼ `Dropout` Layer,ëŠ” í•™ìŠµê³¼ ì¶”ë¡ ë‹¨ê³„ì—ì„œ ì„œë¡œë‹¤ë¥¸ ë™ì‘ë°©ì‹ì„ ê°€ì§‘ë‹ˆë‹¤. ì´ëŸ¬í•œ ì¢…ë¥˜ì˜ Layerì— ëŒ€í•´ì„ , `call` ë©”ì†Œìœ¼ì˜ (ë¶€ìš¸ í˜•ì‹ì¸)`training` ì¸ìë¥¼ ë…¸ì¶œì‹œí‚¤ëŠ” ê²ƒì´ í‘œì¤€ì ì¸ ê´€ë¡€ì…ë‹ˆë‹¤.

`call` ë©”ì†Œë“œì˜ ì´ ì¸ìë¥¼ ë…¸ì¶œì‹œí‚´ìœ¼ë¡œì¨, ë¯¸ë¦¬ ì œê³µë˜ëŠ” í•™ìŠµê³¼ í‰ê°€ ë°˜ë³µë¬¸(ì˜ˆë¥¼ ë“¤ì–´ì„œ `fit` ë©”ì†Œë“œ)ì´ í•´ë‹¹ Layerë¥¼ í•™ìŠµê³¼ ì¶”ë¡ ì— ëŒ€í•´ì„œ ì˜³ë°”ë¥´ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤.


```
class Dropout(Layer):
  
  def __init__(self, rate):
    super(Dropout, self).__init__()
    self.rate = rate

  def call(self, inputs, training=None):
    if training:
      return tf.nn.dropout(inputs, rate=self.rate)
    return inputs

class MLPWithDropout(Layer):

  def __init__(self):
      super(MLPWithDropout, self).__init__()
      self.linear_1 = Linear(32)
      self.dropout = Dropout(0.5)
      self.linear_3 = Linear(10)

  def call(self, inputs, training=None):
      x = self.linear_1(inputs)
      x = tf.nn.relu(x)
      x = self.dropout(x, training=training)
      return self.linear_3(x)
    
mlp = MLPWithDropout()
y_train = mlp(tf.ones((2, 2)), training=True)
y_test = mlp(tf.ones((2, 2)), training=False)
```

## ì¢€ ë” í•¨ìˆ˜í˜•ì ìœ¼ë¡œ ëª¨ë¸ì„ ì •ì˜í•˜ê¸° ìœ„í•œ ë°©ë²•

ë”¥ ëŸ¬ë‹ ëª¨ë¸ì„ ë§Œë“¤ê¸° ìœ„í•´ì„œ, í•­ìƒ ê°ì²´ì§€í–¥ì  í”„ë¡œê·¸ë˜ë° ë°©ë²•ì„ ì‚¬ìš©í•  í•„ìš”ëŠ” ì—†ìŠµë‹ˆë‹¤. ì•„ë˜ì˜ ì˜ˆì‹œì²˜ëŸ¼ Layerë“¤ì€ í•¨ìˆ˜í˜•ì ìœ¼ë¡œë„ ì¡°í•©ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤ ("í•¨ìˆ˜í˜• API" ë¼ê³  ë¶€ë¦…ë‹ˆë‹¤):


```
# `Input` ê°ì²´ë¥¼ ì‚¬ìš©í•´ì„œ, ì…ë ¥ì˜ shape(ëª¨ì–‘)ê³¼ dtype(ë°ì´í„°í˜•)ì„ ë¬˜ì‚¬í•©ë‹ˆë‹¤.
# ë”¥ëŸ¬ë‹ì—ì„œ ì´ëŠ” ë°ì´í„°í˜•ì„ ì„ ì–¸í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.
# shape ì¸ìëŠ” ìƒ˜í”Œë‹¹ ìœ¼ë¡œ, ë°°ì¹˜ í¬ê¸°ë¥¼ í¬í•¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. 
# í•¨ìˆ˜í˜• APIëŠ” ìƒ˜í”Œë‹¹ ë³€í˜•ì„ ì •ì˜í•˜ëŠ”ë° ì§‘ì¤‘í•©ë‹ˆë‹¤.
# ìƒì„±í•˜ëŠ” ëª¨ë¸ì€ ìë™ìœ¼ë¡œ ìƒ˜í”Œë‹¹ ë³€í˜•ì— ëŒ€í•œ ë°°ì¹˜ë¥¼ ê³ ë ¤í•©ë‹ˆë‹¤.
# ë”°ë¼ì„œ, ëª¨ë¸ì€ ë°ì´í„°ì˜ ë°°ì¹˜ë§ˆë‹¤ í˜¸ì¶œë©ë‹ˆë‹¤.
inputs = tf.keras.Input(shape=(16,))

# ì´ëŸ¬í•œ "ë°ì´í„°í˜•"ì˜ ê°ì²´ì— ëŒ€í•´ì„œ Layerë¥¼ í˜¸ì¶œí•˜ê³ ,
# í˜¸ì¶œ ê²°ê³¼ë¡œ ê°±ì‹ ëœ (ìƒˆë¡œìš´ shapeê³¼ dtypeì„ ê°€ì§€ëŠ”)"ë°ì´í„°í˜•"ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
x = Linear(32)(inputs) # ì•ì„œ ì •ì˜ëœ Linear Layerë¥¼ ì¬ì‚¬ìš© í•©ë‹ˆë‹¤.
x = Dropout(0.5)(x)    # ì•ì„œ ì •ì˜ëœ Droptout Layerë¥¼ ì¬ì‚¬ìš© í•©ë‹ˆë‹¤.
outputs = Linear(10)(x)

# í•¨ìˆ˜í˜• `ëª¨ë¸(Model)`ì€ ì…ë ¥ê³¼ ì¶œë ¥ì„ ëª…ì‹œí•˜ì—¬ ì •ì˜ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
# ëª¨ë¸ì€ ë‹¤ë¥¸ê²ƒê³¼ ë§ˆì°¬ê°€ì§€ë¡œ ìŠ¤ìŠ¤ë¡œê°€ ë˜ í•˜ë‚˜ì˜ Layerê°€ ë©ë‹ˆë‹¤.
model = tf.keras.Model(inputs, outputs)

# í•¨ìˆ˜í˜• ëª¨ë¸ì€ í˜¸ì¶œë˜ê¸°ì „, ì´ë¯¸ ê°€ì¤‘ì¹˜ë¥¼ ê°€ì§‘ë‹ˆë‹¤.
# ê·¸ ì´ìœ ëŠ” ì…ë ¥ì— ëŒ€í•œ shapeì„ `input`ì—ì„œ ì‚¬ì „ì— ì •ì˜í–ˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.
assert len(model.weights) == 4

# ë˜‘ê°™ì€ ë°ì´í„°ì— ëŒ€í•´ì„œ, ëª¨ë¸ì„ ë‹¤ì‹œ í˜¸ì¶œí•´ ë´…ì‹œë‹¤.
y = model(tf.ones((2, 16)))
assert y.shape == (2, 10)
```

í•¨ìˆ˜í˜• APIëŠ” í•˜ìœ„ í´ë˜ìŠ¤ë¥¼ ë§Œë“œëŠ”ê²ƒ ë³´ë‹¤ ë” ê°„ê²°í•˜ê³ , ì—¬ê¸°ì—” ëª‡ëª‡ ë¶€ê°€ì ì¸ ì´ì (ì¼ë°˜ì ìœ¼ë¡œ í•¨ìˆ˜í˜•, í˜• ì„ ì–¸ì  ì–¸ì–´ê°€ í˜• ì„ ì–¸ì ì´ì§€ ì•Šì€ ê°ì²´ì§€í–¥ ê°œë°œì— ë¹„í•´ ê°€ì§€ëŠ” ì´ì ê³¼ ë™ì¼)ì´ ì¡´ì¬í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ, ì´ëŠ” Layerë“¤ì˜ DAGsë¥¼ ì •ì˜í•˜ëŠ”ë°ì—ë§Œ ì‚¬ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¬ê·€ì ì¸ ë„¤íŠ¸ì›Œí¬ëŠ” `Layer`ì˜ í•˜ìœ„ í´ë˜ìŠ¤ë¥¼ í†µí•´ì„œ ì •ì˜ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.

í•¨ìˆ˜í˜• ëª¨ë¸ê³¼ í•˜ìœ„ í´ë˜ìŠ¤ë¥¼ í†µí•´ ì •ì˜ëœ ëª¨ë¸ì˜ ì£¼ìš” ë‹¤ë¥¸ì ì€ [ì´ê³³](https://medium.com/tensorflow/what-are-symbolic-and-imperative-apis-in-tensorflow-2-0-dfccecb01021)ì— ì„¤ëª…ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

[ì´ê³³](https://www.tensorflow.org/alpha/guide/keras/functional)ì„ ë°©ë¬¸í•´ì„œ, í•¨ìˆ˜í˜• APIì— ëŒ€í•´ ì¢€ ë” ë°°ì›Œë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì—°êµ¬ì˜ ì‘ì—… íë¦„ì—ì„œ, ê°ì²´ì§€í–¥ ëª¨ë¸ê³¼ í•¨ìˆ˜í˜• ëª¨ë¸ì„ ì„ì–´ì“°ëŠ” ìì‹ ì„ ì¢…ì¢… ë°œê²¬í•˜ê²Œ ë ì§€ë„ ëª¨ë¦…ë‹ˆë‹¤.

ë‹¨ì¼ ì…ë ¥ê³¼ ì¶œë ¥ì„ ê°€ì§€ëŠ” Layerì„ ì´ìš©í•´ì„œ, ì—¬ëŸ¬ ì¸µìœ¼ë¡œ êµ¬ì„±ëœ ëª¨ë¸ì— ëŒ€í•˜ì—¬ `Sequential` í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ì´ í´ë˜ìŠ¤ëŠ” Layerì˜ ëª©ë¡ì„ `Model`ë¡œ ë³€í™˜í•´ ì¤ë‹ˆë‹¤:


```
from tensorflow.keras import Sequential

model = Sequential([Linear(32), Dropout(0.5), Linear(10)])

y = model(tf.ones((2, 16)))
assert y.shape == (2, 10)
```

## Loss í´ë˜ìŠ¤

KerasëŠ” ë„“ì€ ë²”ìœ„ì˜ ë¯¸ë¦¬ ì •ì˜ëœ ì†ì‹¤í•¨ìˆ˜ì— ëŒ€í•œ Loss í´ë˜ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì´ëŠ” `BinaryCrossentropy`, `CategoricalCrossentropy`, `KLDivergence`ë“±ê³¼ ê°™ì€ ê²ƒì´ í¬í•¨ë˜ë©° ë‹¤ìŒê³¼ ê°™ì´ ì‘ë™í•©ë‹ˆë‹¤:


```
bce = tf.keras.losses.BinaryCrossentropy()
y_true = [0., 0., 1., 1.]  # ëª©í‘œ (ë ˆì´ë¸”)
y_pred = [1., 1., 1., 0.]  # ì˜ˆì¸¡ ê²°ê³¼
loss = bce(y_true, y_pred)
print('ì†ì‹¤:', loss.numpy())
```

    ì†ì‹¤: 11.522857
    

Loss í´ë˜ìŠ¤ëŠ” ìƒíƒœë¥¼ ê°€ì§€ì§€ ì•ŠìŠµë‹ˆë‹¤. ì¦‰, `__call__`ì˜ ì¶œë ¥ì€ ì…ë ¥ì— ëŒ€í•œ í•¨ìˆ˜ì¼ ë¿ì…ë‹ˆë‹¤.

## Metric í´ë˜ìŠ¤

ë˜í•œ, KerasëŠ” ë„“ì€ ë²”ìœ„ì˜ ë¯¸ë¦¬ ì •ì˜ëœ í‰ê°€ì§€í‘œ í•¨ìˆ˜ì— ëŒ€í•œ Metric í´ë˜ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì´ëŠ” `BinaryAccuracy`, `AUC`, `FalsePositives`ë“±ê³¼ ê°™ì€ê²ƒì„ í¬í•¨í•©ë‹ˆë‹¤.

Lossì™€ëŠ” ë‹¤ë¥´ê²Œ, Metricì€ ìƒíƒœë¥¼ ê°€ì§‘ë‹ˆë‹¤. `update_state` ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•´ì„œ ìƒíƒœë¥¼ ê°±ì‹ í•˜ê³ , `result`ë¥¼ ì‚¬ìš©í•´ì„œ ìŠ¤ì¹¼ë¼í˜•íƒœì˜ ê²°ê³¼ê°’ì„ ìš”ì²­í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:


```
m = tf.keras.metrics.AUC()
m.update_state([0, 1, 1, 1], [0, 1, 0, 0])
print('ì¤‘ê°„ ê²°ê³¼: ', m.result().numpy())

m.update_state([1, 1, 1, 1], [0, 1, 1, 0])
print('ìµœì¢… ê²°ê³¼: ', m.result().numpy())
```

    ì¤‘ê°„ ê²°ê³¼:  0.6666667
    ìµœì¢… ê²°ê³¼:  0.71428573
    

ë‚´ë¶€ ìƒíƒœëŠ” `metric.reset_states`ì— ì˜í•´ ì´ˆê¸°í™”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

`Metric` í´ë˜ìŠ¤ì˜ í•˜ìœ„ í´ë˜ìŠ¤ë¥¼ ë§Œë“¤ì–´ì„œ, ì—¬ëŸ¬ë¶„ë§Œì˜ í‰ê°€ì§€í‘œ í•¨ìˆ˜ë¥¼ ì†ì‰½ê²Œ ë§Œë“¤ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤:

- `__init__`ë‚´ì˜ ìƒíƒœ ë³€ìˆ˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤
- `update_state`ë‚´ì—ì„œ ì¸ìë¡œì¨ ì£¼ì–´ì§„ `y_true`ì™€ `y_pred`ë¥¼ ì´ìš©í•´ì„œ ë³€ìˆ˜ë¥¼ ê°±ì‹ í•©ë‹ˆë‹¤
- `result`ë‚´ì—ì„œ í‰ê°€ì§€í‘œì˜ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤
- `reset_states`ë‚´ì—ì„œ ìƒíƒœë¥¼ ì´ˆê¸°í™” í•©ë‹ˆë‹¤

ë‹¤ìŒì€ ì´ ë°©ë²•ì„ ë³´ì—¬ì£¼ê¸° ìœ„í•œ ëª©ì ìœ¼ë¡œ, `BinaryTruePositive` í‰ê°€ì§€í‘œì— ëŒ€í•œ êµ¬í˜„í•˜ê³  ìˆìŠµë‹ˆë‹¤:


```
class BinaryTruePositives(tf.keras.metrics.Metric):

  def __init__(self, name='binary_true_positives', **kwargs):
    super(BinaryTruePositives, self).__init__(name=name, **kwargs)
    self.true_positives = self.add_weight(name='tp', initializer='zeros')

  def update_state(self, y_true, y_pred, sample_weight=None):
    y_true = tf.cast(y_true, tf.bool)
    y_pred = tf.cast(y_pred, tf.bool)

    values = tf.logical_and(tf.equal(y_true, True), tf.equal(y_pred, True))
    values = tf.cast(values, self.dtype)
    if sample_weight is not None:
      sample_weight = tf.cast(sample_weight, self.dtype)
      sample_weight = tf.broadcast_weights(sample_weight, values)
      values = tf.multiply(values, sample_weight)
    self.true_positives.assign_add(tf.reduce_sum(values))

  def result(self):
    return self.true_positives

  def reset_states(self):
    self.true_positive.assign(0)
```

## Optimizer í´ë˜ìŠ¤ & ë¹ ë¥¸ end-to-end í•™ìŠµ ë°˜ë³µë¬¸

ì•ì„œ ë³´ì—¬ì§„ ì„ í˜•íšŒê·€ ì˜ˆì œì—ì„œ ì‘ì„±í•œ, ê²½ì‚¬í•˜ê°•ì‹œ ë³€ìˆ˜ê°’ì„ ì§ì ‘ ê°±ì‹ í•˜ëŠ” ë°©ë²•ì€ ì¼ë°˜ì ìœ¼ë¡œ í•˜ì§€ ì•Šì•„ë„ ë©ë‹ˆë‹¤. ë³´í†µì€ `SGD`, `RMSprop`, ë˜ëŠ” `Adam`ë“±ê³¼ ê°™ì´ Kerasì—ì„œ ë¯¸ë¦¬ ì œê³µë˜ëŠ” Optimizer ì¤‘ í•˜ë‚˜ë¥¼ ì‚¬ìš©í•˜ë©´ ë©ë‹ˆë‹¤.

ì•„ë˜ëŠ” MNIST ë°ì´í„°ì— ëŒ€í•´ì„œ, Loss, Metric í´ë˜ìŠ¤ì™€ Optimizerê°€ ëª¨ë‘ í•¨ê»˜ ì‚¬ìš©ë˜ëŠ” ì˜ˆë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.


```
from tensorflow.keras import layers

# ë°ì´í„°ì…‹ë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
x_train = x_train[:].reshape(60000, 784).astype('float32') / 255
dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))
dataset = dataset.shuffle(buffer_size=1024).batch(64)

# ê°„ë‹¨í•œ ë¶„ë¥˜ë¥¼ ìœ„í•œ ëª¨ë¸ì˜ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë§Œë“­ë‹ˆë‹¤
model = tf.keras.Sequential([
  layers.Dense(256, activation=tf.nn.relu),
  layers.Dense(256, activation=tf.nn.relu),
  layers.Dense(10)
])

# ì •ìˆ˜í˜• ë ˆì´ë¸”ì„ ì¸ìë¡œ ë°›ì•„ë“¤ì´ëŠ”, ë¡œì§€ìŠ¤í‹± Lossì˜ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë§Œë“­ë‹ˆë‹¤
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)

# ì •í™•ë„ì— ëŒ€í•œ Metricì˜ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë§Œë“­ë‹ˆë‹¤
accuracy = tf.keras.metrics.SparseCategoricalAccuracy()

# Optimizerì˜ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë§Œë“­ë‹ˆë‹¤
optimizer = tf.keras.optimizers.Adam()

# ë°ì´í„°ì…‹ì˜ ë°ì´í„° ë°°ì¹˜ë¥¼ ìˆœíšŒí•©ë‹ˆë‹¤
for step, (x, y) in enumerate(dataset):
  
  # GradientTape ì—´ì–´ì¤ë‹ˆë‹¤
  with tf.GradientTape() as tape:

    # ìˆœë°©í–¥ ì „íŒŒ(forward)ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤
    logits = model(x)

    # í˜„ì¬ ë°°ì¹˜ì— ëŒ€í•œ ì†ì‹¤ê°’ì„ ì¸¡ì •í•©ë‹ˆë‹¤
    loss_value = loss(y, logits)
     
  # ì†ì‹¤ì— ëŒ€í•œ ê°€ì¤‘ì¹˜ì˜ ê²½ì‚¬ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤
  gradients = tape.gradient(loss_value, model.trainable_weights)
  
  # ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ê°±ì‹ í•©ë‹ˆë‹¤
  optimizer.apply_gradients(zip(gradients, model.trainable_weights))

  # í˜„ì¬ê¹Œì§€ ìˆ˜í–‰ëœ ì „ì²´ì— ëŒ€í•œ ëª¨ë¸ì˜ ì •í™•ë„ë¥¼ ê°±ì‹ í•©ë‹ˆë‹¤
  accuracy.update_state(y, logits)
  
  # ë¡œê·¸ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤
  if step % 100 == 0:
    print('ë‹¨ê³„(Step):', step)
    print('ë§ˆì§€ë§‰ ë‹¨ê³„(Step)ì˜ ì†ì‹¤:', float(loss_value))
    print('ì§€ê¸ˆê¹Œì§€ ìˆ˜í–‰ëœ ì „ì²´ì— ëŒ€í•œ ì •í™•ë„:', float(accuracy.result()))
```

    Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz
    11493376/11490434 [==============================] - 0s 0us/step
    ë‹¨ê³„(Step): 0
    ë§ˆì§€ë§‰ ë‹¨ê³„(Step)ì˜ ì†ì‹¤: 2.341179132461548
    ì§€ê¸ˆê¹Œì§€ ìˆ˜í–‰ëœ ì „ì²´ì— ëŒ€í•œ ì •í™•ë„: 0.03125
    ë‹¨ê³„(Step): 100
    ë§ˆì§€ë§‰ ë‹¨ê³„(Step)ì˜ ì†ì‹¤: 0.33624351024627686
    ì§€ê¸ˆê¹Œì§€ ìˆ˜í–‰ëœ ì „ì²´ì— ëŒ€í•œ ì •í™•ë„: 0.843440592288971
    ë‹¨ê³„(Step): 200
    ë§ˆì§€ë§‰ ë‹¨ê³„(Step)ì˜ ì†ì‹¤: 0.18553781509399414
    ì§€ê¸ˆê¹Œì§€ ìˆ˜í–‰ëœ ì „ì²´ì— ëŒ€í•œ ì •í™•ë„: 0.8798196315765381
    ë‹¨ê³„(Step): 300
    ë§ˆì§€ë§‰ ë‹¨ê³„(Step)ì˜ ì†ì‹¤: 0.3094934821128845
    ì§€ê¸ˆê¹Œì§€ ìˆ˜í–‰ëœ ì „ì²´ì— ëŒ€í•œ ì •í™•ë„: 0.8974252343177795
    ë‹¨ê³„(Step): 400
    ë§ˆì§€ë§‰ ë‹¨ê³„(Step)ì˜ ì†ì‹¤: 0.20692837238311768
    ì§€ê¸ˆê¹Œì§€ ìˆ˜í–‰ëœ ì „ì²´ì— ëŒ€í•œ ì •í™•ë„: 0.9101075530052185
    ë‹¨ê³„(Step): 500
    ë§ˆì§€ë§‰ ë‹¨ê³„(Step)ì˜ ì†ì‹¤: 0.23231974244117737
    ì§€ê¸ˆê¹Œì§€ ìˆ˜í–‰ëœ ì „ì²´ì— ëŒ€í•œ ì •í™•ë„: 0.9177582263946533
    ë‹¨ê³„(Step): 600
    ë§ˆì§€ë§‰ ë‹¨ê³„(Step)ì˜ ì†ì‹¤: 0.1896420568227768
    ì§€ê¸ˆê¹Œì§€ ìˆ˜í–‰ëœ ì „ì²´ì— ëŒ€í•œ ì •í™•ë„: 0.9241368770599365
    ë‹¨ê³„(Step): 700
    ë§ˆì§€ë§‰ ë‹¨ê³„(Step)ì˜ ì†ì‹¤: 0.11397643387317657
    ì§€ê¸ˆê¹Œì§€ ìˆ˜í–‰ëœ ì „ì²´ì— ëŒ€í•œ ì •í™•ë„: 0.9284726977348328
    ë‹¨ê³„(Step): 800
    ë§ˆì§€ë§‰ ë‹¨ê³„(Step)ì˜ ì†ì‹¤: 0.22063642740249634
    ì§€ê¸ˆê¹Œì§€ ìˆ˜í–‰ëœ ì „ì²´ì— ëŒ€í•œ ì •í™•ë„: 0.9317259788513184
    ë‹¨ê³„(Step): 900
    ë§ˆì§€ë§‰ ë‹¨ê³„(Step)ì˜ ì†ì‹¤: 0.03219543397426605
    ì§€ê¸ˆê¹Œì§€ ìˆ˜í–‰ëœ ì „ì²´ì— ëŒ€í•œ ì •í™•ë„: 0.9354883432388306
    

`SparseCategoricalAccuracy` Metric ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì¬ì‚¬ìš©í•´ì„œ í…ŒìŠ¤íŠ¸ ë°˜ë³µë¬¸ì„ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:


```
x_test = x_test[:].reshape(10000, 784).astype('float32') / 255
test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))
test_dataset = test_dataset.batch(128)

accuracy.reset_states()  # ì´ ì½”ë“œëŠ” Metricì˜ ë‚´ë¶€ ìƒíƒœë¥¼ ì´ˆê¸°í™” í•©ë‹ˆë‹¤

for step, (x, y) in enumerate(test_dataset):
  logits = model(x)
  accuracy.update_state(y, logits)

print('ìµœì¢… í…ŒìŠ¤íŠ¸ ì •í™•ë„:', float(accuracy.result()))
```

    ìµœì¢… í…ŒìŠ¤íŠ¸ ì •í™•ë„: 0.9593999981880188
    

## `add_loss` ë©”ì†Œë“œ

ë•Œë¡œëŠ” ìˆœë°©í–¥ ì „íŒŒ(forward) ìˆ˜í–‰ ì¤‘ ì†ì‹¤ê°’ì„ ê³„ì‚°í•´ ë³´ê³  ì‹¶ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤ (íŠ¹íˆ, ì •ê·œí™”(regularization) ì†ì‹¤ì— ëŒ€í•´ì„œ). KerasëŠ” ì–´ëŠì‹œì ì—ì„œë“ ì§€ ì†ì‹¤ê°’ì„ ê³„ì‚°í•  ìˆ˜ ìˆê²Œ í•´ ì£¼ê³ , `add_loss` ë©”ì†Œë“œë¥¼ í†µí•´ ì´ ì†ì‹¤ê°’ì„ ì¬ê·€ì ìœ¼ë¡œ ê³„ì† ì¶”ì í•  ìˆ˜ ìˆê²Œ í•´ ì¤ë‹ˆë‹¤.

ë‹¤ìŒì€ ì…ë ¥ì— ëŒ€í•œ L2 ë…¸ë¦„ì— ê¸°ë°˜í•œ í¬ì†Œ ì •ê·œí™”(regularization) ì†ì‹¤ì„ ì¶”ê°€í•˜ëŠ” Layerì˜ ì˜ˆë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤:


```
class ActivityRegularization(Layer):
  """í™œì„± í¬ì†Œ ì •ê·œí™” ì†ì‹¤(activity sparsity regularization loss)ì„ ìƒì„±í•˜ëŠ” Layer ì…ë‹ˆë‹¤"""
  
  def __init__(self, rate=1e-2):
    super(ActivityRegularization, self).__init__()
    self.rate = rate
  
  def call(self, inputs):
    # ì…ë ¥ê°’ì— ê¸°ë°˜í•˜ëŠ”
    # `add_loss`ë¥¼ ì‚¬ìš©í•´ì„œ ì •ê·œí™” ì†ì‹¤ì„ ìƒì„±í•©ë‹ˆë‹¤
    self.add_loss(self.rate * tf.reduce_sum(tf.square(inputs)))
    return inputs
```

`add_loss`ë¥¼ ì´ìš©í•´ì„œ ì¶”ê°€ëœ ì†ì‹¤ê°’ì€ `Layer` ë˜ëŠ” `Model`ì˜ ë¦¬ìŠ¤íŠ¸í˜• ì†ì„±ì¸ `.losses`ë¥¼ í†µí•´ì„œ ì ‘ê·¼ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤:


```
from tensorflow.keras import layers

class SparseMLP(Layer):
  """í¬ì†Œ ì •ê·œí™” ì†ì‹¤ì„ ê°€ì§€ëŠ” ì„ í˜• ê³„ì¸µì„ ìŒ“ì•„ì˜¬ë¦° Layer ì…ë‹ˆë‹¤"""

  def __init__(self, output_dim):
      super(SparseMLP, self).__init__()
      self.dense_1 = layers.Dense(32, activation=tf.nn.relu)
      self.regularization = ActivityRegularization(1e-2)
      self.dense_2 = layers.Dense(output_dim)

  def call(self, inputs):
      x = self.dense_1(inputs)
      x = self.regularization(x)
      return self.dense_2(x)
    

mlp = SparseMLP(1)
y = mlp(tf.ones((10, 10)))

print(mlp.losses)  # float32 ìë£Œí˜•ì˜ ë‹¨ì¼ ìŠ¤ì¹¼ë¼ê°’ì„ ê°€ì§€ëŠ” ë¦¬ìŠ¤íŠ¸ ì…ë‹ˆë‹¤
```

    [<tf.Tensor: id=186153, shape=(), dtype=float32, numpy=0.79275525>]
    

ì´ ì†ì‹¤ê°’ë“¤ì€ ìˆœë°©í–¥ ì „íŒŒ(forward)ì˜ ì‹œì‘ì ì— ìˆëŠ” ìµœìƒìœ„ Layerë¡œë¶€í„° ì´ˆê¸°í™”ë˜ë©° ì¶•ì ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë”°ë¼ì„œ `layer.losses`ëŠ” í•­ìƒ ë§ˆì§€ë§‰ ìˆœë°©í–¥ ì „íŒŒë™ì•ˆ ìƒì„±ëœ ì†ì‹¤ê°’ë§Œì„ ê°€ì§€ê²Œ ë©ë‹ˆë‹¤. í•™ìŠµ ë°˜ë³µë¬¸ì„ ì‘ì„±í•  ë•Œ, ì¼ë°˜ì ìœ¼ë¡œ ê²½ì‚¬ë„ ê³„ì‚° ì´ì „ì— ì´ ì†ì‹¤ê°’ë“¤ì— ëŒ€í•œ í•©ì‚°ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.


```
# *ë§ˆì§€ë§‰* ìˆœë°©í–¥ ì „íŒŒì— í•´ë‹¹í•˜ëŠ” ì†ì‹¤ê°’ë“¤ ì…ë‹ˆë‹¤
mlp = SparseMLP(1)
mlp(tf.ones((10, 10)))
assert len(mlp.losses) == 1
mlp(tf.ones((10, 10)))
assert len(mlp.losses) == 1  # ì¶•ì ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤

# ì´ ì†ì‹¤ê°’ë“¤ì„ í•™ìŠµ ë°˜ë³µë¬¸ì—ì„œ ì‚¬ìš©í•˜ëŠ”ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤

# ë°ì´í„°ì…‹ì„ ì¤€ë¹„í•©ë‹ˆë‹¤
(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()
dataset = tf.data.Dataset.from_tensor_slices(
    (x_train.reshape(60000, 784).astype('float32') / 255, y_train))
dataset = dataset.shuffle(buffer_size=1024).batch(64)

# ìƒˆë¡œìš´ MLPë¥¼ ë§Œë“­ë‹ˆë‹¤
mlp = SparseMLP(10)

# Lossì™€ Optimizerë¥¼ ë§Œë“­ë‹ˆë‹¤
loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)

for step, (x, y) in enumerate(dataset):
  with tf.GradientTape() as tape:

    # ìˆœë°©í–¥ ì „íŒŒë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤
    logits = mlp(x)

    # í˜„ì¬ ë°°ì¹˜ì— ëŒ€í•œ ì™¸ë¶€ì˜ ì†ì‹¤ê°’ì„ ê³„ì‚°í•©ë‹ˆë‹¤
    loss = loss_fn(y, logits)
    
    # ìˆœë°©í–¥ ì „íŒŒì‹œ ìƒì„±ëœ ì†ì‹¤ê°’ì„ ë”í•´ì¤ë‹ˆë‹¤ 
    loss += sum(mlp.losses)
     
    # í•´ë‹¹ ì†ì‹¤ì— ëŒ€í•œ ê°€ì¤‘ì¹˜ì˜ ê²½ì‚¬ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤
    gradients = tape.gradient(loss, mlp.trainable_weights)
  
  # ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ê°±ì‹ í•©ë‹ˆë‹¤
  optimizer.apply_gradients(zip(gradients, mlp.trainable_weights))
  
  # ë¡œê·¸ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤
  if step % 100 == 0:
    print(step, float(loss))
```

    0 4.2266459465026855
    100 2.2934560775756836
    200 2.2962656021118164
    300 2.1863455772399902
    400 2.158012866973877
    500 2.1282315254211426
    600 2.0247669219970703
    700 2.041032314300537
    800 1.9458585977554321
    900 1.7418930530548096
    

## ìì„¸í•œ end-to-end ì˜ˆì œ: Variational AutoEncoder (VAE)

ê¸°ì´ˆì ì¸ ë‚´ìš©ì˜ ê³µë¶€ë¥¼ ì ì‹œ ë¯¸ë¤„ë‘ê³ , ì•½ê°„ ë” ì–´ë ¤ìš´ ì˜ˆì œë¥¼ ì‚´í´ë³´ê³  ì‹¶ë‹¤ë©´, [ì—¬ê¸°ì— ì†Œê°œëœ VAE](https://www.tensorflow.org/guide/keras/custom_layers_and_models#putting_it_all_together_an_end-to-end_example)ì— ëŒ€í•œ êµ¬í˜„ì˜ ì˜ˆì œë¥¼ í™•ì¸í•´ ë³´ì‹œê¸° ë°”ëë‹ˆë‹¤. ì´ëŠ” ì—¬ëŸ¬ë¶„ì´ ì§€ê¸ˆê¹Œì§€ ë°°ì›Œì™”ë˜ ëª¨ë“ ê²ƒì˜ ë‚´ìš©ì„ ë‹´ê³  ìˆìŠµë‹ˆë‹¤:

- `Layer`ì˜ í•˜ìœ„ í´ë˜ìŠ¤ë¥¼ ë§Œë“œëŠ”ê²ƒ
- ì¬ê·€ì ìœ¼ë¡œ Layerë¥¼ êµ¬ì„±í•˜ëŠ”ê²ƒ
- Loss ë° Metric í´ë˜ìŠ¤ì— ëŒ€í•œê²ƒ
- `add_loss`
- `GradientTape`

## ë¯¸ë¦¬ ì •ì˜ëœ í•™ìŠµ ë°˜ë³µë¬¸ì„ ì‚¬ìš©í•˜ëŠ”ê²ƒ

ê°„ë‹¨í•œ ì¼€ì´ìŠ¤ì— ëŒ€í•´ì„œ ì¡°ì°¨ ì—¬ëŸ¬ë¶„ì´ ìŠ¤ìŠ¤ë¡œ ì €ìˆ˜ì¤€ì˜ í•™ìŠµ ë°˜ë³µë¬¸ì„ ë§¤ë²ˆ ì‘ì„±í•´ì•¼ í•œë‹¤ë©´, ì´ëŠ” ì–´ë¦¬ì„ì€ ì¼ì¼ì§€ë„ ëª¨ë¦…ë‹ˆë‹¤. KerasëŠ” ë¯¸ë¦¬ ì •ì˜ëœ í•™ìŠµ ë°˜ë³µë¬¸ì„ `Model` í´ë˜ìŠ¤ì—ì„œ ì œê³µí•©ë‹ˆë‹¤. ì‚¬ìš©í•˜ê³ ì í•œë‹¤ë©´, `Model`ì˜ í•˜ìœ„ í´ë˜ìŠ¤ë¥¼ ë§Œë“¤ê±°ë‚˜ `Functional(í•¨ìˆ˜í˜•)` ë˜ëŠ” `Sequential(ìˆœì°¨í˜•)` ëª¨ë¸ì„ ìƒì„±í•˜ë©´ ë©ë‹ˆë‹¤.

ì´ë¥¼ ë³´ì—¬ì£¼ê¸° ìœ„í•´ì„œ, ì•ì„œ ë§Œë“¤ì–´ë‘” MNISTì˜ ì˜ˆë¥¼ ì¬ì‚¬ìš© í•´ ë³´ê² ìŠµë‹ˆë‹¤:


```
# ë°ì´í„°ì…‹ì„ ì¤€ë¹„í•©ë‹ˆë‹¤
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
x_train = x_train.reshape(60000, 784).astype('float32') / 255
dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))
dataset = dataset.shuffle(buffer_size=1024).batch(64)

# ê°„ë‹¨í•œ ë¶„ë¥˜ëª¨ë¸ì˜ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë§Œë“­ë‹ˆë‹¤
model = tf.keras.Sequential([
  layers.Dense(256, activation=tf.nn.relu),
  layers.Dense(256, activation=tf.nn.relu),
  layers.Dense(10)
])

# ì •ìˆ˜í˜• ë ˆì´ë¸”ì„ ì¸ìë¡œ ë°›ì•„ë“¤ì´ëŠ”, ë¡œì§€ìŠ¤í‹± Lossì˜ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë§Œë“­ë‹ˆë‹¤
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)

# ì •í™•ë„ì— ëŒ€í•œ Metricì˜ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë§Œë“­ë‹ˆë‹¤
accuracy = tf.keras.metrics.SparseCategoricalAccuracy()

# Optimizerì˜ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë§Œë“­ë‹ˆë‹¤
optimizer = tf.keras.optimizers.Adam()
```

ê°€ì¥ ì²« ë²ˆì§¸ë¡œ, `compile` ë©”ì†Œë“œë¥¼ í˜¸ì¶œí•˜ì—¬ Optimizer, Loss, ëª¨ë‹ˆí„°ë§í•˜ê¸° ìœ„í•œ Metricì„ ì„¤ì •í•©ë‹ˆë‹¤.


```
model.compile(optimizer=optimizer, loss=loss, metrics=[accuracy])
```

ê·¸ë¦¬ê³  ë‚˜ì„  `fit` ë©”ì†Œë“œë¥¼ í˜¸ì¶œí•˜ê³ , ì´ ë•Œ ë°ì´í„°ë¥¼ ì „ë‹¬í•´ ì¤ë‹ˆë‹¤:


```
model.fit(dataset, epochs=3)
```

    Epoch 1/3
    938/938 [==============================] - 7s 7ms/step - loss: 0.2177 - sparse_categorical_accuracy: 0.9361
    Epoch 2/3
    938/938 [==============================] - 4s 4ms/step - loss: 0.0842 - sparse_categorical_accuracy: 0.9747
    Epoch 3/3
    938/938 [==============================] - 4s 4ms/step - loss: 0.0564 - sparse_categorical_accuracy: 0.9821
    




    <tensorflow.python.keras.callbacks.History at 0x7ff0b1254fd0>



ì´ê²Œ ëì…ë‹ˆë‹¤! ì´ì œëŠ” í…ŒìŠ¤íŠ¸ë¥¼ í•´ ë´…ì‹œë‹¤:


```
x_test = x_test[:].reshape(10000, 784).astype('float32') / 255
test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))
test_dataset = test_dataset.batch(128)

loss, acc = model.evaluate(test_dataset)
print('ì†ì‹¤:', loss, 'ì •í™•ë„:', acc)
```

    79/79 [==============================] - 0s 4ms/step - loss: 0.0949 - sparse_categorical_accuracy: 0.9712
    ì†ì‹¤: 0.09488680568940737 ì •í™•ë„: 0.9712
    

`fit`ì´ ìˆ˜í–‰ë˜ëŠ” ë™ì•ˆ ê²€ì¦ìš© ë°ì´í„°ì…‹ì— ëŒ€í•œ Lossì™€ Metricì„ ëª¨ë‹ˆí„°ë§ í•˜ëŠ”ê²ƒ ë˜í•œ ê°€ëŠ¥í•©ë‹ˆë‹¤.

ë˜í•œ, Numpyí˜•ì˜ ë°°ì—´ì— ëŒ€í•´ì„œë„ ì§ì ‘ì ìœ¼ë¡œ `fit`ì„ í˜¸ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ë°ì´í„°ì…‹ì— ëŒ€í•œ ë³€í™˜ì´ í•„ìš” ì—†ìŠµë‹ˆë‹¤:


```
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
x_train = x_train.reshape(60000, 784).astype('float32') / 255

num_val_samples = 10000
x_val = x_train[-num_val_samples:]
y_val = y_train[-num_val_samples:]
x_train = x_train[:-num_val_samples]
y_train = y_train[:-num_val_samples]

# ê°„ë‹¨í•œ ë¶„ë¥˜ëª¨ë¸ì˜ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë§Œë“­ë‹ˆë‹¤
model = tf.keras.Sequential([
  layers.Dense(256, activation=tf.nn.relu),
  layers.Dense(256, activation=tf.nn.relu),
  layers.Dense(10)
])

# ì •ìˆ˜í˜• ë ˆì´ë¸”ì„ ì¸ìë¡œ ë°›ì•„ë“¤ì´ëŠ”, ë¡œì§€ìŠ¤í‹± Lossì˜ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë§Œë“­ë‹ˆë‹¤
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)

# ì •í™•ë„ì— ëŒ€í•œ Metricì˜ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë§Œë“­ë‹ˆë‹¤
accuracy = tf.keras.metrics.SparseCategoricalAccuracy()

# Optimizerì˜ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë§Œë“­ë‹ˆë‹¤
optimizer = tf.keras.optimizers.Adam()

model.compile(optimizer=optimizer,
              loss=loss,
              metrics=[accuracy])
model.fit(x_train, y_train,
          validation_data=(x_val, y_val),
          epochs=3,
          batch_size=64)
```

    Train on 50000 samples, validate on 10000 samples
    Epoch 1/3
    50000/50000 [==============================] - 4s 83us/sample - loss: 0.2399 - sparse_categorical_accuracy: 0.9292 - val_loss: 0.1223 - val_sparse_categorical_accuracy: 0.9632
    Epoch 2/3
    50000/50000 [==============================] - 4s 75us/sample - loss: 0.0951 - sparse_categorical_accuracy: 0.9704 - val_loss: 0.0872 - val_sparse_categorical_accuracy: 0.9747
    Epoch 3/3
    50000/50000 [==============================] - 4s 73us/sample - loss: 0.0616 - sparse_categorical_accuracy: 0.9805 - val_loss: 0.0805 - val_sparse_categorical_accuracy: 0.9755
    




    <tensorflow.python.keras.callbacks.History at 0x7ff0afa09278>



## Callbacks

`fit`ì´ ê°€ì§€ëŠ” ê°„ë‹¨í•˜ì§€ë§Œ í›Œë¥­í•œ ê¸°ëŠ¥ ì¤‘ í•˜ë‚˜ë¡œ, [callbacks](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/)ì„ ì‚¬ìš©í•´ì„œ í•™ìŠµê³¼ í‰ê°€ ë„ì¤‘ ì¼ì–´ë‚˜ëŠ” ì¼ì— ëŒ€í•œ ì‚¬ìš©ì ì •ì˜í™”ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.

Callbackì€ ê°ì²´ì˜ í•œ ì¢…ë¥˜ë¡œ, í•™ìŠµ ì¤‘ê°„ ì¤‘ê°„ì— í˜¸ì¶œ(ì˜ˆë¥¼ë“¤ì–´, ë§¤ ë°°ì¹˜ë§ˆë‹¤ ë˜ëŠ” ë§¤ epochë§ˆë‹¤) ë˜ë©° ì–´ë–¤ ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

ë¯¸ë¦¬ ì •ì˜ëœ ì—¬ëŸ¬ê°€ì§€ Callbackì´ ì¡´ì¬í•©ë‹ˆë‹¤. `ModelCheckpoint`ëŠ” í•™ìŠµë„ì¤‘ ë§¤ epochë§ˆë‹¤ ëª¨ë¸ì„ ì €ì¥í•˜ê³ , `EarlyStopping`ì€ ê²€ì¦ìš© í‰ê°€ì§€í‘œ(metrics)ê°€ í–¥ìƒë˜ì§€ ì•Šì„ ë•Œ í•™ìŠµì„ ì¤‘ë‹¨ì‹œí‚µë‹ˆë‹¤.

ë¬¼ë¡ , ì†ì‰½ê²Œ [ì—¬ëŸ¬ë¶„ë§Œì˜ callbackì„ ì‘ì„±í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤](https://www.tensorflow.org/guide/keras/custom_callback).



```
# ê°„ë‹¨í•œ ë¶„ë¥˜ëª¨ë¸ì˜ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë§Œë“­ë‹ˆë‹¤
model = tf.keras.Sequential([
  layers.Dense(256, activation=tf.nn.relu),
  layers.Dense(256, activation=tf.nn.relu),
  layers.Dense(10)
])

# ì •ìˆ˜í˜• ë ˆì´ë¸”ì„ ì¸ìë¡œ ë°›ì•„ë“¤ì´ëŠ”, ë¡œì§€ìŠ¤í‹± Lossì˜ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë§Œë“­ë‹ˆë‹¤
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)

# ì •í™•ë„ì— ëŒ€í•œ Metricì˜ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë§Œë“­ë‹ˆë‹¤
accuracy = tf.keras.metrics.SparseCategoricalAccuracy()

# Optimizerì˜ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë§Œë“­ë‹ˆë‹¤
optimizer = tf.keras.optimizers.Adam()

model.compile(optimizer=optimizer,
              loss=loss,
              metrics=[accuracy])

# ëª‡ê°€ì§€ Callbackì˜ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë§Œë“­ë‹ˆë‹¤
callbacks = [tf.keras.callbacks.EarlyStopping(),
             tf.keras.callbacks.ModelCheckpoint(filepath='my_model.keras',
                                                save_best_only=True)]

model.fit(x_train, y_train,
          validation_data=(x_val, y_val),
          epochs=30,
          batch_size=64,
          callbacks=callbacks)
```

    Train on 50000 samples, validate on 10000 samples
    Epoch 1/30
    50000/50000 [==============================] - 4s 83us/sample - loss: 0.2432 - sparse_categorical_accuracy: 0.9281 - val_loss: 0.1253 - val_sparse_categorical_accuracy: 0.9635
    Epoch 2/30
    50000/50000 [==============================] - 4s 72us/sample - loss: 0.0939 - sparse_categorical_accuracy: 0.9717 - val_loss: 0.0964 - val_sparse_categorical_accuracy: 0.9695
    Epoch 3/30
    50000/50000 [==============================] - 4s 72us/sample - loss: 0.0635 - sparse_categorical_accuracy: 0.9800 - val_loss: 0.0788 - val_sparse_categorical_accuracy: 0.9774
    Epoch 4/30
    50000/50000 [==============================] - 4s 70us/sample - loss: 0.0456 - sparse_categorical_accuracy: 0.9858 - val_loss: 0.0845 - val_sparse_categorical_accuracy: 0.9757
    




    <tensorflow.python.keras.callbacks.History at 0x7ff0af546748>



# ì‘ë³„ ì¸ì‚¬

ì €ëŠ” ì´ ê°€ì´ë“œê°€ ì—¬ëŸ¬ë¶„ì—ê²Œ TensorFlow2.0ê³¼ Kerasë¡œ ë¬´ì—‡ì„ í•  ìˆ˜ ìˆëŠ”ì§€ ì•Œë ¤ì£¼ëŠ” ì¢‹ì€ ì˜¤ë²„ë·°ê°€ ë˜ê¸¸ í¬ë§í•©ë‹ˆë‹¤!

TensorFlowì™€ KerasëŠ” ë‹¨ì¼ ì‘ì—… íë¦„ë§Œì„ ëŒ€ë³€í•˜ëŠ”ê²Œ ì•„ë‹ˆë¼ëŠ” ê²ƒì„ ê¸°ì–µí•˜ì„¸ìš”. ì‚¬ìš©ì„±ê³¼ ìœ ì—°ì„±ì´ë¼ëŠ” íŠ¸ë ˆì´ë“œì˜¤í”„ë¥¼ ê°€ì§€ëŠ” ì—¬ëŸ¬ ë²”ìœ„ì˜ ì‘ì—…íë¦„ì„ ì§€ì›í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ì„œ, `fit` ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•˜ëŠ”ê²ƒì´ ì‚¬ìš©ìì •ì˜ í•™ìŠµ ë°˜ë³µë¬¸ì„ ì‘ì„±í•˜ëŠ”ê²ƒë³´ë‹¤ í›¨ì”¬ ì‰½ì§€ë§Œ, `fit`ì€ ì—°êµ¬ì—ì„œ í•„ìš”í•œ ë¯¸ì„¸í•œ ì¡°ì ˆì´ ê°€ëŠ¥í•œ ìˆ˜ì¤€ê¹Œì§€ë¥¼ ì œê³µí•˜ì§„ ëª»í•©ë‹ˆë‹¤.

ë”°ë¼ì„œ, ì—¬ëŸ¬ë¶„ì˜ ì¼ì— ë§ëŠ” ì•Œë§ì€ íˆ´ì„ ì‚¬ìš©í•˜ì„¸ìš”!

Kerasì˜ ì¤‘ì‹¬ì´ ë˜ëŠ” ì›ì¹™ì€ "ë³µì¡ë„ì˜ ì ì§„ì ì¸ ê³µê°œ" ì…ë‹ˆë‹¤. ë§¤ìš° ì‰½ê²Œ ì‹œì‘í•  ìˆ˜ ìˆê³ , ì ì  ë” ë§ì€ ë¶€ë¶„ì„ ë°‘ë°”ë‹¥ì—ì„œ ë¶€í„° êµ¬í˜„í•´ì•¼ í•˜ëŠ” ì‘ì—…íë¦„ì— ëŒ€í•´ì„œ ì ì§„ì ìœ¼ë¡œ ì¢€ ë” ê¹Šì´ ë“¤ì—¬ë‹¤ë³´ê³ , ê·¸ë ‡ê²Œí•¨ìœ¼ë¡œì¨ ì™„ì „í•œ ì œì–´ë¥¼ í•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤.


ì´ ì‚¬ì‹¤ì€ ëª¨ë¸ì˜ ì •ì˜ì™€ ëª¨ë¸ì˜ í•™ìŠµ ëª¨ë‘ì— ì ìš©ë˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

![ëª¨ë¸ì˜ ì •ì˜: ì‘ì—… íë¦„ì˜ ë²”ìœ„](https://drive.google.com/uc?export=view&id=1bTHrw-OXaKqnJI-DVGG04suWHVOl8UVj)

![ëª¨ë¸ì˜ í•™ìŠµ: ì‘ì—… íë¦„ì˜ ë²”ìœ„](https://drive.google.com/uc?export=view&id=1a6oMJ9IKyMg19JX7NkihyOfzdiJo5Mpq)


## ì´ ë‹¤ìŒìœ¼ë¡œ ë³´ë©´ ì¢‹ì„ë§Œí•œ ê²ƒë“¤

ì´ ê°€ì´ë“œ ë‹¤ìŒìœ¼ë¡œ, ì—¬ëŸ¬ë¶„ì´ ê´€ì‹¬ì„ ê°€ì§ˆë§Œí•œ ì£¼ì œê°€ ë” ìˆìŠµë‹ˆë‹¤:

- [ì €ì¥ê³¼ ì§ë ¬í™”](https://www.tensorflow.org/guide/keras/save_and_serialize)
- [ë‹¤ì¤‘ GPUsì—ì„œì˜ ë¶„ì‚° í•™ìŠµ](https://www.tensorflow.org/guide/distributed_training)
- [ì„ë² ë””ë“œ ì‹œìŠ¤í…œì´ë‚˜ ì•ˆë“œë¡œì´ë“œ ê°œë°œì— í™œìš©í•˜ê¸° ìœ„í•´ ëª¨ë¸ì„ TFLiteë¡œ ë‚´ë³´ë‚´ê¸°](https://www.tensorflow.org/lite/convert/python_api#converting_a_keras_model_)
- [ë¸Œë¼ìš°ì ¸ì—ì„œì˜ ê°œë°œì— í™œìš©í•˜ê¸° ìœ„í•´ ëª¨ë¸ì„ TensorFlow.jsë¡œ ë‚´ë³´ë‚´ê¸°](https://www.tensorflow.org/js/tutorials/conversion/import_keras)
