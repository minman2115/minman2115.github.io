---
layout: post
title: "Airflow 아키텍처 및 구성요소"
tags: [Data Engineering]
comments: true
---

.

Data_Engineering_TIL(20210922)

[참고자료]

아래의 블로그글들을 읽고 공부한 내용입니다.

- "Apache Airflow" 최정민님 깃허브 자료

URL : https://github.com/cjungm/with-aws/tree/main/airflow

- "Kubernetes를 이용한 효율적인 데이터 엔지니어링(Airflow on Kubernetes VS Airflow Kubernetes Executor) – 1" 이웅규님 Line 블로그글 

URL : https://engineering.linecorp.com/ko/blog/data-engineering-with-airflow-k8s-1/

- "How Apache Airflow Distributes Jobs on Celery workers" 블로그글

URL : https://medium.com/sicara/using-airflow-with-celery-workers-54cb5212d405


[학습내용]

- Airflow 란

Airbnb에서 개발한 워크플로우 스케줄링, 모니터링 플랫폼

개별 작업들을 절차에 따라 관리 진행할 수 있는 소프트웨어임

- 아키텍처 예시로 이해하는 Airflow 구성요소

멀티노드, Celery executor를 가정하는 아키텍처임

![1](https://user-images.githubusercontent.com/41605276/134341991-a8a85204-4074-4f2b-94d1-cc88b63bc96e.png)

1) Scheduler : 예약된 워크플로를 트리거하고, Executor에다가 Task가 실행되도록 명령하는 역할

2) Executor : Task instance를 실행하는 주체 

3) Message broker : Executor로부터 Task 처리작업 명령을 받아서 worker들에게 적절하게 분배하는 역할

4) DAG Directory(Dag Bag) : Scheduler, Executor, Worker 가 읽는 DAG python file들이 저장되어 있는 폴더

5) Metadata DB : Airflow의 상태 및 계정 정보 등의 Metadata를 저장, Scheduler, Executor 및 Webserver에서 사용

6) Webserver : 사용자 인터페이스(UI) 제공, DAG의 검사, 트리거 및 Task의 동작 디버깅

7) 아키텍처 설명

Webserver에서 Workflow 대한 실행 시 Workflow의 실행 정보가 Metadata Database에 저장

Scheduler는 Metadata DB로 부터 실행 내용확인 및 Workflow를 Executor에 제출

Executor는 DAG Directory에서 DAG 정보 획득 및 Worker에 작업 할당

8) 추가 참고자료 : Apache Airflow의 기본 동작 원리

추가 참고자료 출처 : "Kubernetes를 이용한 효율적인 데이터 엔지니어링(Airflow on Kubernetes VS Airflow Kubernetes Executor) – 1" 블로그글 

URL : https://engineering.linecorp.com/ko/blog/data-engineering-with-airflow-k8s-1/

![2](https://user-images.githubusercontent.com/41605276/134343602-274e26bd-385d-4448-9046-4742cb9efb30.png)