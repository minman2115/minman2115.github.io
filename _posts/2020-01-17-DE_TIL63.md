---
layout: post
title: "YARN 아키텍처 기초개념"
tags: [Data Engineering]
comments: true
---

.

Data_Engineering_TIL_(20200115)

### [학습한 Contents]

1) 주제 : Things you need to know about Hadoop and YARN being a Spark developer / 블로그글

2) URL : https://luminousmen.com/post/hadoop-yarn-spark

### [필기노트]

#### 1. YARN을 OS에 비유한다면

하둡을 이해하기 위해 OS에 비유해보자. 전통적인 OS는 high-level 관점에서보면 파일시스템과 프로세싱 컴포넌트로 대표되는 여러 구성으로 이루어진다. 싱글머신 상에서 파일시스템은 FAT32, NFS 등 다른 형태일 수 있으나 데이터를 관리하고 스토어링 한다는 점에서는 어떠한 파일시스템이던지 비슷할 것이다.

![1](https://user-images.githubusercontent.com/41605276/72579811-85368900-391d-11ea-8952-fba77ac3caaa.png)

또한, OS는 커널, 스케쥴러,그리고 프로그래밍을 구동할 수 있도록 해주는 쓰레드와 프로세스로 구성되는 프로세싱 컴포넌트가 있다.

![2](https://user-images.githubusercontent.com/41605276/72579817-8b2c6a00-391d-11ea-9c15-ddc81cdc111d.png)

그래서 하둡 클러스터 관점에서 스토지리와 프로세싱 영역으로 나누어 본다면 OS와 비슷하다고 할 수 있다. 하둡에서 스토리지 영역에서는 HDFS가 있고, YARN(Yet Another Resource Negotiator)가 execution, 스케쥴링, 특정 테스크를 실행하는 프로세싱 컴포넌트 룰을 담당하고 있다.

#### 2. YARN 아키텍처

![3](https://user-images.githubusercontent.com/41605276/72579822-9089b480-391d-11ea-8a6e-3604f781fd69.png)

그래서 YARN은 어떻게 작동되는 원리인가?

YARN은 ResourceManager라는 일종의 메인 프로세스를 갖고 있다. ResourceManager는 클러스터내에서 돌아가는 모든 어플리케이션에 대해 리소스 분배를 중재하는 궁극적인 권한이 있다. 그리고 클러스터 노드들에서 동작하는 NodeManager라는 구성요소도 있다.

![4](https://user-images.githubusercontent.com/41605276/72579830-95e6ff00-391d-11ea-8f8d-90b7da65fa5e.png)

모든 클러스터 노드들은 특정 숫자의 container가 있다. 컨테이너는 computaional unit들로 유저가 제출한 application의 테스크를 실행하기 위해 노드 리소스를 wrapping 해준 것이라고 할 수 있다. YARN에서 메이 프로세싱 유닛이라고 할 수 있다. 컨테이너는 라라미터를 통해 램이나 cpu등과 같은 사양을 설정할 수 있다.

![5](https://user-images.githubusercontent.com/41605276/72579839-9bdce000-391d-11ea-8ad0-6a863c90eb71.png)

각각의 노드에 있는 컨테이너들은 NodeManager 데몬이 컨트롤한다. 새로운 어플리케이션이 클러스터에 launching 되면 ResourceManager는 하나의 컨테이너를 ApplicationMaster로 할당한다.

각각의 어플리케이션별 ApplicationMaster는 framework-specific한 entity로 ResourceManager로부터 할당받은 리소스를 테스크를 수행 시 어떻게 활용할건지 결정한다. 또한 ApplicationMaster는 컴포넌트 테스크들을 NodeManager와 함께 실행하고 모니터링한다. 

ResourceManager는 pluggable한 스케쥴 컴포넌트로 capacity나 queue등 유사한 제약사항이 있는 다양한 어플리케이션들에 대해 자원을 할당하는 것을 책임진다.

ApplicationMaster가 구동되면 분산되어진 application에 대해 전체 라이프 싸이클에 대한 책임이 부여된다. 무엇보다도 ResourceManger로 어플리케이션 테스크들을 구동하기 위한 컨테이너를 요청하기 위한 리소스 requests가 주요 기능이라고 할 수 있다. 리소스 request는 아래와 같은 요구사항을 충족시키는 다수의 컨테이너들을 요청하는 단순한 요청이라고 할 수 있다.

- A number of resources, today expressed as megabytes of memory and CPU shares


- A preferred location, specified by hostname, rackname, or * to indicate no preference


- A priority within this application, and not across multiple applications

민약에 ApplicationMaster가 충돌이 나거나 사용할 수 없는 상태가 되면 ResourceManager가 다른 container를 만들고 ApplicationMaster를 그 위에서 재시작할 수 있다.

ResourceManager는 현재 구동중인 application 정보와 HDFS안에서 완료된 테스크 정보를 저장한다. 만약에 ResourceManager가 재시작되면 그 정보들은 다시 만들어지고, incomplete한 테스크들에 대해서만 다시 re-run된다.

ResourceManager와 NodeManager 그리고 컨테이너는 어플리케이션 타입이나 테스크에 크게 신경쓰지 않는다. 핵심은 모든 Application framework-specific code는 단순히 그것들의 ApplicationMaster로 가는 형태로 YARN기반의 어떠한 분산 컴퓨팅 프레임워크에서도 동작할 수 있다는 것이다. 이런 제네럴한 접근방법 덕분에 Hadoop YARN 클러스터에서 다양한 워크로드를 수행할 수 있게 되었다.

#### 3. Submit application

1) A client program submits the application, including the necessary specifications to launch the application-specific ApplicationMaster itself.


2) The ResourceManager gets responsibility for allocating the required container in which the ApplicationMaster will starts. Then ResourceManager launches the ApplicationMaster.


3) The ApplicationMaster registers itself on ResourceManager. Registration allows the client program to ask the ResourceManager for specific information which allows it to directly communicate with its own ApplicationMaster.


4) During normal operation, the ApplicationMaster requests the appropriate resource containers.


5) Upon successful receipt of the containers, the ApplicationMaster launches the container by providing the NodeManager with a container configuration.


6) Inside the container, the user application code starts. Then the user application provides information (stage of execution, status) to ApplicationMaster.


7) During the user application execution, the client communicates with the ApplicationMaster to obtain the status of the application.


8) When the application has completed execution and all the necessary work has been finished, the ApplicationMaster deregisters from ResourceManager and shuts down, freeing its container for other purposes.


#### 4. Interesting facts and features
YARN offers several other great features. Describing all of them is outside the scope of the post, but I’ve included some noteworthy features:

- Uberization is the possibility to run all tasks of a MapReduce job in the ApplicationMaster’s JVM if the job is small enough. This way, you avoid the overhead of requesting containers from the ResourceManager and asking the NodeManagers to start (supposedly small) tasks.


- Simplified user-log management and access. Logs generated by applications are not left on individual slave nodes (as with MRv1) but are moved to central storage, such as HDFS. Later, they can be used for debugging purposes or for historical analyses to discover performance issues.
