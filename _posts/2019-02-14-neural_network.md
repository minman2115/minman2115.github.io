---
layout: post
title: "신경망 구조 기초개념"
tags: [딥러닝]
comments: true
---

.

#### # 학습 시 참고한 URL : https://datascienceschool.net


### # 신경망 구조

- 신경망은 크게봐서 basis function의 형태를 모수값으로 변화시킬 수 있는 함수모형이다. 그 이유는 비선형적인 데이터들을 처리하기 위함이다.


- 기존에 커널이나 basis function을 사용할때는 사람이 기저함수을 골라놓고 이것을 고정해서 쓰는데 신경망은 이 기저함수를 기계가 자동으로 바꿔주게 된다. 따라서 신경망을 adaptive basis function model이라고 부르기도 한다.


- 형태로는 퍼셉트론을 여러층으로 쌓아놓기 때문에 multi-layer perceptron이라고도 부른다.


- 우리가 알고 있는 퍼셉트론은 classification을 0 아니면 1 두개의 바이너리로 할 수 있는 판별함수 모형이다.


예를들어 x라는 이미지를 입력하게 되면 x라는 이미지와 w라는 가중치와 inner product해서 아래와 같이 들어오게 된다. inner product시 이미지에 1이라는 상수항을 미리 augmentation을 시켰으면 x와 w의 innerproduct로 끝났을텐데 신경망에서는 이 1을 나중에 계산상의 편의성을 위해 미리 augmentation을 시키지 않고 바깥쪽으로 빼버린다.

[신경망에서 쓰는 퍼셉트론의 형태]

x1 -> w1 -> 

x2 -> w2 ->  a = w.T+b   ->  z = h(a) -> y헷

x3 -> w3 ->


$$\ a = \sum_{i=1}^3 w_i x_i + b = w^T x + b $$

h는 활성화 함수(activation function)를 말한다.

- 여기까지는 선형모형이고 위의 이 a라는 값을 어떤 비선형 함수에 통과시켜서 z값을 추출하게 된다. 핵심은 비선형 함수를 써야한다는 것이다. 비선형함수를 사용하지 않으면 신경망을 여러겹으로 쌓는 의미가 없다.


- 먼저 활성화 함수를 알아봐야 하는데 신경망은 보통 로지스틱함수를 많이 쓴다. 그 이유는 우리가 최적화를 하려면 미분값을 계산해야 하는데 로지스틱함수는 미분값을 계산할때 다음과 같이 쉽게 계산할 수 있기 때문이다. $$\ \dfrac{d\sigma(a)}{da} = \sigma(a)(1-\sigma(a)) = \sigma(a)\sigma(-a) $$

- 그리고 classification을 해야하기 때문에 $$\ \hat{y} = \text{sign}\left(z - \dfrac{1}{2}\right) = \text{round}(z) $$ 처럼 0.5를 기준으로 classification을 할 수 있다.

- 기존의 퍼셉트론은 헤비사이드 스텝함수를 이용해서 classification을 하였으나 미분이 안된다는 단점때문에 미분이 되는 로지스틱 함수를 이용하는 형태로 x 백터가 들어갔을때 y헷이 나오는 것을 신경망에서는 채택했다

- 하지만 이렇게 하면 y헷은 어디까지나 직선형태가 된다는 문제가 있다. 이 직선형태가 되면 실제로 classification이 안되는 데이터들이 많다. 예를 들어 XOR문제 같은거..


- 그래서 우리는 basis function 방식을 쓸 수 있다. 이 방식이 뭐냐 원래는 x를 쓰는게 맞는데 비선형적인 함수형태를 통과시키자는 것이다. 전체적으로  $$\ a = w^T x + b $$ 형태를 쓰되 x대신에 x를 basis function $$\ \phi(x) $$에 통과시킨 것을 사용하자는 것이다.


- 따라서 아래와 같이 활성화 함수를 쓸 수 있다. $$\ z = h \left( \sum_{j=1}^J w_j \phi_j(x) + b \right) = h \left( w^T \phi(x) + b \right) $$ 여기서 J는 J개의 많은 기저함수를 사용한다는 의미에서 J인 것이다.


- 파이(x)는 다차원 함수이다. x백터가 들어가면 백터가 나오는 형태인데 들어갈때 차원과 나올때 차원은 다를 수 있다. 예를 들어 x백터가 3차원이었는데 100개의 파이가 있었다면 100차원 백터가 나온다는 것이다. 파이를 많이 생각해내면 생각해 낼 수록 좋다 왜냐하면 이 비선형 함수중에 하나는 데이터를 설명할 수 있기 떄문이다. 문제는 파이가 또 너무 많아지면 x간에 상관관계가 심해져서 오버피팅이 발생하거나 컨디션넘버가 높아지는 안좋은 현상이 발생한다.


- 그래서 가장 좋은것은 데이터의 비선형성에 딱맞는 적절한 개수의 파이만 있으면 좋은데 사람은 그것을 시각적으로 확인해서 조율할 수 있는 부분에서 한계가 있기 때문에 현실적으로는 그냥 무작정 많이 파이를 만들어보도록 기계한테 의존하는 것이다.

- 그래서 뭔가 이 파이자체도 모양을 바꿔서 이런저런 파이를 골라내고 싶다. 다시말해 기저함수를 여러개 써본다음에 그중에 좋은게 뭔지를 내가 찾고 싶다. 1차 2차 3차... 등의 함수를 써보고 싶다. 그렇게 하려면 모형자체를 바꾸는 하이퍼파라미터를 조절하면 된다. 좋은 파이를 찾는다는 것은 하이퍼파라미터 최적화를 한다는 의미이다.


- 이 파이에 모양을 바꿀 수 있는 하이퍼파라미터를 하나 생각을 한다. $$\ z = h \left( w^T \phi(x ; \theta) + b \right) $$ 그게 이 식에서 세타가 그것을 의미한다. 예를 들어서 x의 세타승이라고 하면 세타가 1일때는 1차함수, 세타가 2일때는 2차함수 이런방식이 되는 것이다. 그래서 이 세타를 할 수 있는 것을 다써보고 가장 좋은 것을 고르는 것이 하이퍼파라미터 최적화이다.


- 이 파이를 찾는데 어떤 방법이 좋을까 생각하다가 사람들이 발견한 것이 이 파이를 갖다가 뭔가 비선형 함수를 써야하는데 사람이 생각하기에 한계가 있으니까 퍼셉트론 자체에 활성화함수를 파이로 바로 활용해보자는 것이다. 


- $$\ z = h \left( \sum_{j=1}^M w_j h \left(w_{j}^{(1)} x + b_j^{(1)} \right)  + b \right) $$ 이런식으로..


- 위 식에서 w와 b를 바꿔주게 되면 원래 파이의 모양이 달라지게 되는 효과와 같다는 것이다. 다시말해 이 w와 b를 조절해주면 우리가 쓰는 파이라고 하는 basis function이 변한다는 것이다. 이런식으로 만들면 신경망을 구현할 수 있다.

- 정말로 이런식으로 하면 비선형 문제를 풀수 있는가에 대해 수학적으로 증명한 정리 Universal Approximation Theorem에서는 이 basis function(파이)을 무한히 많이 사용하면 어떠한 형태의 함수와도 유사한 형태의 함수 z(x) 를 만들 수 있다고 한다.파이를 몇개를 만들어야 한다고 단정하지는 않았다.


- 퍼셉트론은 하나만 썼을경우 linear한 문제밖에 못풀지만 그걸 여러개로 연결시켜서 네트워크를 만들게되면 XOR 같은 비선형문제도 풀 수 있다.


- 신경망에서는 Multi-classification을 할때 OvR 방식을 사용하기 위해 출력계층을 여러개 두는 경우도 있다. 통상 저렇게 여러개의 출력계층을 둘때는 출력계층에 소프트맥스 함수를 붙인다. 소프트맥스 함수는 들어가는 입력의 순서는 바뀌지 않으면서 모두 더했을때 합이 1이되게 하는 함수이다. 소프트맥스 함수를 쓰게되면 마치 확률인것처럼 보이게 된다. 실제 확률은 아니지만.. 어떤 카테고리 확률값으로 보이는 것이다.
