---
layout: post
title: "회귀분석에서 분산분석과 모형성능 기초개념"
tags: [선형회귀분석]
comments: true
---

.

#### # 학습 시 참고한 URL : https://datascienceschool.net

### # 휘귀분석에서 분산분석과 모형성능

- 분산분석이라고 하는 것은 예를들어서 A라는 사람과 B라는 사람이 둘다 똑같은 문제를 풀기위해 회귀분석을 했다고 치자. 그런데 똑같은 y값을 예측하는 것이 공통된 목표였지만 각자 선택할 독립변수의 갯수라던가 독립변수의 종류다던가는 다를 수 있고, 심지어는 데이터의 갯수도 다를 수도 있다.


- 그러면 A와 B중에 누가 더 좋은 모델을 만들었는가 비교하려면 어떻게 해야하는가. 우리가 생각하는 잔차를 제곱합을 비교하면 되지 않은가라고도 할 수 있다. 그러나 이 잔차제곱합의 문제는 그냥 잔차를 더한것이기 때문에 데이터를 많이 쓴 사람은 더 불리하다. 그러면 이걸 1/N으로 나누면 안되냐. 이 경우는 독립변수의 수를 적게 쓴사람이 불리하게 된다.


- 그래서 우리가 알고 있는 잔차제곱합 또는 mean square error는 올바른 비교평가를 하는 수단으로 적절하지는 않다. 그래서 올바른 비교평가를 하자는게 분산분석이다.


- 이 분산분석을 위해 알아할 용어들이 몇가지가 있는데 가장먼저 알아야할게 TSS(total sum of square)이다.

$$\ \text{TSS} = \sum_{i=1}^N (y_i-\bar{y})^2 = (y - \bar{y})^T(y - \bar{y} ) $$

y는 실제 y값을 말하는 것이고, y bar는 y값의 평균을 말하는 것이다. 이 y값의 분산에 관련된 크기가 TSS다. 종속변수값의 변화정도를 나타내는 값이다.


- ESS(explained sum of squares)라는 것도 알아야 한다.

$$\ \text{ESS}=\sum_{i=1}^N (\hat{y}_i -\bar{\hat{y}})^2 = (\hat{y} - \bar{\hat{y}})^T(\hat{y} - \bar{\hat{y}}) $$

우리가 회귀분석 모형을 만들었으면 거기에서 y에 대한 예측값을 계산할 수 있다. 이게 y hat이다. ESS는 이 y hat의 분산의 정도를 말하는 것이다. 참고로 이게 상수항을 가지는 올바른 모델이라면 y hat의 평균값과 y의 평균값과 똑같이 나와야한다.


- 마지막으로 RSS(residual sum of squares)도 알아야한다.

$$\ \text{RSS}=\sum_{i=1}^N (y_i - \hat{y}_i)^2\ = e^Te $$

이것은 오차를 제곱해서 더한값이다.


- 위와 같은 세가지는 어떤 성질이 존재하냐면 'TSS = ESS + RSS'가 성립한다. 이 말은 ESS와 RSS는 TSS보다 커질 수 없다는 말과 같다.


- 우리는 이러한 성질을 이용해서 회귀분석의 성능을 측정하는 점수를 계산할 수 있다. 그것을 결정계수라고하는데 수식으로는 다음과 같이 표현할 수 있다.

$$\ R^2 \equiv 1 - \dfrac{\text{RSS}}{\text{TSS}}\ = \dfrac{\text{ESS}}{\text{TSS}}\ $$

왜 결정계수를 회귀분석의 성능을 나타낸다고 보고있냐면 예를 들어서 가장 좋은 모델이라고 하면 오차가 하나도 없는 것을 말하는데 이는 잔차가 0이고 1에서 0을 빼면 결국에는 1이기 때문이다. 반대로 RSS가 크면 클수록 결정계수는 0으로 가까워지기 때문에 나쁜모델이다. 이는 통상적인 점수체계라는 것이다.

R 스퀘어는 다음과 같은 성질을 갖고있다.

$$\ 0 \leq R^2  \leq 1 $$


- 실제로 회귀분석을 할때는 특별한 경우가 아닌이상 항상 W0 상수항을 넣어줘야 한다. 상수항을 넣지 않는다는 법칙이 있을때를 빼고는 거의 모든 경우에는 상수항을 넣는다고 본다. 그냥 상수항을 무조건 넣는다고 봐야한다. 상수항을 제외하면 어떤상황이 발생하냐면 잔차의 합 다시말해 잔차평균이 0이라는 것이 성립할 수 없다. 결론적으로 그러면 R 스케어 값이 0 ~ 1 사이의 값으로 존재한다는 보장이 없어진다.


- F검정을 만드는 원리를 응용하면 다음과 같은 것도 가능하다. 원래 single coefficient t-test는 w 하나만 놓고 0이다 아니다 라는 것을 판단하는 것이고, regression t-test는 모든 w가 다 0이다라는 것을 귀무가설로 놓고 있다. 근데 예를들어서 중간에 있는 w2와 w3만 0이다 라는 이런검정도 가능하다. StatsModels에서 anova_lm 명령어를 이용하면 된다.


- 조정결정계수도 선형회귀분석의 성능을 나타내는 지표인데 일종에 패널티요소가 있는 지표이다. 기본적으로 R 스퀘어 값은 독립변수 많이 넣으면 넣을수록 잘나오는 구조상의 문제가 있다. 이 수치만 높이고자 했을때는 나중에 오버피팅이 발생될수도 있다. 그래서 R 스퀘어 값이 잘나온다고해서 정말로 그게 성능이 좋다고 단정할 수는 없다. 그래서 나온 아이디어가 파라미터의 갯수 즉 모수를 갖다 많이 쓴 사람은 벌점을 줘야겠다는 아이디어가 나왔다. 그래서 벌점을 어떻게 주느냐 이런거에 대해 수학적으로 완전한 정의는 없기는 하지만 수학자들이 생각해 낸것이 조정결정계수, AIC, BIC이다.


- 조정결정계수의 수식은 아래와 같다.

$$\ R_{adj}^2 = 1 - \frac{n-1}{n-K}(1-R^2) = \dfrac{(n-1)R^2 +1-K}{n-K} $$

독립변수 K의 갯수가 커질수록 영향력이 커져서 R스퀘어 값에 영향을 미친다.


- AIC, BIC의 수식은 다음과 같다.

$$\ \text{AIC} = -2\log L + 2K $$

$$\ \text{BIC} = -2\log L + K\log n $$

우리가 확률적인 모형을 쓰면 w값을 구할때 최대가능도 방법을 쓰게 되는데 이 최대가능도에도 벌점을 주자는 아이디어다. 파라미터의 갯수가 너무 많으면 파라미터 갯수만큼 패널티를 주자는 것인데 사람마다 다르게 생각해서 AIC는 모수의 갯수만큼 그냥 패널티를 주자이고, BIC는 모수의 갯수에 데이터 갯수에 로그값을 곱해서 패널티를 주자는 것이다. AIC와 BIC 수식 앞에는 -가 붙어있는데 이는 최대가능도를 최대화한 값은 대부분의 경우 음수가 나오기 때문에 보기가 흉해서 양수로 바꾸기 위함이다. AIC와 BIC는 수치가 작을수록 좋은 것이다.
