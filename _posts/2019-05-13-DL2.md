---
layout: post
title: "Back propagation 기초개념"
tags: [딥러닝]
comments: true
---

.

#### # 학습시 참고한 URL : https://www.fastcampus.co.kr/data_online_databasic

우리가 딥러닝에서 weight를 구할때 gradient descent로 미분을 한다는건 알겠는데 그러면 어떻게 할것인가에 대해 아래 그림을 보면서 생각을 해보자.

![1](https://user-images.githubusercontent.com/41605276/57620286-477da580-75c3-11e9-91ca-204201201791.png)

우리가 결국에는 z2를 잘 구해서 최대한 정답과 같아야하는게 목표다.

z11 = x1 * w11 + x2 * w12 + x3 * w13 + x4 * w14

$\  \begin{eqnarray} 
a11 = \sigma(z11) = \frac{1}{1+e^{-z11}} 
\end{eqnarray} $

z2 = a11 * w21 + a12 * w22 + a13 * w23

$\  \begin{eqnarray} 
a2 = \sigma(z2) = \frac{1}{1+e^{-z2}} 
\end{eqnarray} $

$\ L = (y - a2)^2 $ 일때

이때 w11을 update하기 위해 L(Loss)를 w11로 미분을해야 하는데 이거 식이 복잡해서 어떻게 하냐! 라는 문제가 나온다.

Loss부터 거꾸로 한단계씩 미분을 해보자

step1) Loss를 a2로 미분한다.

$\ \begin{eqnarray}  
\frac{\partial L}{\partial a2} =  -2(y - a2)
\end{eqnarray} $

step2) a2를 z2로 미분한다.

$\ \begin{eqnarray}  
\frac{\partial a2}{\partial z2} =  1
\end{eqnarray} $

step3) z2를 a11로 미분한다.

$\ \begin{eqnarray}  
\frac{\partial z2}{\partial a11} =  w21
\end{eqnarray} $

step4) a11를 z11로 미분한다.

$\ \begin{eqnarray}  
\frac{\partial a11}{\partial z11} =  \sigma(z11) * (1-\sigma(z11))
\end{eqnarray} $

step5) z11를 w11로 미분한다.

$\ \begin{eqnarray}  
\frac{\partial a11}{\partial z11} =  x1
\end{eqnarray} $

step6) 위와같이 다섯단계의 미분을 cahin rule이라는 규칙에 의해서 전부 각각 곱하면 아래와 같다.

$\ \begin{eqnarray}  
\frac{\partial L}{\partial a2} * \frac{\partial a2}{\partial z2} * \frac{\partial z2}{\partial a11} * \frac{\partial a11}{\partial z11} * \frac{\partial z11}{\partial w11} = \frac{\partial L}{\partial w11}
\end{eqnarray} $

이런식으로 한단계 한단계 거꾸로 미분값들을 구해서 chain rule에 의해 곱해나가는 것을 back propagation이라고 한다.

정리하면 Loss로부터 거꾸로 한단계씩 미분값을 구하고 이 값들을 chain rule에 의하여 곱해가면서 weight에 대한 gradient를 구하는 방법이다.

Loss로 부터 하나하나 거꾸로 스텝을 밟아가면서 처음에 데이터가 지나갔던 경로의 반대방향으로 weight에 대한 미분값을 곱해나가는 방법이다.
