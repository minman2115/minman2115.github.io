---
layout: post
title: "TF 2.0을 이용한 mnist image classification MLP model 구현"
tags: [딥러닝]
comments: true
---

.

Deep_Learning_Studynotes_(20190713)

study program : https://www.fastcampus.co.kr/data_camp_deeplearning

### step 1) Importing Libraries


```python
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.utils import to_categorical
import numpy as np
import matplotlib.pyplot as plt
import os

print(tf.__version__)
print(keras.__version__)
```

1.15.0
2.2.4-tf
    

### step 2) Enable Eager Mode 설정


```python
if tf.__version__ < '2.0.0':
    tf.enable_eager_execution()
```

### step 3) Hyper Parameters 셋팅


```python
learning_rate = 0.001
training_epochs = 30
batch_size = 100
n_class = 10
```

### step 4) MNIST/Fashion MNIST Data 선택


```python
# ## MNIST Dataset #########################################################
# mnist = keras.datasets.mnist
# class_names = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']
# ##########################################################################

# Fashion MNIST Dataset #################################################
mnist = keras.datasets.fashion_mnist
class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']
#########################################################################
```

### step 5) Datasets 임포트


```python
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()  
```

### step 6) 데이터 셋팅


```python
n_train = train_images.shape[0]
n_test = test_images.shape[0]

train_images = train_images.astype(np.float32) / 255.
test_images = test_images.astype(np.float32) / 255.
    
train_labels = to_categorical(train_labels, 10)
test_labels = to_categorical(test_labels, 10)    
    
train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(buffer_size=100000).batch(batch_size)
test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(batch_size)
```

### step 7) Model 설계


```python
def create_model():
    model = keras.Sequential()
    model.add(keras.layers.Flatten(input_shape=(28,28)))
    ## 기존에 이미지가 28 X 28 사이즈이므로 
    ## 백터로 일자로 펴서 데이터를 넣어줘야 하기 때문에 위와 같이 코딩함
    model.add(keras.layers.Dense(256,activation='relu'))
    ## fully connected layer(MLP 하나의 층)를 추가하는데 몇개의 퍼셉트론을 둘거냐
    ## 여기서는 256개를 두겠다. 활성화함수는 렐루를 쓰겠다.
    model.add(keras.layers.Dense(128,activation='relu'))
    ## 거기에 하나의 층으로 또 128개의 퍼셉트론을 더 쌓겠다. 역시 AF는 렐루로
    model.add(keras.layers.Dense(10,activation='softmax'))
    ## MLP의 마지막 층은 10개로 두겠다. 이 10개는 소프트맥스로..
    return model
```


```python
model = create_model()
model.summary()
```

    Model: "sequential"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    flatten (Flatten)            (None, 784)               0         
    _________________________________________________________________
    dense (Dense)                (None, 256)               200960    
    _________________________________________________________________
    dense_1 (Dense)              (None, 128)               32896     
    _________________________________________________________________
    dense_2 (Dense)              (None, 10)                1290      
    =================================================================
    Total params: 235,146
    Trainable params: 235,146
    Non-trainable params: 0
    _________________________________________________________________
    

### step 8) Loss Function 계산함수 구현


```python
## 아래와 같이 @tf.function을 붙여주면 그래프 모드로 전환되어 연산이 빨라진다.
@tf.function
def loss_fn(model, images, labels):
    predictions = model(images, training=True)
    loss = tf.reduce_mean(keras.losses.categorical_crossentropy(labels, predictions))   
    return loss
```

### step 9) Calculating Gradient & Updating Weights 함수 구현


```python
@tf.function
def train(model, images, labels):
    with tf.GradientTape() as tape:
        loss = loss_fn(model, images, labels)
    grads = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(grads, model.trainable_variables))
```

### step 10) Caculating Model's Accuracy 계산함수 구현


```python
@tf.function
def evaluate(model, images, labels):
    predictions = model(images, training=False)
    correct_prediction = tf.equal(tf.argmax(predictions, 1), tf.argmax(labels, 1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))    
    return accuracy
```

### step 11) Optimizer 설정


```python
optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
```

### step 12) Training 수행


```python
# train my model
print('Learning started. It takes sometime.')
for epoch in range(training_epochs):
    avg_loss = 0.
    avg_train_acc = 0.
    avg_test_acc = 0.
    train_step = 0
    test_step = 0
    
    for images, labels in train_dataset:
    ## for images, labels in train_dataset.repeat(training_epochs):
    ## 위와 같이 .repeat를 적용해주면 한 에포크를 돌때마다 셔플을 해준다.
        train(model,images, labels)
        loss = loss_fn(model, images, labels)
        acc = evaluate(model, images, labels)
        avg_loss = avg_loss + loss
        avg_train_acc = avg_train_acc + acc
        train_step += 1
    avg_loss = avg_loss / train_step
    avg_train_acc = avg_train_acc / train_step
    
    for images, labels in test_dataset:        
        acc = evaluate(model, images, labels)        
        avg_test_acc = avg_test_acc + acc
        test_step += 1    
    avg_test_acc = avg_test_acc / test_step    

    print('Epoch:', '{}'.format(epoch + 1), 
          'loss =', '{:.8f}'.format(avg_loss), 
          'train accuracy = ', '{:.4f}'.format(avg_train_acc), 
          'test accuracy = ', '{:.4f}'.format(avg_test_acc))


print('Learning Finished!')
```

    Learning started. It takes sometime.
    Epoch: 1 loss = 0.47639787 train accuracy =  0.8325 test accuracy =  0.8572
    Epoch: 2 loss = 0.33990750 train accuracy =  0.8770 test accuracy =  0.8619
    Epoch: 3 loss = 0.30069754 train accuracy =  0.8905 test accuracy =  0.8631
    Epoch: 4 loss = 0.27578470 train accuracy =  0.8993 test accuracy =  0.8707
    Epoch: 5 loss = 0.25728405 train accuracy =  0.9048 test accuracy =  0.8709
    Epoch: 6 loss = 0.24201177 train accuracy =  0.9098 test accuracy =  0.8716
    Epoch: 7 loss = 0.22960480 train accuracy =  0.9149 test accuracy =  0.8759
    Epoch: 8 loss = 0.21826755 train accuracy =  0.9182 test accuracy =  0.8763
    Epoch: 9 loss = 0.20687494 train accuracy =  0.9234 test accuracy =  0.8808
    Epoch: 10 loss = 0.19747676 train accuracy =  0.9268 test accuracy =  0.8806
    Epoch: 11 loss = 0.18814477 train accuracy =  0.9298 test accuracy =  0.8798
    Epoch: 12 loss = 0.18081743 train accuracy =  0.9330 test accuracy =  0.8802
    Epoch: 13 loss = 0.17324357 train accuracy =  0.9363 test accuracy =  0.8843
    Epoch: 14 loss = 0.16490680 train accuracy =  0.9396 test accuracy =  0.8802
    Epoch: 15 loss = 0.15808807 train accuracy =  0.9420 test accuracy =  0.8781
    Epoch: 16 loss = 0.15160316 train accuracy =  0.9442 test accuracy =  0.8795
    Epoch: 17 loss = 0.14696893 train accuracy =  0.9464 test accuracy =  0.8813
    Epoch: 18 loss = 0.13925385 train accuracy =  0.9506 test accuracy =  0.8831
    Epoch: 19 loss = 0.13288255 train accuracy =  0.9525 test accuracy =  0.8818
    Epoch: 20 loss = 0.12987131 train accuracy =  0.9530 test accuracy =  0.8793
    Epoch: 21 loss = 0.12476737 train accuracy =  0.9551 test accuracy =  0.8804
    Epoch: 22 loss = 0.11875692 train accuracy =  0.9568 test accuracy =  0.8817
    Epoch: 23 loss = 0.11625367 train accuracy =  0.9588 test accuracy =  0.8772
    Epoch: 24 loss = 0.11066196 train accuracy =  0.9608 test accuracy =  0.8790
    Epoch: 25 loss = 0.10518023 train accuracy =  0.9619 test accuracy =  0.8798
    Epoch: 26 loss = 0.10316288 train accuracy =  0.9631 test accuracy =  0.8867
    Epoch: 27 loss = 0.09916883 train accuracy =  0.9652 test accuracy =  0.8836
    Epoch: 28 loss = 0.09430185 train accuracy =  0.9667 test accuracy =  0.8808
    Epoch: 29 loss = 0.09066495 train accuracy =  0.9678 test accuracy =  0.8788
    Epoch: 30 loss = 0.08960148 train accuracy =  0.9687 test accuracy =  0.8809
    Learning Finished!
    

### step 13) 트레이닝한 모델 테스트


```python
def plot_image(i, predictions_array, true_label, img):
    predictions_array, true_label, img = predictions_array[i], true_label[i], img[i]
    plt.grid(False)
    plt.xticks([])
    plt.yticks([])

    plt.imshow(img,cmap=plt.cm.binary)

    predicted_label = np.argmax(predictions_array)
    if predicted_label == true_label:
        color = 'blue'
    else:
        color = 'red'

    plt.xlabel("{} {:2.0f}% ({})".format(class_names[predicted_label],
                                100*np.max(predictions_array),
                                class_names[true_label]),
                                color=color)

def plot_value_array(i, predictions_array, true_label):
    predictions_array, true_label = predictions_array[i], true_label[i]
    plt.grid(False)
    #plt.xticks([])
    plt.xticks(range(n_class), class_names, rotation=90)
    plt.yticks([])
    thisplot = plt.bar(range(n_class), predictions_array, color="#777777")
    plt.ylim([0, 1]) 
    predicted_label = np.argmax(predictions_array)
 
    thisplot[predicted_label].set_color('red')
    thisplot[true_label].set_color('blue')
```


```python
rnd_idx = np.random.randint(1, n_test//batch_size)
img_cnt = 0
for images, labels in test_dataset:
    img_cnt += 1
    if img_cnt != rnd_idx:
        continue
    predictions = model(images, training=False)
    num_rows = 5
    num_cols = 3
    num_images = num_rows*num_cols
    labels = tf.argmax(labels, axis=-1)
    plt.figure(figsize=(3*2*num_cols, 4*num_rows))
    plt.subplots_adjust(hspace=1.0)
    for i in range(num_images):
        plt.subplot(num_rows, 2*num_cols, 2*i+1)
        plot_image(i, predictions.numpy(), labels.numpy(), images.numpy())
        plt.subplot(num_rows, 2*num_cols, 2*i+2)
        plot_value_array(i, predictions.numpy(), labels.numpy())        
    break
```


![MLP_27_0](https://user-images.githubusercontent.com/41605276/75111007-68a61300-5678-11ea-9493-76fbcaac2014.png)


#### step 14) step 7) 모델 설계 부분에서 레이어를 더 쌓아서 모델을 만들고 테스트를 해보자


```python
def create_model():
    model = keras.Sequential()
    model.add(keras.layers.Flatten(input_shape=(28,28)))
    model.add(keras.layers.Dense(256, activation='relu'))
    model.add(keras.layers.Dense(256, activation='relu'))    
    model.add(keras.layers.Dense(128, activation='relu'))
    model.add(keras.layers.Dense(128, activation='relu'))    
    model.add(keras.layers.Dense(10, activation='softmax'))
    return model

model = create_model()
model.summary()
```

    Model: "sequential_1"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    flatten_1 (Flatten)          (None, 784)               0         
    _________________________________________________________________
    dense_3 (Dense)              (None, 256)               200960    
    _________________________________________________________________
    dense_4 (Dense)              (None, 256)               65792     
    _________________________________________________________________
    dense_5 (Dense)              (None, 128)               32896     
    _________________________________________________________________
    dense_6 (Dense)              (None, 128)               16512     
    _________________________________________________________________
    dense_7 (Dense)              (None, 10)                1290      
    =================================================================
    Total params: 317,450
    Trainable params: 317,450
    Non-trainable params: 0
    _________________________________________________________________
    


```python
@tf.function
def loss_fn(model, images, labels):
    predictions = model(images, training=True)
    loss = tf.reduce_mean(keras.losses.categorical_crossentropy(labels, predictions))   
    return loss
```


```python
@tf.function
def train(model, images, labels):
    with tf.GradientTape() as tape:
        loss = loss_fn(model, images, labels)
    grads = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(grads, model.trainable_variables))
```


```python
@tf.function
def evaluate(model, images, labels):
    predictions = model(images, training=False)
    correct_prediction = tf.equal(tf.argmax(predictions, 1), tf.argmax(labels, 1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))    
    return accuracy
```


```python
optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
```


```python
# train my model
print('Learning started. It takes sometime.')
for epoch in range(training_epochs):
    avg_loss = 0.
    avg_train_acc = 0.
    avg_test_acc = 0.
    train_step = 0
    test_step = 0
    
    for images, labels in train_dataset:
    ## for images, labels in train_dataset.repeat(training_epochs):
    ## 위와 같이 .repeat를 적용해주면 한 에포크를 돌때마다 셔플을 해준다.
        train(model,images, labels)
        loss = loss_fn(model, images, labels)
        acc = evaluate(model, images, labels)
        avg_loss = avg_loss + loss
        avg_train_acc = avg_train_acc + acc
        train_step += 1
    avg_loss = avg_loss / train_step
    avg_train_acc = avg_train_acc / train_step
    
    for images, labels in test_dataset:        
        acc = evaluate(model, images, labels)        
        avg_test_acc = avg_test_acc + acc
        test_step += 1    
    avg_test_acc = avg_test_acc / test_step    

    print('Epoch:', '{}'.format(epoch + 1), 
          'loss =', '{:.8f}'.format(avg_loss), 
          'train accuracy = ', '{:.4f}'.format(avg_train_acc), 
          'test accuracy = ', '{:.4f}'.format(avg_test_acc))


print('Learning Finished!')
```

    Learning started. It takes sometime.
    Epoch: 1 loss = 0.47501740 train accuracy =  0.8304 test accuracy =  0.8478
    Epoch: 2 loss = 0.33531702 train accuracy =  0.8764 test accuracy =  0.8673
    Epoch: 3 loss = 0.29918000 train accuracy =  0.8904 test accuracy =  0.8651
    Epoch: 4 loss = 0.27641636 train accuracy =  0.8987 test accuracy =  0.8673
    Epoch: 5 loss = 0.25761661 train accuracy =  0.9043 test accuracy =  0.8669
    Epoch: 6 loss = 0.24343728 train accuracy =  0.9096 test accuracy =  0.8696
    Epoch: 7 loss = 0.23062919 train accuracy =  0.9132 test accuracy =  0.8714
    Epoch: 8 loss = 0.22028473 train accuracy =  0.9174 test accuracy =  0.8760
    Epoch: 9 loss = 0.20790064 train accuracy =  0.9213 test accuracy =  0.8705
    Epoch: 10 loss = 0.19437486 train accuracy =  0.9270 test accuracy =  0.8668
    Epoch: 11 loss = 0.18968688 train accuracy =  0.9282 test accuracy =  0.8693
    Epoch: 12 loss = 0.17898302 train accuracy =  0.9322 test accuracy =  0.8722
    Epoch: 13 loss = 0.17297472 train accuracy =  0.9339 test accuracy =  0.8733
    Epoch: 14 loss = 0.16414919 train accuracy =  0.9374 test accuracy =  0.8812
    Epoch: 15 loss = 0.15549290 train accuracy =  0.9414 test accuracy =  0.8747
    Epoch: 16 loss = 0.14773223 train accuracy =  0.9438 test accuracy =  0.8733
    Epoch: 17 loss = 0.14385323 train accuracy =  0.9455 test accuracy =  0.8800
    Epoch: 18 loss = 0.13953814 train accuracy =  0.9473 test accuracy =  0.8766
    Epoch: 19 loss = 0.13319071 train accuracy =  0.9498 test accuracy =  0.8733
    Epoch: 20 loss = 0.12888648 train accuracy =  0.9515 test accuracy =  0.8713
    Epoch: 21 loss = 0.12361352 train accuracy =  0.9527 test accuracy =  0.8690
    Epoch: 22 loss = 0.11936069 train accuracy =  0.9553 test accuracy =  0.8751
    Epoch: 23 loss = 0.11733045 train accuracy =  0.9555 test accuracy =  0.8690
    Epoch: 24 loss = 0.11105530 train accuracy =  0.9581 test accuracy =  0.8685
    Epoch: 25 loss = 0.10841321 train accuracy =  0.9588 test accuracy =  0.8714
    Epoch: 26 loss = 0.10420118 train accuracy =  0.9604 test accuracy =  0.8689
    Epoch: 27 loss = 0.10033701 train accuracy =  0.9614 test accuracy =  0.8758
    Epoch: 28 loss = 0.09626953 train accuracy =  0.9630 test accuracy =  0.8770
    Epoch: 29 loss = 0.09360971 train accuracy =  0.9639 test accuracy =  0.8739
    Epoch: 30 loss = 0.08999714 train accuracy =  0.9655 test accuracy =  0.8807
    Learning Finished!
    


```python
rnd_idx = np.random.randint(1, n_test//batch_size)
img_cnt = 0
for images, labels in test_dataset:
    img_cnt += 1
    if img_cnt != rnd_idx:
        continue
    predictions = model(images, training=False)
    num_rows = 5
    num_cols = 3
    num_images = num_rows*num_cols
    labels = tf.argmax(labels, axis=-1)
    plt.figure(figsize=(3*2*num_cols, 4*num_rows))
    plt.subplots_adjust(hspace=1.0)
    for i in range(num_images):
        plt.subplot(num_rows, 2*num_cols, 2*i+1)
        plot_image(i, predictions.numpy(), labels.numpy(), images.numpy())
        plt.subplot(num_rows, 2*num_cols, 2*i+2)
        plot_value_array(i, predictions.numpy(), labels.numpy())        
    break
```


![MLP_35_0](https://user-images.githubusercontent.com/41605276/75111017-7f4c6a00-5678-11ea-802a-15bd1abea2be.png)


위의 결과에서 알 수 있듯이 레이어를 더 쌓아도 기존의 모델과 별 차이가 없음을 확인할 수 있었다.

그렇다면 레이어를 더 쌓았는데 비슷하거나 좀 더 성능이 안좋게 나온거라면 오버피팅이 아니냐라는 판단을 할 수 있고, drop-out을 적용해보자.

### step 15) 위에 모델에서 추가적으로 drop-out을 적용한 MLP 모델 만들기


```python
learning_rate = 0.001
training_epochs = 30
batch_size = 100
n_class = 10
drop_rate = 0.3

def create_model():
    model = keras.Sequential()
    model.add(keras.layers.Flatten(input_shape=(28,28)))
    model.add(keras.layers.Dense(256, activation='relu'))
    model.add(keras.layers.Dropout(drop_rate))
    model.add(keras.layers.Dense(256, activation='relu'))
    model.add(keras.layers.Dropout(drop_rate))
    model.add(keras.layers.Dense(128, activation='relu'))
    model.add(keras.layers.Dropout(drop_rate))
    model.add(keras.layers.Dense(128, activation='relu'))
    model.add(keras.layers.Dropout(drop_rate))
    model.add(keras.layers.Dense(10, activation='softmax'))
    return model
```


```python
model = create_model()
model.summary()
```

    Model: "sequential_2"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    flatten_2 (Flatten)          (None, 784)               0         
    _________________________________________________________________
    dense_8 (Dense)              (None, 256)               200960    
    _________________________________________________________________
    dropout (Dropout)            (None, 256)               0         
    _________________________________________________________________
    dense_9 (Dense)              (None, 256)               65792     
    _________________________________________________________________
    dropout_1 (Dropout)          (None, 256)               0         
    _________________________________________________________________
    dense_10 (Dense)             (None, 128)               32896     
    _________________________________________________________________
    dropout_2 (Dropout)          (None, 128)               0         
    _________________________________________________________________
    dense_11 (Dense)             (None, 128)               16512     
    _________________________________________________________________
    dropout_3 (Dropout)          (None, 128)               0         
    _________________________________________________________________
    dense_12 (Dense)             (None, 10)                1290      
    =================================================================
    Total params: 317,450
    Trainable params: 317,450
    Non-trainable params: 0
    _________________________________________________________________
    


```python
@tf.function
def loss_fn(model, images, labels):
    predictions = model(images, training=True)
    loss = tf.reduce_mean(keras.losses.categorical_crossentropy(labels, predictions))   
    return loss  
```


```python
@tf.function
def train(model, images, labels):
    with tf.GradientTape() as tape:
        loss = loss_fn(model, images, labels)
    grads = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(grads, model.trainable_variables))
```


```python
@tf.function
def evaluate(model, images, labels):
    predictions = model(images, training=False)
    correct_prediction = tf.equal(tf.argmax(predictions, 1), tf.argmax(labels, 1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))    
    return accuracy
```


```python
optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
```


```python
# train my model
print('Learning started. It takes sometime.')
for epoch in range(training_epochs):
    avg_loss = 0.
    avg_train_acc = 0.
    avg_test_acc = 0.
    train_step = 0
    test_step = 0
    
    for images, labels in train_dataset:
        train(model,images, labels)
        loss = loss_fn(model, images, labels)
        acc = evaluate(model, images, labels)
        avg_loss = avg_loss + loss
        avg_train_acc = avg_train_acc + acc
        train_step += 1
    avg_loss = avg_loss / train_step
    avg_train_acc = avg_train_acc / train_step
    
    for images, labels in test_dataset:        
        acc = evaluate(model, images, labels)        
        avg_test_acc = avg_test_acc + acc
        test_step += 1    
    avg_test_acc = avg_test_acc / test_step    

    print('Epoch:', '{}'.format(epoch + 1), 
          'loss =', '{:.8f}'.format(avg_loss), 
          'train accuracy = ', '{:.4f}'.format(avg_train_acc), 
          'test accuracy = ', '{:.4f}'.format(avg_test_acc))


print('Learning Finished!')
```

    Learning started. It takes sometime.
    Epoch: 1 loss = 0.67905092 train accuracy =  0.7969 test accuracy =  0.8321
    Epoch: 2 loss = 0.45486349 train accuracy =  0.8593 test accuracy =  0.8513
    Epoch: 3 loss = 0.41421023 train accuracy =  0.8714 test accuracy =  0.8484
    Epoch: 4 loss = 0.39698279 train accuracy =  0.8756 test accuracy =  0.8561
    Epoch: 5 loss = 0.37147498 train accuracy =  0.8837 test accuracy =  0.8644
    Epoch: 6 loss = 0.35963690 train accuracy =  0.8882 test accuracy =  0.8655
    Epoch: 7 loss = 0.35259974 train accuracy =  0.8906 test accuracy =  0.8659
    Epoch: 8 loss = 0.34292439 train accuracy =  0.8941 test accuracy =  0.8737
    Epoch: 9 loss = 0.33527568 train accuracy =  0.8956 test accuracy =  0.8667
    Epoch: 10 loss = 0.32705668 train accuracy =  0.8983 test accuracy =  0.8776
    Epoch: 11 loss = 0.32352874 train accuracy =  0.8996 test accuracy =  0.8730
    Epoch: 12 loss = 0.31709668 train accuracy =  0.9020 test accuracy =  0.8727
    Epoch: 13 loss = 0.31098703 train accuracy =  0.9036 test accuracy =  0.8711
    Epoch: 14 loss = 0.30652028 train accuracy =  0.9057 test accuracy =  0.8760
    Epoch: 15 loss = 0.30415145 train accuracy =  0.9060 test accuracy =  0.8766
    Epoch: 16 loss = 0.30002409 train accuracy =  0.9080 test accuracy =  0.8779
    Epoch: 17 loss = 0.29688981 train accuracy =  0.9080 test accuracy =  0.8778
    Epoch: 18 loss = 0.29440367 train accuracy =  0.9102 test accuracy =  0.8733
    Epoch: 19 loss = 0.28957576 train accuracy =  0.9117 test accuracy =  0.8856
    Epoch: 20 loss = 0.28372300 train accuracy =  0.9119 test accuracy =  0.8841
    Epoch: 21 loss = 0.28030461 train accuracy =  0.9144 test accuracy =  0.8793
    Epoch: 22 loss = 0.28335866 train accuracy =  0.9130 test accuracy =  0.8829
    Epoch: 23 loss = 0.27794385 train accuracy =  0.9163 test accuracy =  0.8835
    Epoch: 24 loss = 0.27628422 train accuracy =  0.9165 test accuracy =  0.8806
    Epoch: 25 loss = 0.27556121 train accuracy =  0.9165 test accuracy =  0.8847
    Epoch: 26 loss = 0.27177861 train accuracy =  0.9190 test accuracy =  0.8832
    Epoch: 27 loss = 0.27128315 train accuracy =  0.9196 test accuracy =  0.8833
    Epoch: 28 loss = 0.26549268 train accuracy =  0.9200 test accuracy =  0.8837
    Epoch: 29 loss = 0.26837474 train accuracy =  0.9200 test accuracy =  0.8846
    Epoch: 30 loss = 0.26260558 train accuracy =  0.9220 test accuracy =  0.8868
    Learning Finished!
    

트레이닝 속도는 더디지만 실질적인 테스트 퍼포먼스는 더 좋아진 것을 확인할 수 있다.

그러면 이번에는 정규화를 적용해보자.

### step 16) L2 정규화를 적용한 MLP 구현


```python
learning_rate = 0.001
training_epochs = 30
batch_size = 100
n_class = 10
reg_weight = 0.002
## reg_weight는 정규화에서 람다를 얘기한다. 사용자 파라미터로 람다값을 지정해줘야한다.
```


```python
def create_model():
    model = keras.Sequential()
    model.add(keras.layers.Flatten(input_shape=(28,28)))
    model.add(keras.layers.Dense(256, activation='relu',
                                kernel_regularizer=keras.regularizers.l2(reg_weight)))
    model.add(keras.layers.Dense(256, activation='relu',
                                kernel_regularizer=keras.regularizers.l2(reg_weight)))
    model.add(keras.layers.Dense(128, activation='relu',
                                kernel_regularizer=keras.regularizers.l2(reg_weight)))
    model.add(keras.layers.Dense(128, activation='relu',
                                kernel_regularizer=keras.regularizers.l2(reg_weight)))    
    model.add(keras.layers.Dense(10, activation='softmax',
                                kernel_regularizer=keras.regularizers.l2(reg_weight)))
    return model
```


```python
model = create_model()
model.summary()
```

    Model: "sequential_3"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    flatten_3 (Flatten)          (None, 784)               0         
    _________________________________________________________________
    dense_13 (Dense)             (None, 256)               200960    
    _________________________________________________________________
    dense_14 (Dense)             (None, 256)               65792     
    _________________________________________________________________
    dense_15 (Dense)             (None, 128)               32896     
    _________________________________________________________________
    dense_16 (Dense)             (None, 128)               16512     
    _________________________________________________________________
    dense_17 (Dense)             (None, 10)                1290      
    =================================================================
    Total params: 317,450
    Trainable params: 317,450
    Non-trainable params: 0
    _________________________________________________________________
    


```python
@tf.function
def loss_fn(model, images, labels):
    predictions = model(images, training=True)
    loss = tf.reduce_mean(keras.losses.categorical_crossentropy(labels, predictions))   
    return loss  
```


```python
@tf.function
def train(model, images, labels):
    with tf.GradientTape() as tape:
        loss = loss_fn(model, images, labels)
    grads = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(grads, model.trainable_variables))
```


```python
@tf.function
def evaluate(model, images, labels):
    predictions = model(images, training=False)
    correct_prediction = tf.equal(tf.argmax(predictions, 1), tf.argmax(labels, 1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))    
    return accuracy
```


```python
optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
```


```python
# train my model
print('Learning started. It takes sometime.')
for epoch in range(training_epochs):
    avg_loss = 0.
    avg_train_acc = 0.
    avg_test_acc = 0.
    train_step = 0
    test_step = 0
    
    for images, labels in train_dataset:
        train(model,images, labels)
        loss = loss_fn(model, images, labels)
        acc = evaluate(model, images, labels)
        avg_loss = avg_loss + loss
        avg_train_acc = avg_train_acc + acc
        train_step += 1
    avg_loss = avg_loss / train_step
    avg_train_acc = avg_train_acc / train_step
    
    for images, labels in test_dataset:        
        acc = evaluate(model, images, labels)        
        avg_test_acc = avg_test_acc + acc
        test_step += 1    
    avg_test_acc = avg_test_acc / test_step    

    print('Epoch:', '{}'.format(epoch + 1), 
          'loss =', '{:.8f}'.format(avg_loss), 
          'train accuracy = ', '{:.4f}'.format(avg_train_acc), 
          'test accuracy = ', '{:.4f}'.format(avg_test_acc))


print('Learning Finished!')
```

    Learning started. It takes sometime.
    Epoch: 1 loss = 0.47498590 train accuracy =  0.8297 test accuracy =  0.8521
    Epoch: 2 loss = 0.33609486 train accuracy =  0.8769 test accuracy =  0.8532
    Epoch: 3 loss = 0.29754549 train accuracy =  0.8906 test accuracy =  0.8598
    Epoch: 4 loss = 0.27347901 train accuracy =  0.8996 test accuracy =  0.8687
    Epoch: 5 loss = 0.25472662 train accuracy =  0.9056 test accuracy =  0.8663
    Epoch: 6 loss = 0.23882887 train accuracy =  0.9094 test accuracy =  0.8668
    Epoch: 7 loss = 0.22782262 train accuracy =  0.9146 test accuracy =  0.8728
    Epoch: 8 loss = 0.21594048 train accuracy =  0.9194 test accuracy =  0.8743
    Epoch: 9 loss = 0.20323046 train accuracy =  0.9232 test accuracy =  0.8778
    Epoch: 10 loss = 0.19588661 train accuracy =  0.9263 test accuracy =  0.8758
    Epoch: 11 loss = 0.18484439 train accuracy =  0.9305 test accuracy =  0.8797
    Epoch: 12 loss = 0.17420626 train accuracy =  0.9342 test accuracy =  0.8825
    Epoch: 13 loss = 0.17098653 train accuracy =  0.9356 test accuracy =  0.8808
    Epoch: 14 loss = 0.16309783 train accuracy =  0.9390 test accuracy =  0.8813
    Epoch: 15 loss = 0.15573657 train accuracy =  0.9411 test accuracy =  0.8781
    Epoch: 16 loss = 0.14812456 train accuracy =  0.9439 test accuracy =  0.8803
    Epoch: 17 loss = 0.14280631 train accuracy =  0.9458 test accuracy =  0.8789
    Epoch: 18 loss = 0.13859987 train accuracy =  0.9470 test accuracy =  0.8801
    Epoch: 19 loss = 0.13351709 train accuracy =  0.9491 test accuracy =  0.8788
    Epoch: 20 loss = 0.12604228 train accuracy =  0.9525 test accuracy =  0.8801
    Epoch: 21 loss = 0.12021360 train accuracy =  0.9543 test accuracy =  0.8741
    Epoch: 22 loss = 0.11822187 train accuracy =  0.9558 test accuracy =  0.8773
    Epoch: 23 loss = 0.11813249 train accuracy =  0.9563 test accuracy =  0.8796
    Epoch: 24 loss = 0.11304576 train accuracy =  0.9569 test accuracy =  0.8789
    Epoch: 25 loss = 0.10771424 train accuracy =  0.9595 test accuracy =  0.8734
    Epoch: 26 loss = 0.10143368 train accuracy =  0.9618 test accuracy =  0.8828
    Epoch: 27 loss = 0.09970266 train accuracy =  0.9631 test accuracy =  0.8807
    Epoch: 28 loss = 0.09535039 train accuracy =  0.9635 test accuracy =  0.8767
    Epoch: 29 loss = 0.09164222 train accuracy =  0.9651 test accuracy =  0.8814
    Epoch: 30 loss = 0.08736499 train accuracy =  0.9672 test accuracy =  0.8843
    Learning Finished!
    

### step 17) Batch norm을 적용한 MLP 구현


```python
learning_rate = 0.001
training_epochs = 30
batch_size = 100
n_class = 10
```


```python
def create_model():
    ## 배치놈은 적용할때 순서를 아래오 같이 잘 적용해줘야 한다.
    model = keras.Sequential()
    model.add(keras.layers.Flatten(input_shape=(28,28)))
    model.add(keras.layers.Dense(256))
    model.add(keras.layers.BatchNormalization())
    model.add(keras.layers.ReLU())
    model.add(keras.layers.Dense(256))
    model.add(keras.layers.BatchNormalization())
    model.add(keras.layers.ReLU())
    model.add(keras.layers.Dense(128))
    model.add(keras.layers.BatchNormalization())
    model.add(keras.layers.ReLU())
    model.add(keras.layers.Dense(128))
    model.add(keras.layers.BatchNormalization())
    model.add(keras.layers.ReLU())
    model.add(keras.layers.Dense(10))
    model.add(keras.layers.BatchNormalization())
    model.add(keras.layers.Softmax())
    return model
```


```python
model = create_model()
model.summary()
```

    Model: "sequential_4"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    flatten_4 (Flatten)          (None, 784)               0         
    _________________________________________________________________
    dense_18 (Dense)             (None, 256)               200960    
    _________________________________________________________________
    batch_normalization (BatchNo (None, 256)               1024      
    _________________________________________________________________
    re_lu (ReLU)                 (None, 256)               0         
    _________________________________________________________________
    dense_19 (Dense)             (None, 256)               65792     
    _________________________________________________________________
    batch_normalization_1 (Batch (None, 256)               1024      
    _________________________________________________________________
    re_lu_1 (ReLU)               (None, 256)               0         
    _________________________________________________________________
    dense_20 (Dense)             (None, 128)               32896     
    _________________________________________________________________
    batch_normalization_2 (Batch (None, 128)               512       
    _________________________________________________________________
    re_lu_2 (ReLU)               (None, 128)               0         
    _________________________________________________________________
    dense_21 (Dense)             (None, 128)               16512     
    _________________________________________________________________
    batch_normalization_3 (Batch (None, 128)               512       
    _________________________________________________________________
    re_lu_3 (ReLU)               (None, 128)               0         
    _________________________________________________________________
    dense_22 (Dense)             (None, 10)                1290      
    _________________________________________________________________
    batch_normalization_4 (Batch (None, 10)                40        
    _________________________________________________________________
    softmax (Softmax)            (None, 10)                0         
    =================================================================
    Total params: 320,562
    Trainable params: 319,006
    Non-trainable params: 1,556
    _________________________________________________________________
    


```python
@tf.function
def loss_fn(model, images, labels):
    predictions = model(images, training=True)
    loss = tf.reduce_mean(keras.losses.categorical_crossentropy(labels, predictions))   
    return loss  
```


```python
@tf.function
def train(model, images, labels):
    with tf.GradientTape() as tape:
        loss = loss_fn(model, images, labels)
    grads = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(grads, model.trainable_variables))
```


```python
@tf.function
def evaluate(model, images, labels):
    predictions = model(images, training=False)
    correct_prediction = tf.equal(tf.argmax(predictions, 1), tf.argmax(labels, 1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))    
    return accuracy
```


```python
optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
```


```python
# train my model
print('Learning started. It takes sometime.')
for epoch in range(training_epochs):
    avg_loss = 0.
    avg_train_acc = 0.
    avg_test_acc = 0.
    train_step = 0
    test_step = 0
    
    for images, labels in train_dataset:
        train(model,images, labels)
        loss = loss_fn(model, images, labels)
        acc = evaluate(model, images, labels)
        avg_loss = avg_loss + loss
        avg_train_acc = avg_train_acc + acc
        train_step += 1
    avg_loss = avg_loss / train_step
    avg_train_acc = avg_train_acc / train_step
    
    for images, labels in test_dataset:        
        acc = evaluate(model, images, labels)        
        avg_test_acc = avg_test_acc + acc
        test_step += 1    
    avg_test_acc = avg_test_acc / test_step    

    print('Epoch:', '{}'.format(epoch + 1), 
          'loss =', '{:.8f}'.format(avg_loss), 
          'train accuracy = ', '{:.4f}'.format(avg_train_acc), 
          'test accuracy = ', '{:.4f}'.format(avg_test_acc))


print('Learning Finished!')
```

    Learning started. It takes sometime.
    Epoch: 1 loss = 0.55930036 train accuracy =  0.8289 test accuracy =  0.8465
    Epoch: 2 loss = 0.37636802 train accuracy =  0.8780 test accuracy =  0.8409
    Epoch: 3 loss = 0.30589393 train accuracy =  0.8886 test accuracy =  0.8463
    Epoch: 4 loss = 0.25915495 train accuracy =  0.8943 test accuracy =  0.8473
    Epoch: 5 loss = 0.22266315 train accuracy =  0.8995 test accuracy =  0.8390
    Epoch: 6 loss = 0.19376862 train accuracy =  0.9031 test accuracy =  0.8362
    Epoch: 7 loss = 0.16824867 train accuracy =  0.9058 test accuracy =  0.8372
    Epoch: 8 loss = 0.14640175 train accuracy =  0.9070 test accuracy =  0.8394
    Epoch: 9 loss = 0.12915796 train accuracy =  0.9098 test accuracy =  0.8494
    Epoch: 10 loss = 0.11194927 train accuracy =  0.9124 test accuracy =  0.8591
    Epoch: 11 loss = 0.09709196 train accuracy =  0.9118 test accuracy =  0.8363
    Epoch: 12 loss = 0.08585018 train accuracy =  0.9159 test accuracy =  0.8342
    Epoch: 13 loss = 0.07739498 train accuracy =  0.9226 test accuracy =  0.8446
    Epoch: 14 loss = 0.06688821 train accuracy =  0.9257 test accuracy =  0.8347
    Epoch: 15 loss = 0.05965757 train accuracy =  0.9284 test accuracy =  0.8510
    Epoch: 16 loss = 0.05637225 train accuracy =  0.9298 test accuracy =  0.8383
    Epoch: 17 loss = 0.04901105 train accuracy =  0.9310 test accuracy =  0.8328
    Epoch: 18 loss = 0.04322139 train accuracy =  0.9336 test accuracy =  0.8380
    Epoch: 19 loss = 0.04027989 train accuracy =  0.9354 test accuracy =  0.8406
    Epoch: 20 loss = 0.03605103 train accuracy =  0.9359 test accuracy =  0.8161
    Epoch: 21 loss = 0.03304508 train accuracy =  0.9403 test accuracy =  0.8475
    Epoch: 22 loss = 0.02983123 train accuracy =  0.9424 test accuracy =  0.8530
    Epoch: 23 loss = 0.02683980 train accuracy =  0.9428 test accuracy =  0.8265
    Epoch: 24 loss = 0.02569373 train accuracy =  0.9469 test accuracy =  0.8700
    Epoch: 25 loss = 0.02279407 train accuracy =  0.9491 test accuracy =  0.8683
    Epoch: 26 loss = 0.02240094 train accuracy =  0.9459 test accuracy =  0.8477
    Epoch: 27 loss = 0.01976435 train accuracy =  0.9492 test accuracy =  0.8684
    Epoch: 28 loss = 0.01969107 train accuracy =  0.9499 test accuracy =  0.8722
    Epoch: 29 loss = 0.01952051 train accuracy =  0.9509 test accuracy =  0.8769
    Epoch: 30 loss = 0.01732454 train accuracy =  0.9570 test accuracy =  0.8722
    Learning Finished!
    

배치놈은 로스값이 위에 모델들과 다르게 빠르게 떨어지는 것을 알 수 있다.(수렴이 빨리되는 성향) 

따라서 배치놈은 가능하면 모델링할때 써주는게 좋다.

### step 18) learning rate decay를 적용한 MLP 모델


```python
learning_rate = 0.001
training_epochs = 30
batch_size = 100
n_class = 10
```


```python
def create_model():
    model = keras.Sequential()
    model.add(keras.layers.Flatten(input_shape=(28,28)))
    model.add(keras.layers.Dense(256, activation='relu'))
    model.add(keras.layers.Dense(256, activation='relu'))    
    model.add(keras.layers.Dense(128, activation='relu'))
    model.add(keras.layers.Dense(128, activation='relu'))    
    model.add(keras.layers.Dense(10, activation='softmax'))
    return model
```


```python
model = create_model()
model.summary()
```

    Model: "sequential_6"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    flatten_6 (Flatten)          (None, 784)               0         
    _________________________________________________________________
    dense_28 (Dense)             (None, 256)               200960    
    _________________________________________________________________
    dense_29 (Dense)             (None, 256)               65792     
    _________________________________________________________________
    dense_30 (Dense)             (None, 128)               32896     
    _________________________________________________________________
    dense_31 (Dense)             (None, 128)               16512     
    _________________________________________________________________
    dense_32 (Dense)             (None, 10)                1290      
    =================================================================
    Total params: 317,450
    Trainable params: 317,450
    Non-trainable params: 0
    _________________________________________________________________
    


```python
@tf.function
def loss_fn(model, images, labels):
    predictions = model(images, training=True)
    loss = tf.reduce_mean(keras.losses.categorical_crossentropy(labels, predictions))   
    return loss  
```


```python
@tf.function
def train(model, images, labels):
    with tf.GradientTape() as tape:
        loss = loss_fn(model, images, labels)
    grads = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(grads, model.trainable_variables))
```


```python
@tf.function
def evaluate(model, images, labels):
    predictions = model(images, training=False)
    correct_prediction = tf.equal(tf.argmax(predictions, 1), tf.argmax(labels, 1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))    
    return accuracy
```


```python
lr_schedule = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=learning_rate,
                                                          decay_steps=n_train//batch_size*10,
                                                          decay_rate=0.5,
                                                          staircase=True)
optimizer = keras.optimizers.Adam(learning_rate=lr_schedule)
```

위에서 learning rate decay는 모델을 건드는게 아니라 옵티마이저에서 adam 쓰고, 
러닝레이트 디케이가 따로 들어간 걸 볼 수 있다.

decay_steps = 얼마마다 한번씩 이 러닝레이트를 떨굴것이냐 step은 웨이팅을
한번 업데이트 하는 것을 말한다. 배치가 한번 들어가서 업데이트 되면 그게 한 스텝이다.

10 애포그가 되면 러닝레이트를 떨구고 싶을때 아래와 같이 코딩해주면 된다.

한 애포크에 몇 스텝이 들어가는지 계산해서 곱하기 10하면 된다.
600(6만(데이터셋) 나누기 100(배치사이즈))번 스텝을 밟으면 한 애포크가 될 것이다. 

따라서 6000 스텝이 되었을때 러닝레이트를 떨굴건데 0.5,절반 만큼 떨구라는 것이다.

staircase는 10 애포크 동안 러닝레이트를 유지하다가 10 애포크가 되면 계단식으로

확 떨구라는 옵션이다. 디폴트는 false다. 

false일 경우를 적용하면 매 스텝마다 절반씩 조금씩 조금씩 떨구는 방식이다.

일반적으로 staircase를 쭉 유지시키다가 한번에 떨굴때 더 성능이 잘나온다고 한다.


모델링을 할때 자주 적용하는 옵션중에 하나이다.



```python
# train my model
print('Learning started. It takes sometime.')
for epoch in range(training_epochs):
    avg_loss = 0.
    avg_train_acc = 0.
    avg_test_acc = 0.
    train_step = 0
    test_step = 0
    
    for images, labels in train_dataset:
        train(model,images, labels)
        loss = loss_fn(model, images, labels)
        acc = evaluate(model, images, labels)
        avg_loss = avg_loss + loss
        avg_train_acc = avg_train_acc + acc
        train_step += 1
    avg_loss = avg_loss / train_step
    avg_train_acc = avg_train_acc / train_step
    
    for images, labels in test_dataset:        
        acc = evaluate(model, images, labels)        
        avg_test_acc = avg_test_acc + acc
        test_step += 1    
    avg_test_acc = avg_test_acc / test_step    

    print('Epoch:', '{}'.format(epoch + 1), 
          'loss =', '{:.8f}'.format(avg_loss), 
          'train accuracy = ', '{:.4f}'.format(avg_train_acc), 
          'test accuracy = ', '{:.4f}'.format(avg_test_acc))


print('Learning Finished!')
```

    Learning started. It takes sometime.
    Epoch: 1 loss = 0.47184047 train accuracy =  0.8326 test accuracy =  0.8515
    Epoch: 2 loss = 0.33553812 train accuracy =  0.8765 test accuracy =  0.8558
    Epoch: 3 loss = 0.29666546 train accuracy =  0.8914 test accuracy =  0.8648
    Epoch: 4 loss = 0.27512318 train accuracy =  0.8989 test accuracy =  0.8584
    Epoch: 5 loss = 0.25638184 train accuracy =  0.9047 test accuracy =  0.8634
    Epoch: 6 loss = 0.24105893 train accuracy =  0.9107 test accuracy =  0.8589
    Epoch: 7 loss = 0.22922282 train accuracy =  0.9142 test accuracy =  0.8706
    Epoch: 8 loss = 0.21588430 train accuracy =  0.9190 test accuracy =  0.8681
    Epoch: 9 loss = 0.20477138 train accuracy =  0.9230 test accuracy =  0.8724
    Epoch: 10 loss = 0.19347958 train accuracy =  0.9275 test accuracy =  0.8756
    Epoch: 11 loss = 0.17839009 train accuracy =  0.9323 test accuracy =  0.8809
    Epoch: 12 loss = 0.16371204 train accuracy =  0.9383 test accuracy =  0.8763
    Epoch: 13 loss = 0.15079328 train accuracy =  0.9438 test accuracy =  0.8807
    Epoch: 14 loss = 0.14167497 train accuracy =  0.9471 test accuracy =  0.8781
    Epoch: 15 loss = 0.13093963 train accuracy =  0.9515 test accuracy =  0.8779
    Epoch: 16 loss = 0.12464634 train accuracy =  0.9545 test accuracy =  0.8797
    Epoch: 17 loss = 0.11639487 train accuracy =  0.9571 test accuracy =  0.8780
    Epoch: 18 loss = 0.10884894 train accuracy =  0.9603 test accuracy =  0.8800
    Epoch: 19 loss = 0.10412849 train accuracy =  0.9621 test accuracy =  0.8849
    Epoch: 20 loss = 0.09618971 train accuracy =  0.9656 test accuracy =  0.8867
    Epoch: 21 loss = 0.09669129 train accuracy =  0.9633 test accuracy =  0.8840
    Epoch: 22 loss = 0.08683250 train accuracy =  0.9681 test accuracy =  0.8846
    Epoch: 23 loss = 0.07859748 train accuracy =  0.9714 test accuracy =  0.8856
    Epoch: 24 loss = 0.07082907 train accuracy =  0.9747 test accuracy =  0.8867
    Epoch: 25 loss = 0.06371624 train accuracy =  0.9784 test accuracy =  0.8858
    Epoch: 26 loss = 0.05881146 train accuracy =  0.9799 test accuracy =  0.8866
    Epoch: 27 loss = 0.05404144 train accuracy =  0.9818 test accuracy =  0.8883
    Epoch: 28 loss = 0.05191405 train accuracy =  0.9827 test accuracy =  0.8866
    Epoch: 29 loss = 0.04875532 train accuracy =  0.9841 test accuracy =  0.8863
    Epoch: 30 loss = 0.04594989 train accuracy =  0.9855 test accuracy =  0.8870
    Learning Finished!
    

이렇게 쉬운 데이터셋 문제로 모델링하면 트레이닝 퍼포먼스가 100가까이 나오는 것은 흔하다.

만약에 트레이닝 퍼포먼스가 100프로 에 가까운데 테스트 퍼포먼스가 70프로 이하로 나온다 하면 오버피팅을 의심해야한다.

위의 스텝들을 진행해보면서 여러가지 옵션들을 조합하여 MLP의 성능을 최대한 끌어올려 보았다. 이런 조합들을 적당히 잘 섞어서 튜닝하면 조금 더 올릴 수도 있을것이다. 

그런데 테스트 퍼포먼스는 99퍼가 나오기는 힘들것이다.

근데 여기서 CNN을 쓴다!? 그러면 99퍼에 가까운 성능을 낼 수 있는 모델을 구현할 수 있다.
