---
layout: post
title: "Neural Network 작동원리 이해"
tags: [Data Engineering]
comments: true
---

Data_Engineering_TIL_(20190504)


#### [학습목표]

- Neural Network 작동원리 이해


#### [학습기록]


- Neural Network의 Cost Function

![1](https://user-images.githubusercontent.com/41605276/57176777-39c77200-6e97-11e9-9e6d-e2a9db094a8b.jpg)

로지스틱의 결과가 다른 로지스틱의 인풋으로 들어가고 또 로지스틱의 결과가 다른로직의 인풋으로 들어가는 이런방식이다. 인풋으로 들어가면 ax+b 형식의 리니어한 형태로 또 들어가게 된다. 

그래서 i라는 레이어의 1,2,3층 ... 이런식으로 구성되는 구조이다. 

i 코스트가 1층에서 코스트가 나오고, weight만큼 조합되서 다음단의 오차가 누적되서 쌓이게 된다.

리마인딩을 하자면 비용함수를 구하는 이유는 비용함수를 구해서 미분한 다음에 그 미분한 값을 그레디언트 디센트 방식으로  weight를 조정하는 이런 과정을 반복하면 모델이 학습이 되기 때문이다.

뉴럴네트워크에서도 각 노드를 이어주는 선의 굵기가 가중치에 해당되는데 이 가중치를 학습하기 위해서 이 전체 네트워크의 코스트를 잘 정의하고 잘 정의한 코스트를 미분해서 그 코스트를 줄일 수 있는 방향으로 가중치들을 그레디언트 디센트 방법을 통해서 조정을 하는 것이다.

Forward propagation은 입력이 있을때 로지스틱 리그레션에서는 세타 = h(x)를 구현했는데 이거를 뉴럴넷에서 구현해서 계산하는 과정이라고 할 수 있다. 아무래도 여러개층이기 때문에 계산이 로지스틱 리그레션 보다는 당연히 복잡하다. a랑 z 두가지가 나오는데 a는 어떤 레이어의 입력이고, g()가 시그모이드 함수이다. 수식으로 표현하면 아래 그림과 같다.

![3](https://user-images.githubusercontent.com/41605276/57176783-4a77e800-6e97-11e9-94e3-5cd49fc097e7.jpg)

Back propagation는 비용함수의 미분값을 어떻게 계산해야 하느냐에 대한 것이다. 어떤 노드에서 에러가 얼마나 발생하는지 구하는 것이다. 수식으로 표현하면 아래 그림과 같다.

![2](https://user-images.githubusercontent.com/41605276/57176789-506dc900-6e97-11e9-90bf-8be1c7849b20.jpg)

- Forward propagation과 Back propagation 작동의 간단한 예시

![4](https://user-images.githubusercontent.com/41605276/57176791-582d6d80-6e97-11e9-912f-d18c45096913.jpg)

위와 같은 네트워크가 있을때 forward pass와 backward pass가 어떻게 되는지 알 수 있다. 먼저 foraward pass는 아래 그림과 같이 계산이 된다고 보면 된다.

![5](https://user-images.githubusercontent.com/41605276/57176796-6085a880-6e97-11e9-826b-be440936de45.jpg)

반면에 backward pass의 값들은 편미분값들로 구성되어 있다. forward pass에서는 실제값이랑은 상관없이 네트워크 구조가 확정이 되는데 f에서 1이라는 미분값이 추적되는 것은 현재값이 어떤거냐에 따라 backward pass의 영향력이 결정된다.

![6](https://user-images.githubusercontent.com/41605276/57176799-667b8980-6e97-11e9-8ca8-3d4832725528.jpg)

f에 대해서 z가 얼마나 f의 오차값에 기여하고 있느냐하는 것은 z랑 연결된 노드의 현재값이 그대로 적용된다. 마찬가지로 f를 q에 대해서 미분하는 것도 -4q기 때문에 -4이고, 이는 z의 현재값이 그대로 들어가는 것이다. 이런식으로 곱의 형태로 네트워크가 연결되는 경우에는 곱해지는 상대방의 현재 value를 미분값으로 쓰게된다.

그러면 네트워크 앞쪽에 더하기 부분도 back propogation을 해보자.

![7](https://user-images.githubusercontent.com/41605276/57176800-6c716a80-6e97-11e9-8c66-434433ec2108.jpg)

q를 x와 y에 대해 미분을 하고 싶은건데 여기에서 -4의 오차는 x와 y에서 얼마만큼 온건지 알아보고 싶은 것이다. -4의 오차가 x랑 y에서 얼마만큼 씩 온거야 알아보고 싶다. -4의 오차가 x랑 y에서 얼마만큼씩 오는지 편미분을 해보면 더하기 형태이기 때문에 x로 미분하게 되면 dq / dx = 1 이 되고 마찬가지로 y로 미분하면 dq / dy = 1 이 된다. 이는 x랑 y가 각각 1만큼씩 오차를 냈다고 할 수 있다. 이렇게 편미분 값을 구하면 다음단에는 고대로 그 전단계의 -4와 1이 곱해져서 전달되게 된다.

정리하면 cost function을 잘 미분하면 거기에서 오차가 얼마만큼 냈냐가 cost function의 미분값으로 들어가게 되고 그걸가지고 바로 그레디언트 디센트 알고리즘을 통해 세타를 업데이트하면 되는데 마지막 단계에서 알 수 있는 오차값이 각각의 레이어에서 얼마만큼 기여하고 얼마만큼 적용해야 하는지 역계산을해서 세타를 얼마만큼 업데이트 할지 알아내는 과정이 back propogation이다.

실제로는 아래 그림과 같은 방식으로 하는 편이다. 마지막 단에서 1만큼 오차가 났다고 하면 오차가 앞에서 얼마나 났는지 역계산을 하는 것인데 back propogation을 역계산해서 맨 앞단의 세타를 얼마만큼 업데이트 해야하는지를 나타내준다. 

![8](https://user-images.githubusercontent.com/41605276/57176801-74310f00-6e97-11e9-9f3c-f871e85d51f0.jpg)

- Neural Network 구현과정 요약정리

1) 네트워크 구조를 정한다.

인풋 피쳐의 갯수, 아웃 클래스의 갯수, 히든 레이어의 갯수 및 차원


2) 가중치를 랜덤하게 초기화


3) forward propagation, cost function, back propagation 을 구현


4) 모든 데이터들에 대하여, back propagation을 통해 구한 gradient 들을 모델에 적용, weight 값을 조정


5) 이 작업을 loss가 충분히 줄어들때까지 반복


- activation function으로 시그모이드 함수을 쓰게 되면 학습속도가 하이퍼탄젠트 함수보다 매우 느리다. 왜냐하면 백프로포게이션에서 뒷단에서 난 애러를 앞단으로 전파시켜야 하는데 activation function의 미분값이 매 레이어마다 곱해져야 한다. 시그모이드는 수식 모양상 미분값이 거의 0에 가까운 숫자가 나오기 때문에 곱해져도 매우 미미한 영향을 주게 된다. 그만큼 학습속도가 느려지게 된다는 것이다. 그래서 딥러닝에서는 시그모이드 함수가 활성화함수로 어울리지는 않는다. 그래서 보통은 시그모이드 함수가 아니라 다른류의 함수를 많이 쓴다.
