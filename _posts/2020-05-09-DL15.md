---
layout: post
title: "ResNeXt 기초개념"
tags: [딥러닝]
comments: true
---

.
Deep_Learning_TIL(20190727)

study program : https://www.fastcampus.co.kr/data_camp_deeplearning

[학습노트]

- ResNext는 2016년에 2등한 네트워크이다. 2016년에 1등한 네트워크는 별 특징이 없고 그동안에 나왔던 네트워크들을 앙상블을 잘했던 모델이다. ResNet을 만든 사람들이 그 다음 버전의 ResNet을 만든것이라고 보면 된다. 

![image](https://user-images.githubusercontent.com/41605276/81462139-4dfd2800-91eb-11ea-85fd-aa4d2a79303f.png)

- 위에 그림에서 보면 알 수 있듯이 VGGNet이 똑같은 형태의 빌딩블락을 스텍해서 만들었다. 그리고 레즈넷도 유사한 형태로 따라서 했다. 레즈넷도 컨볼루션을 바틀넷레이어 거치고 다시 컨볼루션하고 다시 바틀넥을 줄이고 3x3 컨볼루션하고, 1x1으로 다시확장해서 연결하는 형태가 하나의 블록이었다. 이거를 계속 쌓아서 만들었다. 그런데 VGG와 다른점은 split-transform-merge 전략을 쓰고 있고 이게 좋은 특징이라고 주장한다. split을 한다는 것은 브랜치를 나누는 것이고, transform은 거기서 각각의 브랜치마다 얻은 연산을 transform하는 것이고, 머지는 결과를 concatenation한다는 것이다. 

![image](https://user-images.githubusercontent.com/41605276/81462662-17c1a780-91ef-11ea-8988-63e1b91b70ea.png)

- 레즈넷이 나오고 나서 인셉션 레즈넷이라는 논문이 나왔는데 레즈넷도 인셉션을 보고 뭔가 교훈을 얻은게 있을것이다. split-transform-merge 이 전략을 배워보자는 것이었다. 그래서 위의 그림에서 오른쪽에 있는 구조가 ResNeXt의 기본 블락의 형태이다. 그런데 인셉션은 상당히 아트영역이기 때문에 특이한 연산을 하는 케이스가 다양한데 이렇게 하지말고 브랜치를 나누는 것은 좋은데 이거를 단순하게 각각의 브랜치마다 똑같은 연산을 하자는 것이다. 256개의 채널이 있고, 이 d가 depth다. 입력이 들어오면 이거를 1x1 컨볼루션을 하는데 1x1 필터 4개의 결과를 따로 다음단계로 보내는 것이다. 1x1 컨볼루션 필터가 4x32=128개라고 할 수 있다. 256개의 채널이 들어왔을때 채널이 128개가 되는데 이거를 32등분을 하는 것이다. 그 다음에 4개 채널만 갖고 3x3 컨볼루션을해서 그 다음에 다시 256개로 키운다. 마치 이거 하나가 레즈넷의 하나의 모듈처럼 동작을 하는 것이다. 레즈넷에서도 처음에는 바틀넥레이어를 써서 채널수를 줄이고 그다음에 그거를 유지하는 연산을 하고 그 다음에 다시 키워서 브랜치를 32개로 나누어서 거기마다 이런 연산을 똑같이 한다음에 256개의 출력을 마지막에 다 합치게 된다. 채널수는 그러면 앨리먼트와이즈 덧셈이기 때문에 그냥 똑같이 채널이 256일 것이다. 그 다음에 256d-in에 안들어갔던 얘를 가져와서 더해서 출력을 내는것이다. 

![image](https://user-images.githubusercontent.com/41605276/81463137-28275180-91f2-11ea-81e0-29a0c49f8f32.png)

- 컨볼루션을 할때는 일반적으로 모든채널을 전부연산을 한다. 그래서 컨볼루션 필터 1개사이즈는 가로세로가 3x3이라면 컨볼루션 필터 1개의 채널수는 input feature map의 채널수와 같다. 왜냐하면 연산을 모든채널에서 다 해야하기 때문이다. [256, 1x1 4]에서 다음으로 갈때 1x1 컨볼루션 필터를 128개를 쓰면 가로세로는 그대로 유지하고 채널만 128로 줄었을텐데 이거를 가지고 ResNet은 3x3 필터를 128개를 써서 똑같이 만든다음에 1x1 컨볼루션을해서 256개로 만든다음에 256d-in에 안들어갔던 얘와 더했다. 단, 줄이는 걸 1/2로 줄이지 않고, 1/4 정도로 줄였다. 


- ResNext는 3x3 컨볼루션 필터 1개의 사이즈는 3x3x128이므로 input featuremap의 채널이 128이기 때문에 3x3 필터의 사이즈는 똑같이 128이어야 하는데 그렇게 안하고 ResNeXt는 32도막을 내서 그러면 하나하나가 채널이 4씩 나누어질 것이다. 첫번째꺼를 가져와서 4x4 컨볼루션 필터를 채널 4짜리를 써서 그거를 4개로 하면 똑같은 형태로 나올것이다. 왜냐하면 아웃풋채널은 컨볼루션 필터의 갯수에 의해 결정되기 때문이다. 컨볼루션 한번하면 점수표가 1장 생기니까 이거를 4번하면 4장 생길것이다. 따라서 원래 컨볼루션 연산은 3x3에 채널이 128이어서 모든 채널을 연산을 다해서 그거를 다 더해서 숫자 하나를 만드는데 그룹컨볼루션을 하겠다는 것이다. 그룹컨볼루션은 채널 전체를 다 안보고 그거를 채널을 몇개의 그룹으로 나눈다음에 한그룹에 대해서만 연산을하고 다른 컨볼루션 필터가 두번째 블록을 가져와서 얘도 3x3에 4개의 필터를 써서 연산한다. 4씩 가져왔다는게 128를 만든걸 결과를(레이어 하나를) 32도막으로 나누어준다. 그 다음에 각각의 도막에 대해서 3x3 컨볼루션을 해서 채널을 똑같이 4로 만들고 그거를 다시 256으로 키운 다음에 그거를 다 더한다.


- 위에 중간에 그림에 equivalent박스 안에 3개의 그림은 모두 같은 네트워크이다. 그래서 ResNet과 RexNeXt의 차이점은 3x3 연산을 할때 채널 전체에 대해서 다 하느냐 아니면 도막을내서 부분부분 연산을 한다음에 그 결과를 다시 합치냐 차이이다.

![image](https://user-images.githubusercontent.com/41605276/81464010-3dec4500-91f9-11ea-9dd2-707ef4fffe48.png)


- 위에 그림은 그려면 그룹을 몇개로 나누고 채널을 하는것이 좋은지를 나타내는 것이다. 원래 오리지널 ResNet은 1/4로 줄인다. 256에서 64로 가고, 그 다음에 64에서 64로 3x3 컨볼루션하고 다시 256으로 키운다. 1/4로 줄였다가 4배로 다시 늘리는 방식이다. 이거랑 연산량을 똑같이 맞추기 위해서 계산을 해보니까 d를 몇으로 줄이느냐 그리고 C가 몇개의 그룹으로 나누는거냐를 나타내는데 C를 32로 하면 각각 그룹마다 4개의 채널을 가는게 연산량이 거의 비슷해진다는 것이다. 


![image](https://user-images.githubusercontent.com/41605276/81464226-9cfe8980-91fa-11ea-8459-16066b97c1d5.png)


- 성능을 보면 ResNet에비해 나아진 것을 확인할 수 있다. 위에 그림에서 setting에 ResNet 1x64d는 그룹수가 하나인 것이고(256으로 들어와서 64로 줄인다는 의미), ResNeXt의 32x4d는 256이 들어왔을때 32개의 그룹으로 나눈 것이고, 그룹별로 채널 4개로 만든것이다.


- 아래 그림에서 보면 레즈넷200정도 되면 레즈넷 101에 비해서 2배정도 연산량이 증가한다. 그리고 ResNext101에서 그룹을 64개로 나누고 채널을 4개로한 모델과 그룹을 2개로하고 채널을 64개로한 모델이 연산량이 비슷하다. 보통 그룹을 더 많이 나눈것이 성능이 잘나오는 편이다.


- 그리고 ResNet은 변종들이 많이 만들어진다. 그중에 위에 그림의 표에 있듯이 wider ResNet이라는게 있는데 레이어수를 줄이고, 대신에 채널수를 더 늘리자는 컨셉이다. 그래서 레이어를 많이 쌓는다고 무조건 다 좋다는 것은 아니라는 것을 보여줬다. 모델의 크기를 키우는데 레이어만 늘리거나 채널만 늘리거나 이런것 보다는 채널과 레이어 모두를 골고루 늘리는게 성능이 잘 나온다는 것을 시사해주고 있다. 구글넷은 다른 모델에 비해 레이어만 확 늘린것이고, 이런 모델보다는 채널과 레이어 모두 골고루 늘린 모델이 성능이 더 잘 나온다.
