---
layout: post
title: "AWS Lambda를 이용한 EMR spark batch job 자동화"
tags: [Data Engineering]
comments: true
---

.
#### 1. 참고자료

1) https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/emr.html#EMR.Client.run_job_flow

2) https://lamanus.kr/57

#### 2. Lambda function 구현하기

step 1) Lambda 코드 작성 전에 '환경변수' -> '편집' 클릭

아래와 같이 내 계정의 access key와 secret access key 변수를 설정해준다.

![2](https://user-images.githubusercontent.com/41605276/74790278-79622c00-52fa-11ea-8c75-aab89ad9ff0c.png)

step 2) 람다함수 구현


```python
import os
import boto3

aws_key = os.environ['AWS_KEY']
aws_skey = os.environ['AWS_SKEY']

def lambda_handler(event, context):
    session = boto3.session.Session(region_name='us-west-2') 
    emr_client = session.client('emr', aws_access_key_id = aws_key, aws_secret_access_key = aws_skey)
    
    response = emr_client.run_job_flow(
        Name='pms-emr-test', 
        LogUri='s3://lhw-s3-test/log-folder/',
        ReleaseLabel='emr-5.28.0',
        Instances={
            'InstanceGroups': [
                {
                    'Name': 'master',
                    'Market': 'SPOT',
                    'BidPrice': '0.76',
                    'InstanceRole': 'MASTER',
                    'InstanceType': 'm5.xlarge',
                    'InstanceCount': 1
                },
                {
                    'Name': 'slave',
                    'Market': 'SPOT',
                    'BidPrice': '0.76',
                    'InstanceRole': 'CORE',
                    'InstanceType': 'm5.xlarge',
                    'InstanceCount': 4
                }],
            'Ec2KeyName': 'pms_oregon_key',
            'KeepJobFlowAliveWhenNoSteps': False,
            'TerminationProtected': False,
            'Ec2SubnetId': 'subnet-03ab1e1a1ea3165e5',
            'EmrManagedMasterSecurityGroup': 'sg-017a3f873088b6641',
            'EmrManagedSlaveSecurityGroup': 'sg-0ebcd86afff99eece',
        },
        Applications=[{
            'Name': 'Spark'
        }],
        VisibleToAllUsers=False,
        JobFlowRole='EMR_EC2_DefaultRole',
        ServiceRole='EMR_DefaultRole',
        Steps=[{
            'Name': 'Main',
            'ActionOnFailure': 'TERMINATE_CLUSTER',
            'HadoopJarStep': {
                'Jar': 'command-runner.jar',
                'Args': ['spark-submit',
                    '--master', 'yarn', '--deploy-mode', 'client',
                    '--class', 'main class',
                    's3://lhw-s3-test/emrtest.py'
                ]
            }
        }],
    )
    return response
```

#### 3. 로컬 피시(리눅스 환경)에서 boto3를 이용한 EMR spark job 자동화 구현하기

step 1) 로컬피시에서 환경변수 액세스키와 시큐리티 액세스키를 아래 그림과 같이 설정하여 잡아주기

![1](https://user-images.githubusercontent.com/41605276/74709941-bf1fe580-5263-11ea-8062-086c3cdb3d83.jpg)

step 2) python을 이용한 EMR 구동 및 spark job 부여 코드


```python
import os
import boto3

# aws_key = os.environ['AWS_KEY']
# aws_skey = os.environ['AWS_SKEY']
# 보안상 위와같이 환경변수를 지정해서 사용해야 함.

aws_key ='************'
aws_skey ='***********'

## 람다에서 쓰일경우(람다핸들러) = def lambda_handler(event, context): 로 바꿔서 쓰면됨
def EMR_handler():
    session = boto3.session.Session(region_name='us-west-2') 
    emr_client = session.client('emr', aws_access_key_id = aws_key, aws_secret_access_key = aws_skey)
    
    response = emr_client.run_job_flow(
        Name='pms-emr-test', 
        LogUri='s3://lhw-s3-test/log-folder/',
        ReleaseLabel='emr-5.28.0',
        Instances={
            'InstanceGroups': [
                {
                    'Name': 'master',
                    'Market': 'SPOT',
                    'BidPrice': '0.77', # 단위는 $
                    'InstanceRole': 'MASTER',
                    'InstanceType': 'm5.xlarge',
                    'InstanceCount': 1
                },
                {
                    'Name': 'slave',
                    'Market': 'SPOT',
                    'BidPrice': '0.77', # 단위는 $
                    'InstanceRole': 'CORE',
                    'InstanceType': 'm5.xlarge',
                    'InstanceCount': 4
                }],
            'Ec2KeyName': 'pms_oregon_key',
            'KeepJobFlowAliveWhenNoSteps': False,
            'TerminationProtected': False,
            'Ec2SubnetId': 'subnet-03ab1e1a1ea3165e5',
            'EmrManagedMasterSecurityGroup': 'sg-017a3f873088b6641',
            'EmrManagedSlaveSecurityGroup': 'sg-0ebcd86afff99eece',
        },
        Applications=[{
            'Name': 'Spark'
        }],
        VisibleToAllUsers=False,
        JobFlowRole='EMR_EC2_DefaultRole',
        ServiceRole='EMR_DefaultRole',
        Steps=[{
            'Name': 'Main',
            'ActionOnFailure': 'TERMINATE_CLUSTER',
            'HadoopJarStep': {
                ## command-runner.jar은 기본적으로 내장된 파일임. 이걸 통해서 spark-submit 단계를 추가할 수 있게 된다.
                'Jar': 'command-runner.jar',
                'Args': ['spark-submit',
                    '--master', 'yarn', '--deploy-mode', 'client',
                    '--class', 'main class',
                    's3://lhw-s3-test/emrtest.py'
                ]
            }
        }],
    )
    return response

EMR_handler()
```




    {'JobFlowId': 'j-2EC2R6443AERO',
     'ClusterArn': 'arn:aws:elasticmapreduce:us-west-2:274351873145:cluster/j-2EC2R6443AERO',
     'ResponseMetadata': {'RequestId': 'd6fa7705-09f5-4e4b-8e34-da5767570c27',
      'HTTPStatusCode': 200,
      'HTTPHeaders': {'x-amzn-requestid': 'd6fa7705-09f5-4e4b-8e34-da5767570c27',
       'content-type': 'application/x-amz-json-1.1',
       'content-length': '118',
       'date': 'Tue, 18 Feb 2020 06:19:48 GMT'},
      'RetryAttempts': 0}}



#### 3. emrtest.py 코드내용


```python
import sys
import pyspark
from pyspark.sql import SparkSession
from pyspark.context import SparkContext
from pyspark.sql.functions import *
from pyspark.sql.types import *

spark = SparkSession.builder.getOrCreate()

df_csv = spark.read.format("csv").option("header", "true").load("s3a://lhw-s3-test/source_temp/*.csv")

udf_year = udf(lambda record:record[0:4],StringType())
udf_month = udf(lambda record:record[5:7],StringType())
udf_day = udf(lambda record:record[8:10],StringType())

new_df_csv = df_csv.withColumn('year',udf_year('event_time').cast(IntegerType()))
new_df_csv = new_df_csv.withColumn('month',udf_month('event_time').cast(IntegerType()))
new_df_csv = new_df_csv.withColumn('day',udf_day('event_time').cast(IntegerType()))

new_df_csv.write.partitionBy("year","month","day").save("s3a://lhw-s3-test/destination_temp/", format='parquet', header=True)
```
