---
layout: post
title: "Squeeze and Excitation Networks 기초개념"
tags: [딥러닝]
comments: true
---

.

Deep_Learning_TIL(20210228)

study program : https://www.fastcampus.co.kr/data_camp_deeplearning

- 패스트캠퍼스 "TensorFlow로 시작하는 딥러닝 입문 CAMP"를 공부하고 정리한 내용입니다.

** 참고자료 : https://github.com/jwlee-ml/TensorFlow_Training_13th

[학습내용]

- 2017년 마지막 컴퓨터비전 대회에서 1등했던 CNN 모델이고 중국유저들이 만들었음

![1](https://user-images.githubusercontent.com/41605276/109414196-f7b8da00-79f4-11eb-8a4d-1c505cf98322.png)

- 이 모델도 특이한 구조를 갖고 있다. Squeeze and Excitation Blocks이라는 것을 갖고 있다.

![2](https://user-images.githubusercontent.com/41605276/109414332-b2e17300-79f5-11eb-9fc2-6363991ab21b.PNG)

위에 그림에서 별도로 거쳐가는 branch가 없으면 일반적인 CNN과 동일하다. 이 별도로 거쳐가는 branch가 Squeeze and Excitation Block이다.

- Squeeze and Excitation Networks 가 나오게된 계기 : Standard Convolution의 문제

In standard convolution case, the output is produced by a summation through all channels

Channel dependencies are implicitly embedded in output feature maps

But, they are entangled with the local spatial correlation captured by filters

feature 맵(위 그림에서 C')이 나왔을때 채널마다 중요도가 전부 같다고 할 수 없다. 실제로 피쳐맵을 출력해보면 아무것도 없는 것도 있고, 데드렐루에 빠진것들도 있다. 그래서 피쳐맵이 완전 까맣게 보이는 경우도 있다. 따라서 상황별로 피쳐맵의 중요도가 다를 것이다. 또는 어떤 입력이 들어오냐에 따라 피쳐맵의 중요도가 다를 것이다. 그래서 피쳐맵의 중요도에 따라 가중치를 주고 싶은데 컨볼루션 연산을 3x3 컨볼루션을 한다고 그러면 채널을 전체 다보게 된다. 이 경우에도 필터 사이즈도 3 x 3 x 256 이니까 채널마다 3 x 3의 rate를 가지고 있을것이다. 사실 이걸로 중요도를 조절할 수 있는 것이다. 만약에 첫번째 채널이 중요하다고 하면 컨볼루션 필터 3 x 3 x 256 중에 필터의 첫번째 채널에 있는 얘들의 숫자가 커지게 하면 된다. 그런데 가로 세로를 동시에 보니까 3 x 3의 특징을 뽑아 내야하기 때문에 그걸로 인해서 커지게 하고 싶어도 못하고, 작게 하고 싶어도 그거를 컨트롤 하지 못하는 경우가 생긴다(3x3의 특징을 뽑아 낸다는 말은 위에 마지막 문장에 spatial correlation captured by filters라는 얘기다). 그래서 피쳐맵마다의 중요한 정도를 뽑아내기는 힘들다. 그래서 이 작업을 따로 해주겠다는 것이 이 Squeeze and Excitation Networks 의 아이디어다.

- Squeeze : Global Information Embedding

Authors propose to squeeze global spatial information into a channel descriptor.

This is achieved by using global average pooling to generate channel wise statistics.

The output of the transformation(GAP) can be interpreted as a collection of the local descriptors whose statistics are expressive for whole image

![3](https://user-images.githubusercontent.com/41605276/109414821-69465780-79f8-11eb-87b9-0a5d195ca5b2.PNG)

그래서 위에 그림에서 색갈칠한게 중요도라고 생각하면 된다. 빨간색으로 갈 수록 중요하다고 보면 된다. 중요도를 계산해서 1 x 1 x C , 즉 C개라는 채널숫자만큼 숫자가 있는 것이다. 다음레이어로 갈때 이 채널들간에 어떤게 더 중요하고 덜 중요한지 별도의 네트워크로 따로 계산해서 그걸 적용한 다음에 다음레이어로 보내주겠다 라는게 이 네트워크의 중요한 특징이다.

그러면 그 중요도를 어떻게 계산할거냐 위에 그림에서 빨간색 박스 부분 W x H x C가 1 x 1 x C로 가는거는 global average pooling에 의해 이루어진다. 그러면 C가 계산이 된다.

- Excitation : Adaptive Recalibration

![4](https://user-images.githubusercontent.com/41605276/109415030-a65f1980-79f9-11eb-80a5-1292c3725ced.PNG)

그 다음에 위에 그림 빨간색 부분은 fully connected layer이다. fully connected layer를 두개를 쌓은 것이다. 앞에 단계는 Squeeze 연산이 된 것이고, fully connected layer 간에는 Excitation 하는 것이다. 다시말해서 Squeeze 하는 것은 압축하는게 그냥 global average pooling을 하고, 위에 빨간색 박스부분은 fully connected layer를 두개를 쓰는 것이다.

![5](https://user-images.githubusercontent.com/41605276/109415188-8d0a9d00-79fa-11eb-9674-8bf1a69afb87.PNG)

fully connected layer를 두개중 첫번째 레이어에는 z와 W를 곱한게 fully connected layer를 표현한것이고 그런다음에 델타를 통과시키는데 델타는 렐루다. 이런식으로 pooling한 다음에 두번째 레이어에서는 시그모이드를 쓰는 것이다. 소프트맥스를 안쓰고 시그모이드를 썼는데 이것은 뭐를 의미하냐면 모든채널이 다 중요할 수도 있고, 다 안중요할 수도 있다는 것을 염두한 것이다. 소프트맥스를 쓴다는 것은 전부 더해서 1이 되기 위해서 어떤것은 커지고 어떤것은 작아지게 하는 것인데 이렇게 안하고, 각각에 대해 시그모이드를 써서 각각에 대해 0과 1 사이 값으로 만들어서 이 수를 곱해서 다음 레이어로 보내는 것이다.

이 방법은 사실 자연어처리에서 어디가 더 중요한지 보는 attention 개념으로 이 attention 개념을 CNN과 접목한 모델이라고 이해하면 된다. 자연어처리에서도 attention을 할때 소프트맥스를 쓰기도하고 시그모이드를 쓰기도 한다. 둘다 쓰는데 자연어처리에서는 소프트맥스를 좀더 많이 쓰는 편이다. 예를들어서 '나는 학교에 간다'라는 문장을 영어로 해석한다 그러면 I가 나오고 그 다음에 뭐가 나올것이냐에 대해서 전체 문장중에 어디를 볼건지, 즉 어디를 중요하게 볼건지를 연산하는데 소프트맥스를 쓰는 것이다.

The final output of the block is obtained by rescaling the transformation output with the activations channel wise multiplication

![6](https://user-images.githubusercontent.com/41605276/109415487-5b92d100-79fc-11eb-990c-a2cbb3466f2d.PNG)

그래서 channel wise multiplication 즉, 위에 그림과 같이 예를들어서 빨간색 숫자하나를 마지막 채널에 전부다 곱하게 된다. 그리고 노란색숫자는 맨처음 채널에 전부다 곱하게 된다.

- Instantiation

![7](https://user-images.githubusercontent.com/41605276/109415601-f55a7e00-79fc-11eb-86f1-3313ef44bb7d.PNG)

이런식으로 했을때 장점은 어떤 네트워크에 다 갖다 붙일 수 있다. inception 모듈이 있으면 inception 모듈에서 나와서 다음 inception 모듈에 들어가기전에 Squeeze and Excitation Blocks을 한번해서 갈 수 있다. 이렇게 한게 SE-Inception module이다. (위에 그림에서 좌측화면)

레즈넷에서도 마찬가지로 Squeeze and Excitation Blocks을 한번하고, scale한거를 더하면 된다. 이렇게 한게 SE-ResNet module(위에 그림에서 우측화면)

- Example Architecture

![8](https://user-images.githubusercontent.com/41605276/109415761-f5a74900-79fd-11eb-8090-567f1c124a02.png)

웬만한 어떤 CNN에 다 끼워 넣을 수 있다는 것이 장점이다. 그리고 성능도 기존거보다 잘나오더라 라는게 논문의 주장이다.

- Results

![9](https://user-images.githubusercontent.com/41605276/109415777-0d7ecd00-79fe-11eb-8aa7-759442b9f947.PNG)