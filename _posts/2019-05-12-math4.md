---
layout: post
title: "Gradient Descent 기초개념"
tags: [수학기초]
comments: true
---

.

#### # 학습시 참고한 URL : https://www.fastcampus.co.kr/data_online_databasic

#### 1. 개요

그레디언트라는 말은 벡터의 미분 쉽게 말해서 그냥 미분이라고 생각하면 된다. 

미분을 이용해서 무언가 아래로 내려가는 그런 것을 말한다.

#### 2. weight 초기화

원하는 weight는 어떻게 찾을까

인풋데이터를 넣고 어떤 숫자를 곱해줬을때 결과값을 잘 예측할 수 있을것인가에 대한 고민이 weight를 어떻게 잘 줄 것인가이다.

그래서 학습이라는 것이 입력값과 곱해지는 가중치를 좋은 가중치를 찾아내는 과정이라고 할 수 있다.

그래서 weight를 어떻게 학습할 것인가 고민해야하는데 처음에 그러면 weight를 어떻게 부여할 것인가에 대한 고민에 직면할 것이다.

그러나 우리는 알고있는 정보가 별로 없기 때문에 그냥 랜덤값을 넣고 시작하자는 것이 random initialization이다.

딥러닝에서 가중치의 초기값을 부여할때 통상적으로 쓰는 방법이다.

![1](https://user-images.githubusercontent.com/41605276/57580107-4fb0e480-74e0-11e9-99b6-7dc95fa32f46.png)

#### 3. loss function

그래서 학습이 끝나고 결과값을 예측했는데 어떤 weight를 줬을때 결과값 예측을 잘 할 것인가에 대한 척도를 알수있는 수치가 loss function 또는 cost function이다.

loss function에서 통상적으로 쓰는 방법이 MSE이다. 

대략적으로는 (신경망의 예측값과 실제값의 차이)의 제곱이다.

EX) 어떤 제조업에서 상품을 만들기위해 들어가는 2가지 재료비용 조합에 대하여 신경망이 예측한 판매량이 (100,800)이고 실제 펀매량이 (105,78)일때 loss function = (105-100)제곱 + (78-80)제곱 = 29

loss function은 그래서 내가 원하는 목적을 잘 예측할수록 loss function 수치가 작아지게 된다. 반대로 잘 예측을 못할수록 커지게된다.

그래서 loss function의 값이 줄어들도록 weight 값들을 조금씩 바꿔줘야한다.

그러면 weight를 어떻게 바꿔줄것인가에 대한 의문이 드는데 미분을 이용하면 된다.

#### 4. 미분

아래 그림에서 x축은 가중치에 대한것이고 y축이 loss function에 대한 것이다.

현재 가중치가 w0에 있다고 치자. 만약에 w0의 현재 loss function이 10이라고 할때 나는 이 loss function을 줄이고 싶다.

그러면 w0을 줄여야지 감소하는지, 아니면 증가시켜야지 loss function이 감소하는지 알기위해서 현재 w0에서 미분을 한다.

미분을 한다는것은 접선의 기울기를 본다는 것인데 만약에 w0를 미분했을때 접선의 기울기가 -2라고하면 그게 무슨의미냐면 w0을 마이너스 방향으로가면 loss function이 증가할것이라는 말이다. 그리고 숫자 2는 2배만큼 loss function이 변할것이라는 말이다. 

즉 왼쪽으로 조금 움직이면 그 2배만큼 loss function이 증가한다는 뜻이다.

우리는 목적이 loss function을 줄이는 것이 목적이기 때문에 마이너스 방향으로 가면 loss function이 증가하겠다는 것을 인지하고 w0을 플러스 방향으로 움직여줘야 한다.

다시말해서 loss를 w로 미분하고 미분값이 가리키는 방향의 반대방향으로 아주 조금씩 w를 바꿔나가면 loss를 감소시킬 수 있다.

주의해야할점은 아주 조금씩 w를 바꿔야 한다는 것이다. 만약에 w를 지나치게 증가시키면 아래 그림에서도 볼 수 있듯이 오히려 loss function이 증가하는 부분쪽으로 갈 수도 있다.

Gredient Descent를 예로 들면 마치 등산객이 눈을 가리고 정상에서 산을 내려가야 하는 상황과 비슷하다. 그 등산객은 앞은 볼수 없고, 단지 내가 서있는 자리의 경사만 알 수 있다. 그러면 등산객은 경사가 내려가는 쪽으로 조금씩 조금씩 움직여서 결국에는 가장 아래로 내려갈 것이다.

![2](https://user-images.githubusercontent.com/41605276/57580112-56d7f280-74e0-11e9-8e79-1a4fcc116e91.png)

- gradient descent의 수식

![3](https://user-images.githubusercontent.com/41605276/57580114-5d666a00-74e0-11e9-8653-fd0afaf31136.png)

위에 weight update 수식에서 learning rate 앞에 마이너스가 붙은 이유는 미분값(gradient)이 방향을 가르키는데 그 방향이 loss를 증가시키는 방향을 가키리기 때문에 그 반대로 가야하니까 마이너슬 붙어준것이다. learning rate는 이 수치를 키우면 한번에 많이 이동하게되고 수치를 적게해주면 한번에 조금씩 이동한다. 그래서 통상적으로 0.001 같이 아주 작은 값을 부여해준다.
