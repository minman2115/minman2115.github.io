---
layout: post
title: "차원의 저주 문제"
tags: [데이터전처리]
comments: true
---

Data_Preprocessing_Studynotes_(20190612)

study program : https://www.fastcampus.co.kr/data_camp_ppc


#### [학습목표]


- 차원의 저주 문제 및 해결방법 이해


#### [학습기록]

#### 1. resampling

1) 오버샘플링 : 다수클래스와 가까운 위치에 소수 클래스 샘플 생성

2) 언더샘플링 : 소수클래스와 가까운 다수클래스 샘플 제거

그렇다면 소수클래스와 가까운 것을 어떻게 찾을것인가 -> 언더샘플링에서 가장 많이 고민하는 것.

#### 2. knn 기반의 언더샘플링은 잘 안쓰인다 

왜냐하면 knn을 사용해서 분류했을때 결과가 '소수'가 나오기 때문이다.

k개의 이웃가운데 k/2 개 이상의 이웃이 소수클래스면 그 샘플을 지우는 것이다.

예를들어서 5개 이웃가운데 3개 이상 소수클래이면 제거한다기 보다는 1개이상일때 제가한다라는 것이 더 낫다. 그런데 이 방법 역시 잘 작동하지 않는다.

#### 3. 니어미스 3가 제일 많이 안쓰인다. 

왜냐하면 클래스 불균형 문제를 확실하게 제거하지 못하기 때문이다. 니어미스 2가 통상 가장 많이 쓰인다.

#### 4. 서포트 벡터머신 참고사항

파트 4, 57페이지

$$\ \ min \dfrac{1}{2} ||w||^2 + C \sum_{i=1}^N \xi_i $$

c가 커지면 오차최소화

c가 작아지면 마진최대화 를 의도한다.

슬랙i는 샘플 i에 대한 오차정도를 말한다.

$$\ min \dfrac{1}{2} ||w||^2$$ 

부분은 마친을 최대화하는 의도이고 그 뒤의 식은 오차를 최소화 하는 의도이다.

#### 5. 차원의 저주 정의

- 차원이 증가하면 필요한 데이터 양과 시간 복잡도가 급격하게 증하하는 문제


- 차원이 커질수록 같은 양의 데이터가 있으면 차원의 빈공간을 데이터가 점점 그 공간을 채우지 못하는 문제가 발생하여 모델의 설명력이 떨어진다.


- 따라서 차원의 저주는 시간복잡도 증가뿐만 아니라, 과적합으로 인한 성능 저하로 이어지게 된다.

#### 6. 특징선택이란

- 특징선택은 n개의 특징으로 구성된 특징집합에서 새로운 특징부분 집합을 구성하는 방법. 쉽게 말해서 전체 피쳐들이 있으면 일부 피쳐만 뽑는 방법 


- 특징선택의 효과

1) 분류 및 예측 성능이 향상될 수 있다.

2) 차원축소 효과로 인한 계산량이 감소하고, 데이터 수집에 대한 부담이 줄어든다.


- 특징선택은 피쳐수가 적어도 해야한다.

![1](https://user-images.githubusercontent.com/41605276/59478916-044e7500-8e96-11e9-8ee4-4700744b9d81.png)

#### 7. 최적의 특징 집합을 구성하는 방법

- 모든 특징 집합을 비교하면 최적의 특징 집합을 반드시 찾을 수 있을 것이다. 하지만 현실적으로 특징개수가 n개라고 하면 2의 n승 -1 번의 모형학습이 필요하게 되므로 적용하기 쉽지 않다.


- 그러면 다른 효율적인 방법이 필요한데 '클래스 관련성'을 고려해야 한다.


- 특정 피쳐가 클래스를 얼마나 잘 설명하는지를 나타내는 것이다.


- 명목형 변수의 경우에는 이 특징의 값이 얼마나 불순도를 줄여주는지를 측정한다. decision tree에서 엔트로피 개념이다.


- 정수형 및 연속형 피쳐의 경우에는, 특징과 클래스 변수 간 상관성을 바탕으로 측정한다.


- 클래스 관련성을 측정하는 함수를 사용하여, 각 특징의 클래스 관련성을 계산하고 점수화하여 클래스 관련성이 큰 특징을 선택한다.



#### 8. 특징선택 시 척도선택 방법 (필터링 방법)

1) 분류문제에서 사용하는 척도

- F-통계량 (연속형 피쳐의 경우에만)

그룹 간 변동을 그룹 내 변동으로 나눈 것으로 계산하는 것으로 머신러닝에서 그룹은 같은 클래스 값을 가지는 샘플 집합이라고 해석 할 수 있다.


- 카이제곱 통계량 (명목형 특징의 경우에만)


- 상호정보량(크로스 엔트로피)


2) 예측문제에서 사용하는 척도

- R스케어(연속형 특징의 경우에만 사용)


- 상호정보량(크로스 엔트로피)


x가 연속형이고 y도 연속형이면 특징선택 시  r스퀘어를 쓴다. r스퀘어값(0~1)
        
x가 전부 명목형이면 카이스퀘어를 쓰면된다. 카이스퀘어값 (0 ~ 무한대)

문제가 어떤 변수는 연속형, 어떤 변수는 명목형이라고 일 경우이다.

피쳐유형들이 서로 다 혼합일때는 척도 통일이 어렵다. 이런 경우 필터링방법이 더 적합하다.

피쳐스케일에도 영향을 받는다.

#### 9. 필터링 방법의 문제점 

무조건 베스트 피쳐들만 뽑는게 최선이냐 그것도 아니다. 단순히 점수를 기준으로 몇개만 뽑는 것은 최선은 아니다. 

예를 들어서 축구팀에서 베스트 일레븐을 구성한다고 했을때 가장 실력이 뛰어난 선수로 11명을 구성한다고 한하면 무조건 골을 잘 넣는 11명을 뽑아서 구성하는 것이 능사는 아닌 것이다.

이말은 머신러닝에서 지도학습 모델에 따라 포지션 별로 적절하게 선수를 선발해야하는 것처럼 좋은 특징집합이 다를 수 있다는 것이다.

#### 10. 레퍼 방법의 기본개념

필터링 방법을 썼을때 현실적으로 전역최적인 피쳐들을 뽑는것은 불가능하다. 그러면 우리는 전역적으로 최선은 아니더라고 지역적으로라도 최선의 피쳐들을 뽑아줘야한다.

- 휴리스틱 : 대충, 어느정도 정답에 가까우나 정답은 아닌 

->이 휴리스틱을 일반화, 구조화하면 -> 메타 휴리스틱


- 보통은 한점교차연산을 많이 쓴다. 두개의 부모해당 하나의 자식을 만들도록 설정


- 래퍼방법

해집합을 평가하기 위한 모델을 미리선정을 해놓고 지역적으로 다 계산해놓고 각각의 세대에서 베스트 집합을 비교해서 좋은 피쳐를 뽑아내는방법 

- 래퍼방법을 적용할때 파라미터

1) 세대수(이터레이션)
세대수가 커질수록 좋은 해를 골라낼 확률이 커진다.
하지만 그만큼 시간이 길어진다.

2) 한세대에 포함된 해의 개수
한세대 포함된 해의 개수도 많아지면 좋은 해를 찾을 가능성이 커지지만
그만큼 역시 시간이 오래걸린다.

3) 선택되는 해의 개수(비율)

4) 돌연변이로 선택되는 해의개수

5) 돌연변이 정도

- 래퍼방법 시 극단적인 경우 예시 

1세대에 100개가 있다고 쳤을때

1) 99개 선택 -> 새로운 해를 탐색하기 어려움

2) 2개 선택 -> 수렴하기 어려움

돌연변이 비율이 높을수록 새로운 해를 찾을 가능성이 높아진다. 돌연변이 비율이 높을수록 또한 수렴가능성이 떨어진다. (이터레이션이 심해진다)