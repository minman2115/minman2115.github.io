---
layout: post
title: "Neural Network 기초개념 3"
tags: [딥러닝]
comments: true
---

.

Deep_Learning_Studynotes_(20190706)

study program : https://www.fastcampus.co.kr/data_camp_deeplearning

#### [학습기록]

#### 1. 멀티레이어 퍼셉트론에서 멀티 클래스 처리

- 결론적으로 얘기하면 softmax를 쓰면 된다. 예를 들어 클래스가 개, 고양이, 사자, 호랑이 4개가 있다고 치자. 그러면 아래 왼쪽그림처럼 output을 낼때 각각의 노드에 시그모이드를 쓸 수 있다. 개일 확률, 고양이일 확률, 호랑이일확률 이런식으로 할 수 있다. 하지만 이론적으로 전부 1이 될 수도 있고, 전부 0이 될수도 있다는 문제가 발생한다. 보통 그래서 이런 경우에는 멀티 레이블 문제를 풀때 정도 사용하고, 보통은 소프트맥스를 쓰게 된다.


- 그래서 소프트맥스 함수는 각각의 z에다가 exp를 취한 다음에 전부 이를 더한 수를 각각의 z의 exp 취한 것으로 나누어주면 된다. 그러면 아래 그림의 빨간색 박스처럼 된다.


- 그러면 ajL을 다 더한게 1이 되기 때문에 확률처럼 보이게 된다.

![1](https://user-images.githubusercontent.com/41605276/60888256-574afa80-a291-11e9-8074-872b72ea8a77.png)

- 멀티클래스의 분류를 할때는 소프트맥스를 마지막 레이어의 활성화 함수로 쓰게 된다. regression 할때는 보통 활성화 함수를 쓰지는 않는다. 가끔 시그모이드난 하이퍼볼릭 탄젠트를 쓰는 경우도 있는데 이 경우는 회귀를 하는데 회귀의 목표치가 0 ~ 1 인경우이다. 그리고 classification할때는 binary classification 할때는 보통 시그모이드를 쓰고, 그외에 멀티클래스면 소프트맥스를 쓰게 된다.


- 그러면 소프트맥스의 loss는 어떻게 계산하는가. 이때 loss function은 크로스엔트로피를 쓴다. 보통 분류문제에서는 크로스엔트로피를 쓴다고 보면된다. 크로스엔트로피는 바이너리 크로스엔트로피만 받는거 같은데 어떻게 멀티클래스를 처리하냐. 정답인 클래스와 정답이 아닌 나머지 클래스들 두덩어리로 나누어서 처리하면 된다.

![2](https://user-images.githubusercontent.com/41605276/60888259-5ca84500-a291-11e9-9554-804d456f5860.png)

- 그러면 예를 들어서 계산을 해보자. 2가지 모델이 있다고 치자 이 모델의 classification error는 둘다 33%다. 이때 크로스엔트로피와, squared loss에 따른 각각의 loss를 구해보자

![3](https://user-images.githubusercontent.com/41605276/60888270-62058f80-a291-11e9-9730-25e323e74d85.png)

- 딱봐도 model2가 더 좋은 모델이다. model1은 아리까리한 확률로 class를 맞췄지만 model2는 상대적으로 model1보다 확실한 확률로 class를 맞췄기 때문이다. loss 값을 실제로 계산해도 model2가 더 loss가 적게 나온다. 


- MSE는 참고로 정답에서 멀어질 수록 학습속도가 떨어지는 문제가 있다.

#### 2. 활성화 함수

- 아래 그림과 같이 여러 활성화 함수가 있고, 이 외에도 더 있지만 통상 아래 있는 함수들이 가장 많이 쓰인다. 그리고 이중에서도 relu가 가장 많이 쓰이는 편이다. 내가 활성화 함수에 대해 잘 모르겠다 싶으면 랠루를 쓰는 것이 가장 무난하다.

![4](https://user-images.githubusercontent.com/41605276/60888280-67fb7080-a291-11e9-99b1-cdb1c46fb596.png)

[시그모이드 함수]

- 특징 : 모든 입력값에 대해 0 ~ 1 사이의 값의 출력으로 변환해준다. 


- 문제점 :

1) Saturated neurons “kill” the gradients

그레디언트 소멸문제. 그레디언트가 0에 가까워지는 영역이 있는 문제


2) Sigmoid outputs are not zero centered

시그모이드 아웃풋이 음수가 안나오는 문제

시그모이드로 나오는 출력은 모두 양수이기 때문에 이 출력값이 다음 노드로 들어간다면 입력이 전부 역시 양수가 들어가게 된다. 특히 back propogation할때 input이 모두 positive일때 w에 대한 그레디언트가 모두 다 음수이거나 모두 다 양수이게 된다. 다시말해서 모든 w에 대해서 미분값이 전부다 양수이거나 전부다 음수이거나해서 최적화 시 아래 그림처럼 1사분면과 3사분면 방향으로 밖에 못가는 문제가 발생한다. 그래서 아래 그림처럼 zig zag path로 최적값을 찾게 되는데 아예 최적값을 못찾는것은 아니지만 속도가 느리다. 결론은 학습속도가 느리다는 것이다.

![5](https://user-images.githubusercontent.com/41605276/60888296-6df15180-a291-11e9-883b-67f733ae282f.png)

3) Exp () is a bit compute expensive

exp 연산이 있기 때문에 컴퓨터가 연산하는데 부담이 있다.(이런 문제점 때문에 지니불순도를 대신하여 사용하기도 한다.)

[하이퍼볼릭 탄젠트 함수]

![6](https://user-images.githubusercontent.com/41605276/60888302-72b60580-a291-11e9-8f85-f69c249e6160.png)

- 기울기가 가장클때는 1이다. 그래서 그레디언트의 크기가 항상 1보다 작거나 같게 된다. 그래서 saturated 영역이 있어서 시그모이드 함수와 마찬가지로 그레디언트 소멸문제가 발생할 수 있다.

[랠루 함수]

![7](https://user-images.githubusercontent.com/41605276/60888321-79447d00-a291-11e9-9c75-3ef506aa9601.png)

- 그레디언트 계산하면 0 아니면 1이다. 그래서 0은 차피 그냥 날아가버릴것이고 forward 방향으로 갈때 연산이 안되고 사라져버리기 때문에 backword로 올때 그레디언트가 0이어도 문제가 되지 않는다.


- 문제점

1) Not zero centered output

2) An annoyance dead ReLU Slide

모든 레이어는 f(wx+b)로 쓸 수 있는데 예를들어 w를 초기값을 잘못줘서 아래 그림과 같이 빨간색 선으로 그어버리게 되면 dead 랠루에 빠져서 가중치 업데이트가 불가능하게 된다. 또는 learning rate를 잘못줘서 선을 빨간색 선처럼 그어버리게 되면 또 데드랠루에 빠져서 가중치 업데이트가 아예 불가능해진다. 초기값을 부여할때 그래서 트릭으로 바이어스값을 살짝 양수로 줘서 바이어스 만큼 약간 양수쪽으로 쉬프트되기 때문에 완전히 다 데드랠루에 빠지지 않는다고 한다.

![8](https://user-images.githubusercontent.com/41605276/60888334-7f3a5e00-a291-11e9-8412-7158ce68c6f9.png)

그래서 사람들이 랠루처럼 음수쪽을 전부 버리는 것이 올바른것이냐 생각하게 되었고 음수쪽에도 약간의 기울기를 주자는 아이디어가 나왔다. 

그래서 아래 그림과 같이 랠루의 단점을 보완한 함수들이 등장했다.

![9](https://user-images.githubusercontent.com/41605276/60888343-86fa0280-a291-11e9-926c-896bd86f50b8.png)

- ELU에서 음수쪽에 살짝 꺽은 이유는 아웃라이어에 둔감하게 만들기 위해서다.

[maxout]

랠루와 위키랠루 이런 얘들의 정규화 버전이다.

![10](https://user-images.githubusercontent.com/41605276/60888353-8c574d00-a291-11e9-9beb-bb3c04a1b20f.png)

#### 3. 데이터 전처리

- 이미지 데이터의 전처리

variance를 노멀라이징을 잘 안하는 편이다. 왜냐하면 픽셀값이 0 ~ 255로 보통 정해져 있기 때문이다.


그러면 min max 스케일링 같은거 할때는 어떻게 하냐. alexnet에서는 예를 들어서 이미지가 3만장이 있으면 3만장의 이미지에 대해 각각의 사진에 대한 픽셀값의 평균을 계산한다. 반면에 vgcnet에서는 채널별로 평균을 구해서 적용했다. 

#### 4. 가중치 초기화

모든 w에 대해 같은 값으로 초기화를 해주면 어떻게 될까. 예를 들어 모든 w값을 constant로 주게되면 얘네들이 똑같이 업데이트가 되어 학습이 거의 안된다고 보면 된다. 그래서 가중치를 줄때 w를 모두 랜덤하게해서 다르게 줘야한다.


그러면 네트워크를 딥하게 해도 학습이 잘 될 수 있도록 초기값을 어떻게 줘야 할까. 가장 기초적인 아이디어는 아래 그림과 같이 가우시안 노멀분포에서 랜덤하게 뽑아서 w에 할당해주는 것이다.

아래 그림의 예시는 가중치값에 대해서 하이퍼볼릭 탄젠트 함수를 통과한 레이어의 출력값(output값) 대한 분산을 보여주는 것이다. 

** 아래 그림에서 참고사항 = fan_in : 입력개수, fan_in : 출력개수

![11](https://user-images.githubusercontent.com/41605276/60888364-92e5c480-a291-11e9-9fdb-f7092a2a825d.png)

- 실제 가우시안 분포로 w값을 뽑는 것을 시뮬레이션하면 레이어가 깊어질 수록 평균이 0이고 표준편차도 0이 되버리는 문제가 발생한다. 입력으로 들어오는 값이 0의 근처의 값이 들어오고 거기에 w도 0에 가까운 값이 들어오게 되면 하이퍼볼릭탄젠트 함수는 0 근처에서 그냥 linear하게 통과시키기 때문에 점점 0으로 수렴하게 된다.


- 이런문제를 막아보고자 가중치를 키우는데 가우시안 노멀분포에서 0.01이 아니라 1을 곱해봤다. 그런데 분포를 보니까 양쪽 극단으로 쫙 퍼진 현상을 볼 수 있다. 왜냐하면 하이퍼볼릭 탄젠트 함수가 모든 입력에 대해서 -1 ~ 1 사이로 출력을 주니까 다 극단으로 가게 된다. 그래서 세츄에이션 영역으로 빠져버렸다 그리고 그레디언트가 0이 되어버리는 문제가 발생한다.

![12](https://user-images.githubusercontent.com/41605276/60888371-98dba580-a291-11e9-8388-17356e17d6c9.png)

- 이런 문제 때문에 나온것이 하비에르 초기화 방법이다. 결과적으로 분포 모양을 보면 w값이 가우시안 정규분포를 이쁘게 이루고 있다. 이 가중치를 표준편차를 정해놓고 무작정 초기화 하는 것은 좋지 않은 방법이라고 판단하고, 퍼셉트론의 가지가 몇개 달려있는지 보고 그 표준편차를 정해야한다는 아이디어다.  그래서 in/out 의 variance 를 같게 해보자


- 그런데 하비에르 초기화 방법은 activation 함수를 linear한 함수라고 가정한 것이 문제다. 그러나 활성화 함수중에 non-linear한 함수가 대부분이다. 만약에 아래 그림처럼 랠루함수를 활성화 함수로 쓰는 경우에 하비에르 초기화 방법을 쓰게 되면 분산이 0으로 점점 줄어드는 문제가 발생한다.

![13](https://user-images.githubusercontent.com/41605276/60888383-9ed18680-a291-11e9-89b2-f25fc2a7bc16.png)

- 그래서 Activation function 을 ReLU 나 PReLU를 쓰고도 in/out의 variance를 동일하게 해보자 해서 나온 것이 위의 그림과 같은  He 초기화 방법이다. 


#### 가중치 초기화 결론 

사실 가중치 초기화를 대충 막 해도 batch nomalization 때문에 학습이 잘 된다. 분포가 0이 되거나 쪼그라드는 문제가 있는데 매 레이어마다 정규화를 하면 이런 문제가 사라진다. 그래도 초기의 학습방향을 잘 잡기 위해 가중치 초기화를 잘 해야하는 것이다. 캐라스에서는 initializer API의 디폴트값으로 하비에르 initializer를 쓴다.


** batch nomalization : 매 레이어마다 정규화를 시켜주는 개념

#### 5. Learning Rate

- 러닝레이트도 하이퍼파라미터로 상당히 중요한 요소이다. 러닝레이트가 너무 크면 데드렐루에 빠질 수도 있고, 반면에 너무 작으면 학습이 잘 안될수도 있다.

![14](https://user-images.githubusercontent.com/41605276/60888388-a4c76780-a291-11e9-92d9-6f3a9379ded5.png)

- 이런 러닝레이트의 고민으로 나온 방법이 Leanring Rate Decay다. 

쉽게 말해서 학습되는 것을 모니터링 하다가 잘 학습이 안된다고 하면 러닝레이트를 탄력적으로 적용하는 것이다.

![15](https://user-images.githubusercontent.com/41605276/60888396-a98c1b80-a291-11e9-9f96-62f3faea89af.png)

- cyclic learning rate 방법도 있다.

![16](https://user-images.githubusercontent.com/41605276/60888402-aee96600-a291-11e9-9329-53bfafd53031.png)

#### 6. 정규화방법

![17](https://user-images.githubusercontent.com/41605276/60888411-b3ae1a00-a291-11e9-91ca-cdf16ab31937.png)

가로 축이 모델사이즈라고 보면 되는데 결론적으로 적절한 모델사이즈를 찾는 것이 중요하다.

내가 풀고자 하는 문제를 풀 수 있는 가장 작은 모델을 쓰는 것을 만드는 것이 가장좋다고 하는데 이는  참 애매한 개념이다. 하이퍼 파라미터 튜닝할것도 많은데 현실적으로 찾을 수 없다.

그래서 딥러닝에서는 모델을 일단 키우고 대신에 generalization gap을 최대한 줄이는 방법을 찾아보자는 방법을 일반적으로 쓴다.

그래서 많이 쓰는 정규화 방법중 하나가 early stopping이다. 아래 오른쪽 그림처럼 gap이 커지기 전에 멈추는 방법인데 어디서 멈춰야 할지 아는 것이 쉽지는 않다.

![18](https://user-images.githubusercontent.com/41605276/60888421-b9a3fb00-a291-11e9-9521-54279448c413.png)

또 다른 방법으로 generalization gap을 직접적으로 줄여주는 것이 정규화방법이다. 

일반적인 loss function에 추가로 penalty term을 추가해서 이 penalty term이 커질수록 loss가 커지는 것처럼 효과를 주는 것을 말한다.

L2 방법은 모든 가중치값들을 제곱해서 더한다. L1 방법은 반면에 모든 가중치값들의 절대값을 취해서 더한다. elastic net 방법은 L1방법과 L2방법을 적절하게 조화시킨 방법이다.

그래서 핵심은 W값들이 너무 튀게 하지 않게 줄여줌으로써 함수의 일반화를 유도하는 것이다.

![19](https://user-images.githubusercontent.com/41605276/60888436-bf99dc00-a291-11e9-829d-92790e6ad078.png)

정규화를 할 수 있는 방법으로 Dropout 이라는 방법도 있다. 특정 노드에 대해서 동전을 던져서 이 노드를 살릴지 죽일지 결정하여 노드를 날리는 방법이다. 동전 던지기 확률은 사용자가 파라미터로 지정해줄 수 있다. 이렇게 임의의 노드를 날려서 과적합을 막을 수 있다.

트레인할때 노드를 드랍아웃으로 날리고 학습을 하고, 테스트 할때는 다시 전부 노드를 살려서 테스트를 하게 된다. 아래 그림과 같이 만약에 드랍아웃 확률이 0.5인 네트워크가 있다고 하자. training time에는 둘다 살아있는 경우, 둘다죽은경우, 하나만 사는 경우로 하면 1/4씩 해서 각각의 경우에 대한 기대값을 구한다. 그리고 test time에는 dropout 확률을 곱해서 원래 크기를 맞춰준다.

![20](https://user-images.githubusercontent.com/41605276/60888451-c4f72680-a291-11e9-9f90-47d6ed3106df.png)

정규화를 위한 또 다른 방법으로 Batch normalization이 있다. 이 방법이 가장 대중적으로 많이 쓰이는 방법이다. 
간단하게 말하면 배치 정규화는 학습 시의 미니배치를 한 단위로 정규화를 하는 것으로 분포의 평균이 0, 분산이 1이 되도록 정규화(nomalize)하는 것을 말한다.

먼저 Input으로 사용된 미니배치의 평균과 분산을 계산을 한다. 그 다음 hidden layer의 활성화값 및 출력값에 대해서 평균이 0, 분산이 1이 되도록 정규화를 한다(=변환). 그래서 데이터 분포가 덜 치우치게 되고 배치 정규화 단계마다 확대scale와 이동shift 변환을 수행한다.

데이터가 있으면 아래 그림과 같이 N이 batch size다. 이 랜덤으로 뽑힌 특정 batch size의 데이터에 대해서 첫번째 레이어에 들어가는 것을 mean zero라고 하고, 그 다음부터 들어가는 것은 batch가 한꺼번에 통과해서 출력이 batch 단위로 나올건데 그 batch 단위로 나오는 출력에 대해서 노멀라이즈를 하는 것이다. 그런데 이렇게 무작정 노멀라이즈를 하면 바이어스를 더해주는 의미가 없고, variance도 이렇게 정규화해서 1로 맞추는것도 가장 바람직한 방법은 아니다. 그래서 이 노멀라이즈 한 배치 출력값에 대해 감마를 곱하고 배타를 더해준다. 즉 w를 곱해주고 b를 다시 더해주는 셈이 된다. 이 감마와 베타도 학습을 하면서 최적의 값을 찾아나간다. 

이런 배치 노멀라이제이션을 해주면 그레디언트 플로우가 잘 흘러가는 장점이 있다. w값의 초기화를 대충해도 잘 학습이 된다. 또한 learning rate를 키워도 잘 학습이 된다고 알려져 있다. 왜냐하면 distribution이 잘 유지되기 때문이다. 또한 정규화 효과도 있다.

![21](https://user-images.githubusercontent.com/41605276/60888463-cc1e3480-a291-11e9-8ad7-7f5bcafac292.png)

이 배치노멀라이제이션도 트레인과 테스트를 다르게 해주는 것이 특징이다. 트레이닝할때 배치 단위로 평균과 분산을 구했는데, 테스트 타임에 배치단위의 데이터가 들어올거라는 보장이 없기 때문이다. 그러면 얘를들어서 나는 이미지 데이터를 가지고 뉴럴네트워크를 트레이닝할때 100장의 배치단위로 배치노멀라이제이션을 해줬는데 그러면 모델 서비스를 제공할때 사람들이 100장 이미지를 올릴때까지 기다려야 하냐. 이런 문제를 해결하기 위해서 트레이닝 할때 배치의 평균들의 평균을 계산해놨다가 이걸 테스트 타임에 한장이 들어와도 이 평균값을 이용해서 테스트를 할 수 있게 해준다. 다시말해서 트레이닝 타임에 학습할때 썼던 평균과 분산의 평균과 분산을 기록해두었다가 테스트 할때 써먹게 된다.

이 배치 노멀라이즈도 단점이 있는데 배치사이즈가 작으면 문제가 생길 수 있다. 왜그러냐면 아무래도 평균을 이용하기 때문에 노이즈에 취약할 수 있다. 샘플링의 양이 적을수록 variance가 엄청커지기 때문이다.

그 밖에 데이터를 어떻게 쪼개는지에 따라서 노멀라이즈 방법이 달라지게 된다.

또한 오버피팅을 잡는 가장 확실한 방법은 데이터의 양을 늘려주는 것인데 내가 갖고 있는 데이터를 살짝 변형해서 데이터의 수를 강제로 늘려주는 방법도 있을것이다. 이를 data augmentation이라고 한다.

![22](https://user-images.githubusercontent.com/41605276/60888469-d17b7f00-a291-11e9-99b0-b8176ff5d5e1.png)

#### 7. tensorflow를 이용한 logistic regression 구현실습 1

## Pima Indians Diabetes Dataset for Binary Classification

This dataset describes the medical records for Pima Indians and whether or not each patient will have an onset of diabetes within five years.

This dataset can be downloaded from https://www.kaggle.com/kumargh/pimaindiansdiabetescsv

이 dataset의 몇가지 주요 항목을 살펴보면 다음과 같습니다

- 인스턴스 수 : 768개
- 속성 수 : 8가지
- 클래스 수 : 2가지

8가지 속성(1번~8번)과 결과(9번)의 상세 내용은 다음과 같습니다.

1. 임신 횟수
2. 경구 포도당 내성 검사에서 2시간 동안의 혈장 포도당 농도
3. 이완기 혈압 (mm Hg)
4. 삼두근 피부 두겹 두께 (mm)
5. 2 시간 혈청 인슐린 (mu U/ml)
6. 체질량 지수
7. 당뇨 직계 가족력
8. 나이 (세)
9. 5년 이내 당뇨병이 발병 여부


```python
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
#from sklearn.preprocessing import MinMaxScaler

print(tf.__version__)
```

    2.0.0-beta1
    


```python
#tf.enable_eager_execution()
```


```python
#DATA_FILE = './data/pima-indians-diabetes.csv'
DATA_FILE = '/content/gdrive/My Drive/TensorFlow_Training_13th/data/pima-indians-diabetes.csv'
```


```python
xy = np.loadtxt(DATA_FILE, delimiter=',', dtype=np.float32)
x_train = xy[:, 0:-1]
y_train = xy[:, [-1]]
# 여기서 -1이라는 것은 맨 마지막 부터인것은 동일하나 대괄호가 들어가면 디멘전이 달라지게 된다.

print(x_train.shape, y_train.shape)
print(xy)
```

    (768, 8) (768, 1)
    [[  6.    148.     72.    ...   0.627  50.      1.   ]
     [  1.     85.     66.    ...   0.351  31.      0.   ]
     [  8.    183.     64.    ...   0.672  32.      1.   ]
     ...
     [  5.    121.     72.    ...   0.245  30.      0.   ]
     [  1.    126.     60.    ...   0.349  47.      1.   ]
     [  1.     93.     70.    ...   0.315  23.      0.   ]]
    


```python
# 피쳐들의 스케일이 다르면 가중치를 곱해져서 더할건데 이때 스케일이 큰숫자가 가장 영향력이 커지기 때문에
# 모든데이터에서 이 데이터들 중에 미니멈값을 뺀 숫자를 맥시멈에서 미니멈을 뺀 것을 나눠주게 된다.
# 통상 피쳐들을 0 ~ 1 사이의 값으로 노멀라이징하는 전처리를 해주게 된다.

def MinMaxScaler(data):
    ''' Min Max Normalization
    Parameters
    ----------
    data : numpy.ndarray
        input data to be normalized
        shape: [Batch size, dimension]
    Returns
    ----------
    data : numpy.ndarry
        normalized data
        shape: [Batch size, dimension]
    References
    ----------
    .. [1] http://sebastianraschka.com/Articles/2014_about_feature_scaling.html
    '''
    numerator = data - np.min(data, 0)
    denominator = np.max(data, 0) - np.min(data, 0)
    # noise term prevents the zero division
    return numerator / (denominator + 1e-7)
  
  # 여기서 10의 -7승이 왜 들어갔냐면 모든숫자가 다 같으면 맥시멈 - 미니멈이 0이 되기 때문에 그것을 방지하기 위함이다.
```


```python
x_train = MinMaxScaler(x_train)
x_train
```




    array([[0.3529412 , 0.74371856, 0.59016395, ..., 0.5007451 , 0.23441501,
            0.48333332],
           [0.05882353, 0.42713568, 0.5409836 , ..., 0.39642325, 0.11656704,
            0.16666667],
           [0.47058824, 0.919598  , 0.52459013, ..., 0.34724292, 0.25362936,
            0.18333334],
           ...,
           [0.29411766, 0.6080402 , 0.59016395, ..., 0.390462  , 0.07130657,
            0.15      ],
           [0.05882353, 0.63316584, 0.4918033 , ..., 0.44858423, 0.11571307,
            0.43333334],
           [0.05882353, 0.46733668, 0.57377046, ..., 0.45305514, 0.10119555,
            0.03333334]], dtype=float32)




```python
batch_size = x_train.shape[0]
n_epoch = 1000
learning_rate = 0.1
```


```python
dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(1000).batch(batch_size)
```


```python
w = tf.Variable(tf.random_normal_initializer()([8, 1]))
# 8행 1열 매트릭스가 되야지 길이가 8짜리 x가 들어오면 곱할 수 있기 때문이다.
# 그래서 8값에 입력의 길이를 보통 넣어주게 된다.

b = tf.Variable(tf.random_normal_initializer()([1]))
```


```python
def logistic_regression(inputs):
    hypothesis = tf.keras.activations.sigmoid(tf.matmul(inputs, w) + b)
    return hypothesis
```


```python
def loss_fn(inputs, labels):
    hypothesis = logistic_regression(inputs)
    loss = tf.reduce_mean(tf.keras.losses.binary_crossentropy(labels, hypothesis))
    return loss
```


```python
def grad(inputs, labels):
    hypothesis = logistic_regression(inputs)
    with tf.GradientTape() as tape:
        loss = loss_fn(inputs, labels)
    grads = tape.gradient(loss, [w, b])
    return grads
```


```python
def accuracy_fn(inputs, labels):
    hypothesis = logistic_regression(inputs)
    prediction = tf.cast(hypothesis > 0.5, dtype=tf.float32)
    # true는 1.0, false는 0.0을 리턴해줄 것이다.
    
    accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, labels), dtype=tf.float32))
    return accuracy
```


```python
optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)
```


```python
total_steps = int(x_train.shape[0]/batch_size)
for epoch in range(n_epoch):
    total_loss = 0.
    for x, y in dataset: 
        grads = grad(x, y)        
        optimizer.apply_gradients(grads_and_vars=zip(grads, [w, b]))
        loss = loss_fn(x, y)
        total_loss += loss / total_steps
    if (epoch+1) % 10 == 0:
        print('Epoch {0}: {1:.8f}'.format(epoch+1, total_loss))
```

    Epoch 10: 0.67067021
    Epoch 20: 0.66268939
    Epoch 30: 0.65868026
    Epoch 40: 0.65606815
    Epoch 50: 0.65396786
    Epoch 60: 0.65206867
    Epoch 70: 0.65026015
    Epoch 80: 0.64850199
    Epoch 90: 0.64677942
    Epoch 100: 0.64508677
    Epoch 110: 0.64342165
    Epoch 120: 0.64178276
    Epoch 130: 0.64016962
    Epoch 140: 0.63858140
    Epoch 150: 0.63701773
    Epoch 160: 0.63547802
    Epoch 170: 0.63396186
    Epoch 180: 0.63246888
    Epoch 190: 0.63099843
    Epoch 200: 0.62955028
    Epoch 210: 0.62812376
    Epoch 220: 0.62671870
    Epoch 230: 0.62533456
    Epoch 240: 0.62397087
    Epoch 250: 0.62262732
    Epoch 260: 0.62130350
    Epoch 270: 0.61999905
    Epoch 280: 0.61871356
    Epoch 290: 0.61744678
    Epoch 300: 0.61619812
    Epoch 310: 0.61496741
    Epoch 320: 0.61375433
    Epoch 330: 0.61255842
    Epoch 340: 0.61137944
    Epoch 350: 0.61021698
    Epoch 360: 0.60907078
    Epoch 370: 0.60794050
    Epoch 380: 0.60682589
    Epoch 390: 0.60572672
    Epoch 400: 0.60464257
    Epoch 410: 0.60357314
    Epoch 420: 0.60251826
    Epoch 430: 0.60147756
    Epoch 440: 0.60045081
    Epoch 450: 0.59943789
    Epoch 460: 0.59843826
    Epoch 470: 0.59745193
    Epoch 480: 0.59647858
    Epoch 490: 0.59551787
    Epoch 500: 0.59456968
    Epoch 510: 0.59363371
    Epoch 520: 0.59270978
    Epoch 530: 0.59179777
    Epoch 540: 0.59089726
    Epoch 550: 0.59000814
    Epoch 560: 0.58913022
    Epoch 570: 0.58826333
    Epoch 580: 0.58740717
    Epoch 590: 0.58656162
    Epoch 600: 0.58572656
    Epoch 610: 0.58490169
    Epoch 620: 0.58408684
    Epoch 630: 0.58328193
    Epoch 640: 0.58248663
    Epoch 650: 0.58170098
    Epoch 660: 0.58092457
    Epoch 670: 0.58015746
    Epoch 680: 0.57939935
    Epoch 690: 0.57865018
    Epoch 700: 0.57790965
    Epoch 710: 0.57717788
    Epoch 720: 0.57645446
    Epoch 730: 0.57573932
    Epoch 740: 0.57503241
    Epoch 750: 0.57433361
    Epoch 760: 0.57364255
    Epoch 770: 0.57295936
    Epoch 780: 0.57228380
    Epoch 790: 0.57161576
    Epoch 800: 0.57095510
    Epoch 810: 0.57030171
    Epoch 820: 0.56965548
    Epoch 830: 0.56901628
    Epoch 840: 0.56838399
    Epoch 850: 0.56775862
    Epoch 860: 0.56713992
    Epoch 870: 0.56652778
    Epoch 880: 0.56592220
    Epoch 890: 0.56532294
    Epoch 900: 0.56473005
    Epoch 910: 0.56414336
    Epoch 920: 0.56356275
    Epoch 930: 0.56298822
    Epoch 940: 0.56241953
    Epoch 950: 0.56185669
    Epoch 960: 0.56129956
    Epoch 970: 0.56074816
    Epoch 980: 0.56020230
    Epoch 990: 0.55966181
    Epoch 1000: 0.55912691
    


```python
print('Accuracy: {}'.format(accuracy_fn(x_train, y_train)))
```

    Accuracy: 0.7200520634651184
    

#### 8. tensorflow를 이용한 logistic regression 구현실습 2

## Importing Libraries


```python
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.utils import to_categorical
import numpy as np
import matplotlib.pyplot as plt
import os

print(tf.__version__)
print(keras.__version__)
```

    2.0.0-beta1
    2.2.4-tf
    

## Enable Eager Mode


```python
if tf.__version__ < '2.0.0':
    tf.enable_eager_execution()
```

## Hyper Parameters


```python
learning_rate = 0.001
training_epochs = 20
batch_size = 100
n_class = 10
```

## MNIST/Fashion MNIST Data


```python
## MNIST Dataset #########################################################
mnist = keras.datasets.mnist
class_names = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']
##########################################################################

## Fashion MNIST Dataset #################################################
#mnist = keras.datasets.fashion_mnist
#class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']
##########################################################################
```

## Datasets


```python
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()  
```

    Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz
    11493376/11490434 [==============================] - 0s 0us/step
    


```python
type(train_images), type(train_labels)
```




    (numpy.ndarray, numpy.ndarray)




```python
train_images.shape, train_labels.shape
```




    ((60000, 28, 28), (60000,))




```python
test_images.shape, test_labels.shape
```




    ((10000, 28, 28), (10000,))




```python
n_train = train_images.shape[0]
n_test = test_images.shape[0]
```


```python
plt.figure()
plt.imshow(train_images[0], cmap=plt.cm.binary)
plt.colorbar()
```

![Neural Network 기초개념 3_79_1](https://user-images.githubusercontent.com/41605276/60888518-efe17a80-a291-11e9-9e84-0070400fc98b.png)



```python
plt.figure(figsize=(15,15))
for i in range(25):
    plt.subplot(5,5,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(train_images[i], cmap=plt.cm.binary)
    plt.xlabel(class_names[train_labels[i]])
```


![Neural Network 기초개념 3_80_0](https://user-images.githubusercontent.com/41605276/60888528-f5d75b80-a291-11e9-9e99-be20b81d13a4.png)



```python
train_images = train_images.astype(np.float32) / 255.
test_images = test_images.astype(np.float32) / 255.
    
train_labels = to_categorical(train_labels, n_class)
test_labels = to_categorical(test_labels, n_class)    
    
train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(
                buffer_size=100000).batch(batch_size)
test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(batch_size)
```

## Model Function


```python
def create_model():
    model = keras.Sequential()
    model.add(keras.layers.Flatten(input_shape=(28,28)))
    model.add(keras.layers.Dense(10, kernel_initializer=tf.keras.initializers.RandomNormal(),
                                activation='softmax'))    
    return model
```


```python
model = create_model()
model.summary()
```

    Model: "sequential"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    flatten (Flatten)            (None, 784)               0         
    _________________________________________________________________
    dense (Dense)                (None, 10)                7850      
    =================================================================
    Total params: 7,850
    Trainable params: 7,850
    Non-trainable params: 0
    _________________________________________________________________
    

## Loss Function




```python
def loss_fn(model, images, labels):
    predictions = model(images, training=True)
    loss = tf.reduce_mean(keras.losses.categorical_crossentropy(labels, predictions))   
    return loss
```

## Calculating Gradient & Updating Weights


```python
def train(model, images, labels):
    with tf.GradientTape() as tape:
        loss = loss_fn(model, images, labels)
    grads = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(grads, model.trainable_variables))
```

## Caculating Model's Accuracy


```python
def evaluate(model, images, labels):
    predictions = model(images, training=False)
    correct_prediction = tf.equal(tf.argmax(predictions, 1), tf.argmax(labels, 1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))    
    return accuracy
```

## Optimizer


```python
optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
```

## Training


```python
# train my model
print('Learning started. It takes sometime.')
for epoch in range(training_epochs):
    avg_loss = 0.
    avg_train_acc = 0.
    avg_test_acc = 0.
    train_step = 0
    test_step = 0
    
    for images, labels in train_dataset:
        train(model,images, labels)
        loss = loss_fn(model, images, labels)
        acc = evaluate(model, images, labels)        
        avg_loss = avg_loss + loss
        avg_train_acc = avg_train_acc + acc
        train_step += 1
    avg_loss = avg_loss / train_step
    avg_train_acc = avg_train_acc / train_step
    
    for images, labels in test_dataset:        
        acc = evaluate(model, images, labels)        
        avg_test_acc = avg_test_acc + acc
        test_step += 1    
    avg_test_acc = avg_test_acc / test_step    

    print('Epoch:', '{}'.format(epoch + 1), 'loss =', '{:.8f}'.format(avg_loss), 
          'train accuracy = ', '{:.4f}'.format(avg_train_acc), 
          'test accuracy = ', '{:.4f}'.format(avg_test_acc))


print('Learning Finished!')
```

    Learning started. It takes sometime.
    

    WARNING: Logging before flag parsing goes to stderr.
    W0706 04:04:00.560467 139733226547072 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1220: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
    Instructions for updating:
    Use tf.where in 2.0, which has the same broadcast rule as np.where
    

    Epoch: 1 loss = 0.62537342 train accuracy =  0.8450 test accuracy =  0.9039
    Epoch: 2 loss = 0.34447038 train accuracy =  0.9061 test accuracy =  0.9140
    Epoch: 3 loss = 0.30790135 train accuracy =  0.9153 test accuracy =  0.9180
    Epoch: 4 loss = 0.29076752 train accuracy =  0.9192 test accuracy =  0.9222
    Epoch: 5 loss = 0.28040630 train accuracy =  0.9218 test accuracy =  0.9239
    Epoch: 6 loss = 0.27327609 train accuracy =  0.9237 test accuracy =  0.9250
    Epoch: 7 loss = 0.26795337 train accuracy =  0.9256 test accuracy =  0.9254
    Epoch: 8 loss = 0.26376852 train accuracy =  0.9266 test accuracy =  0.9252
    Epoch: 9 loss = 0.26036763 train accuracy =  0.9274 test accuracy =  0.9257
    Epoch: 10 loss = 0.25752684 train accuracy =  0.9283 test accuracy =  0.9261
    Epoch: 11 loss = 0.25509214 train accuracy =  0.9290 test accuracy =  0.9269
    Epoch: 12 loss = 0.25297555 train accuracy =  0.9298 test accuracy =  0.9274
    Epoch: 13 loss = 0.25111151 train accuracy =  0.9304 test accuracy =  0.9275
    Epoch: 14 loss = 0.24945579 train accuracy =  0.9311 test accuracy =  0.9274
    Epoch: 15 loss = 0.24797162 train accuracy =  0.9318 test accuracy =  0.9278
    Epoch: 16 loss = 0.24662957 train accuracy =  0.9323 test accuracy =  0.9277
    Epoch: 17 loss = 0.24539943 train accuracy =  0.9325 test accuracy =  0.9275
    Epoch: 18 loss = 0.24427384 train accuracy =  0.9329 test accuracy =  0.9273
    Epoch: 19 loss = 0.24323754 train accuracy =  0.9331 test accuracy =  0.9274
    Epoch: 20 loss = 0.24227658 train accuracy =  0.9334 test accuracy =  0.9273
    Learning Finished!
    


```python
def plot_image(i, predictions_array, true_label, img):
    predictions_array, true_label, img = predictions_array[i], true_label[i], img[i]
    plt.grid(False)
    plt.xticks([])
    plt.yticks([])

    plt.imshow(img,cmap=plt.cm.binary)

    predicted_label = np.argmax(predictions_array)
    if predicted_label == true_label:
        color = 'blue'
    else:
        color = 'red'

    plt.xlabel("{} {:2.0f}% ({})".format(class_names[predicted_label],
                                100*np.max(predictions_array),
                                class_names[true_label]),
                                color=color)

def plot_value_array(i, predictions_array, true_label):
    predictions_array, true_label = predictions_array[i], true_label[i]
    plt.grid(False)
    #plt.xticks([])
    plt.xticks(range(n_class), class_names, rotation=90)
    plt.yticks([])
    thisplot = plt.bar(range(n_class), predictions_array, color="#777777")
    plt.ylim([0, 1]) 
    predicted_label = np.argmax(predictions_array)
 
    thisplot[predicted_label].set_color('red')
    thisplot[true_label].set_color('blue')
```


```python
rnd_idx = np.random.randint(1, n_test//batch_size)
img_cnt = 0
for images, labels in test_dataset:
    img_cnt += 1
    if img_cnt != rnd_idx:
        continue
    predictions = model(images, training=False)
    num_rows = 5
    num_cols = 3
    num_images = num_rows*num_cols
    labels = tf.argmax(labels, axis=-1)
    plt.figure(figsize=(3*2*num_cols, 4*num_rows))
    plt.subplots_adjust(hspace=1.0)
    for i in range(num_images):
        plt.subplot(num_rows, 2*num_cols, 2*i+1)
        plot_image(i, predictions.numpy(), labels.numpy(), images.numpy())
        plt.subplot(num_rows, 2*num_cols, 2*i+2)
        plot_value_array(i, predictions.numpy(), labels.numpy())        
    break
```

![Neural Network 기초개념 3_97_0](https://user-images.githubusercontent.com/41605276/60888536-fa9c0f80-a291-11e9-92b6-460922706fff.png)